{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Поиск паттернов в многомерных временных рядах для предотвращения отказов в технологическом оборудовании](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача состоит в поиске паттернов, предшествующих отказу технологического оборудования. Задача относится к классификации многомерных временных рядов.\n",
    "\n",
    "В качестве данных используются показания с датчиков температуры подшипников компрессора за 2 года."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Поиск паттернов в многомерных временных рядах для предотвращения отказов в технологическом оборудовании](#toc1_)    \n",
    "- [1 EDA](#toc2_)    \n",
    "- [2 Preprocessing](#toc3_)    \n",
    "- [3 Modelling](#toc4_)    \n",
    "  - [tsfresf + CatBoost](#toc4_1_)    \n",
    "  - [CNN](#toc4_2_)    \n",
    "    - [CNN](#toc4_2_1_)    \n",
    "    - [CNN на смещенном паттерне](#toc4_2_2_)    \n",
    "    - [CNN с масштабированием каждого окна](#toc4_2_3_)    \n",
    "  - [Vanilla Transformer](#toc4_3_)    \n",
    "    - [С масштабированием всей выборки](#toc4_3_1_)    \n",
    "    - [С масштабированием каждого окна](#toc4_3_2_)    \n",
    "  - [CNN + Transformer](#toc4_4_)    \n",
    "    - [С масштабированием всей выборки](#toc4_4_1_)    \n",
    "    - [CNN + Transformer с масштабированием каждого окна](#toc4_4_2_)    \n",
    "  - [Informer](#toc4_5_)    \n",
    "    - [С масштабированием всей выборки](#toc4_5_1_)    \n",
    "    - [Informer с масштабированием каждого окна](#toc4_5_2_)    \n",
    "  - [iTransformer](#toc4_6_)    \n",
    "    - [С масштабированием всей выборки](#toc4_6_1_)    \n",
    "    - [С масштабированием каждого окна](#toc4_6_2_)    \n",
    "- [4 Выводы](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1731251528831,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "Auv1JdpknRm1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from seaborn import heatmap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from   plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction.settings import MinimalFCParameters\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('Компрессор/X.csv', parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[1 EDA](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fy0CtN3WnRm3",
    "outputId": "31654afe-7837-4295-db6a-e80c8408a50b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>39.366243</td>\n",
       "      <td>39.042265</td>\n",
       "      <td>38.878460</td>\n",
       "      <td>38.814402</td>\n",
       "      <td>84.992254</td>\n",
       "      <td>84.822547</td>\n",
       "      <td>85.748036</td>\n",
       "      <td>86.139467</td>\n",
       "      <td>84.896086</td>\n",
       "      <td>85.096198</td>\n",
       "      <td>85.219893</td>\n",
       "      <td>85.124333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:10:00</th>\n",
       "      <td>39.306476</td>\n",
       "      <td>39.322175</td>\n",
       "      <td>38.834323</td>\n",
       "      <td>39.124901</td>\n",
       "      <td>84.978888</td>\n",
       "      <td>84.764294</td>\n",
       "      <td>85.768047</td>\n",
       "      <td>86.228679</td>\n",
       "      <td>85.133108</td>\n",
       "      <td>85.445578</td>\n",
       "      <td>85.176276</td>\n",
       "      <td>85.435617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:20:00</th>\n",
       "      <td>39.301215</td>\n",
       "      <td>39.319921</td>\n",
       "      <td>38.835075</td>\n",
       "      <td>39.130163</td>\n",
       "      <td>84.987167</td>\n",
       "      <td>84.765807</td>\n",
       "      <td>86.114947</td>\n",
       "      <td>86.226437</td>\n",
       "      <td>85.090743</td>\n",
       "      <td>85.414732</td>\n",
       "      <td>85.177789</td>\n",
       "      <td>85.438617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:30:00</th>\n",
       "      <td>39.205322</td>\n",
       "      <td>39.284548</td>\n",
       "      <td>38.848006</td>\n",
       "      <td>39.227044</td>\n",
       "      <td>85.145350</td>\n",
       "      <td>84.806108</td>\n",
       "      <td>85.970018</td>\n",
       "      <td>86.184978</td>\n",
       "      <td>84.291436</td>\n",
       "      <td>84.826922</td>\n",
       "      <td>85.214580</td>\n",
       "      <td>85.492154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:40:00</th>\n",
       "      <td>39.355293</td>\n",
       "      <td>39.518925</td>\n",
       "      <td>38.804020</td>\n",
       "      <td>39.108708</td>\n",
       "      <td>85.145861</td>\n",
       "      <td>85.099827</td>\n",
       "      <td>85.999919</td>\n",
       "      <td>86.258341</td>\n",
       "      <td>84.306880</td>\n",
       "      <td>84.882241</td>\n",
       "      <td>85.421010</td>\n",
       "      <td>85.359492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0          1          2          3          4  \\\n",
       "datetime                                                                     \n",
       "2022-01-01 00:00:00  39.366243  39.042265  38.878460  38.814402  84.992254   \n",
       "2022-01-01 00:10:00  39.306476  39.322175  38.834323  39.124901  84.978888   \n",
       "2022-01-01 00:20:00  39.301215  39.319921  38.835075  39.130163  84.987167   \n",
       "2022-01-01 00:30:00  39.205322  39.284548  38.848006  39.227044  85.145350   \n",
       "2022-01-01 00:40:00  39.355293  39.518925  38.804020  39.108708  85.145861   \n",
       "\n",
       "                             5          6          7          8          9  \\\n",
       "datetime                                                                     \n",
       "2022-01-01 00:00:00  84.822547  85.748036  86.139467  84.896086  85.096198   \n",
       "2022-01-01 00:10:00  84.764294  85.768047  86.228679  85.133108  85.445578   \n",
       "2022-01-01 00:20:00  84.765807  86.114947  86.226437  85.090743  85.414732   \n",
       "2022-01-01 00:30:00  84.806108  85.970018  86.184978  84.291436  84.826922   \n",
       "2022-01-01 00:40:00  85.099827  85.999919  86.258341  84.306880  84.882241   \n",
       "\n",
       "                            10         11  \n",
       "datetime                                   \n",
       "2022-01-01 00:00:00  85.219893  85.124333  \n",
       "2022-01-01 00:10:00  85.176276  85.435617  \n",
       "2022-01-01 00:20:00  85.177789  85.438617  \n",
       "2022-01-01 00:30:00  85.214580  85.492154  \n",
       "2022-01-01 00:40:00  85.421010  85.359492  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-19 23:10:00</th>\n",
       "      <td>22.295343</td>\n",
       "      <td>21.577818</td>\n",
       "      <td>21.596045</td>\n",
       "      <td>21.799421</td>\n",
       "      <td>48.395706</td>\n",
       "      <td>48.54356</td>\n",
       "      <td>46.96782</td>\n",
       "      <td>47.04134</td>\n",
       "      <td>47.358223</td>\n",
       "      <td>47.698696</td>\n",
       "      <td>47.689053</td>\n",
       "      <td>47.919636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-19 23:20:00</th>\n",
       "      <td>22.295343</td>\n",
       "      <td>21.577818</td>\n",
       "      <td>21.596045</td>\n",
       "      <td>21.799421</td>\n",
       "      <td>48.395706</td>\n",
       "      <td>48.54356</td>\n",
       "      <td>46.96782</td>\n",
       "      <td>47.04134</td>\n",
       "      <td>47.358223</td>\n",
       "      <td>47.698696</td>\n",
       "      <td>47.689053</td>\n",
       "      <td>47.919636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-19 23:30:00</th>\n",
       "      <td>22.295343</td>\n",
       "      <td>21.577818</td>\n",
       "      <td>21.596045</td>\n",
       "      <td>21.799421</td>\n",
       "      <td>48.395706</td>\n",
       "      <td>48.54356</td>\n",
       "      <td>46.96782</td>\n",
       "      <td>47.04134</td>\n",
       "      <td>47.358223</td>\n",
       "      <td>47.698696</td>\n",
       "      <td>47.689053</td>\n",
       "      <td>47.919636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-19 23:40:00</th>\n",
       "      <td>22.295343</td>\n",
       "      <td>21.577818</td>\n",
       "      <td>21.596045</td>\n",
       "      <td>21.799421</td>\n",
       "      <td>48.395706</td>\n",
       "      <td>48.54356</td>\n",
       "      <td>46.96782</td>\n",
       "      <td>47.04134</td>\n",
       "      <td>47.358223</td>\n",
       "      <td>47.698696</td>\n",
       "      <td>47.689053</td>\n",
       "      <td>47.919636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-19 23:50:00</th>\n",
       "      <td>22.295343</td>\n",
       "      <td>21.577818</td>\n",
       "      <td>21.596045</td>\n",
       "      <td>21.799421</td>\n",
       "      <td>48.395706</td>\n",
       "      <td>48.54356</td>\n",
       "      <td>46.96782</td>\n",
       "      <td>47.04134</td>\n",
       "      <td>47.358223</td>\n",
       "      <td>47.698696</td>\n",
       "      <td>47.689053</td>\n",
       "      <td>47.919636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0          1          2          3          4  \\\n",
       "datetime                                                                     \n",
       "2023-11-19 23:10:00  22.295343  21.577818  21.596045  21.799421  48.395706   \n",
       "2023-11-19 23:20:00  22.295343  21.577818  21.596045  21.799421  48.395706   \n",
       "2023-11-19 23:30:00  22.295343  21.577818  21.596045  21.799421  48.395706   \n",
       "2023-11-19 23:40:00  22.295343  21.577818  21.596045  21.799421  48.395706   \n",
       "2023-11-19 23:50:00  22.295343  21.577818  21.596045  21.799421  48.395706   \n",
       "\n",
       "                            5         6         7          8          9  \\\n",
       "datetime                                                                  \n",
       "2023-11-19 23:10:00  48.54356  46.96782  47.04134  47.358223  47.698696   \n",
       "2023-11-19 23:20:00  48.54356  46.96782  47.04134  47.358223  47.698696   \n",
       "2023-11-19 23:30:00  48.54356  46.96782  47.04134  47.358223  47.698696   \n",
       "2023-11-19 23:40:00  48.54356  46.96782  47.04134  47.358223  47.698696   \n",
       "2023-11-19 23:50:00  48.54356  46.96782  47.04134  47.358223  47.698696   \n",
       "\n",
       "                            10         11  \n",
       "datetime                                   \n",
       "2023-11-19 23:10:00  47.689053  47.919636  \n",
       "2023-11-19 23:20:00  47.689053  47.919636  \n",
       "2023-11-19 23:30:00  47.689053  47.919636  \n",
       "2023-11-19 23:40:00  47.689053  47.919636  \n",
       "2023-11-19 23:50:00  47.689053  47.919636  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.head())\n",
    "display(X.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j0j9zwoknRm3",
    "outputId": "fd2c64c3-6723-4fef-e605-344460e49a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 99072 entries, 2022-01-01 00:00:00 to 2023-11-19 23:50:00\n",
      "Data columns (total 12 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       99072 non-null  float64\n",
      " 1   1       99072 non-null  float64\n",
      " 2   2       99072 non-null  float64\n",
      " 3   3       99072 non-null  float64\n",
      " 4   4       99072 non-null  float64\n",
      " 5   5       99072 non-null  float64\n",
      " 6   6       99072 non-null  float64\n",
      " 7   7       99072 non-null  float64\n",
      " 8   8       99072 non-null  float64\n",
      " 9   9       99072 non-null  float64\n",
      " 10  10      99072 non-null  float64\n",
      " 11  11      99072 non-null  float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 9.8 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBZSQUvWnRm4"
   },
   "source": [
    "Пропусков нет.\n",
    "\n",
    "Дубликатов нет.\n",
    "\n",
    "Данные упорядочены.\n",
    "\n",
    "12 столбцов - температура подшипников.\n",
    "\n",
    "Данные ресемплированы по 10 мин.\n",
    "\n",
    "Данные взяты за период 2022-01-01 00:00:00 - 2023-11-19 23:50:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uKBl5L7gnRm4",
    "outputId": "ac792094-5558-4a0e-bcc4-1d7369fff50a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "      <td>99072.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.418240</td>\n",
       "      <td>31.428742</td>\n",
       "      <td>31.008759</td>\n",
       "      <td>31.677152</td>\n",
       "      <td>68.972505</td>\n",
       "      <td>68.770263</td>\n",
       "      <td>69.128971</td>\n",
       "      <td>69.287269</td>\n",
       "      <td>68.322175</td>\n",
       "      <td>68.688310</td>\n",
       "      <td>69.540378</td>\n",
       "      <td>69.589454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.734143</td>\n",
       "      <td>7.558409</td>\n",
       "      <td>7.764753</td>\n",
       "      <td>7.998436</td>\n",
       "      <td>24.987407</td>\n",
       "      <td>24.957804</td>\n",
       "      <td>25.072285</td>\n",
       "      <td>25.086779</td>\n",
       "      <td>24.981971</td>\n",
       "      <td>24.979657</td>\n",
       "      <td>25.547969</td>\n",
       "      <td>25.586881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.366222</td>\n",
       "      <td>2.990804</td>\n",
       "      <td>-0.335690</td>\n",
       "      <td>2.838214</td>\n",
       "      <td>4.608231</td>\n",
       "      <td>4.547192</td>\n",
       "      <td>4.730302</td>\n",
       "      <td>4.577709</td>\n",
       "      <td>4.562450</td>\n",
       "      <td>4.776081</td>\n",
       "      <td>5.020223</td>\n",
       "      <td>5.004963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.541471</td>\n",
       "      <td>29.480434</td>\n",
       "      <td>29.282069</td>\n",
       "      <td>29.465178</td>\n",
       "      <td>74.296180</td>\n",
       "      <td>74.113076</td>\n",
       "      <td>73.167019</td>\n",
       "      <td>73.304345</td>\n",
       "      <td>73.243321</td>\n",
       "      <td>73.624790</td>\n",
       "      <td>72.037840</td>\n",
       "      <td>72.068360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.600385</td>\n",
       "      <td>33.463042</td>\n",
       "      <td>33.279932</td>\n",
       "      <td>33.630886</td>\n",
       "      <td>79.072263</td>\n",
       "      <td>78.873885</td>\n",
       "      <td>79.087524</td>\n",
       "      <td>79.224846</td>\n",
       "      <td>78.645010</td>\n",
       "      <td>78.980707</td>\n",
       "      <td>79.316402</td>\n",
       "      <td>79.377418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.667440</td>\n",
       "      <td>36.636918</td>\n",
       "      <td>36.224918</td>\n",
       "      <td>37.277791</td>\n",
       "      <td>82.825987</td>\n",
       "      <td>82.642887</td>\n",
       "      <td>83.710994</td>\n",
       "      <td>83.970385</td>\n",
       "      <td>82.215611</td>\n",
       "      <td>82.658127</td>\n",
       "      <td>84.702834</td>\n",
       "      <td>84.855423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.988114</td>\n",
       "      <td>61.249710</td>\n",
       "      <td>64.789746</td>\n",
       "      <td>61.509090</td>\n",
       "      <td>135.698489</td>\n",
       "      <td>135.133898</td>\n",
       "      <td>128.786150</td>\n",
       "      <td>128.908220</td>\n",
       "      <td>129.274440</td>\n",
       "      <td>129.777980</td>\n",
       "      <td>130.571440</td>\n",
       "      <td>130.556180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  99072.000000  99072.000000  99072.000000  99072.000000  99072.000000   \n",
       "mean      31.418240     31.428742     31.008759     31.677152     68.972505   \n",
       "std        7.734143      7.558409      7.764753      7.998436     24.987407   \n",
       "min        0.366222      2.990804     -0.335690      2.838214      4.608231   \n",
       "25%       29.541471     29.480434     29.282069     29.465178     74.296180   \n",
       "50%       33.600385     33.463042     33.279932     33.630886     79.072263   \n",
       "75%       36.667440     36.636918     36.224918     37.277791     82.825987   \n",
       "max       64.988114     61.249710     64.789746     61.509090    135.698489   \n",
       "\n",
       "                  5             6             7             8             9  \\\n",
       "count  99072.000000  99072.000000  99072.000000  99072.000000  99072.000000   \n",
       "mean      68.770263     69.128971     69.287269     68.322175     68.688310   \n",
       "std       24.957804     25.072285     25.086779     24.981971     24.979657   \n",
       "min        4.547192      4.730302      4.577709      4.562450      4.776081   \n",
       "25%       74.113076     73.167019     73.304345     73.243321     73.624790   \n",
       "50%       78.873885     79.087524     79.224846     78.645010     78.980707   \n",
       "75%       82.642887     83.710994     83.970385     82.215611     82.658127   \n",
       "max      135.133898    128.786150    128.908220    129.274440    129.777980   \n",
       "\n",
       "                 10            11  \n",
       "count  99072.000000  99072.000000  \n",
       "mean      69.540378     69.589454  \n",
       "std       25.547969     25.586881  \n",
       "min        5.020223      5.004963  \n",
       "25%       72.037840     72.068360  \n",
       "50%       79.316402     79.377418  \n",
       "75%       84.702834     84.855423  \n",
       "max      130.571440    130.556180  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y1n5SEk9nRm4",
    "outputId": "3d9743aa-db56-471d-d97b-b9fe676ff7f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS5GL7t6nRm5"
   },
   "source": [
    "Визуализируем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kM_wLewMnRm5",
    "outputId": "7506aae6-9c40-4dab-abf2-0128ec3c49c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/data.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=1, vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}],], shared_xaxes = True)\n",
    "\n",
    "for col in X.columns:\n",
    "        fig.add_trace(go.Scattergl( x = X.index,\n",
    "                                    y = X[col].values,\n",
    "                                    mode = 'lines',\n",
    "                                    name = col,\n",
    "                                    showlegend =True),\n",
    "                                    row = 1, col = 1)\n",
    "\n",
    "plotly.offline.plot(fig, filename = \"graphs/data.html\",  show_link=False, auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных есть участки, когда оборудование не работало - офлайн-периоды. На этих участках делать расчет нет необходимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1731263402939,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "g8S6d2HznRm5"
   },
   "outputs": [],
   "source": [
    "offline_periods1 = [{'begin': '2022-01-11T20:30', 'end': '2022-01-14T10:10'},\n",
    " {'begin': '2022-01-14T10:30', 'end': '2022-01-14T15:10'},\n",
    " {'begin': '2022-02-18T13:20', 'end': '2022-02-18T19:50'},\n",
    " {'begin': '2022-02-20T07:20', 'end': '2022-02-25T05:10'},\n",
    " {'begin': '2022-04-25T09:50', 'end': '2022-04-28T05:20'},\n",
    " {'begin': '2022-05-16T05:50', 'end': '2022-05-20T05:30'},\n",
    " {'begin': '2022-06-05T01:20', 'end': '2022-06-05T02:30'},\n",
    " {'begin': '2022-06-05T04:50', 'end': '2022-06-05T05:30'},\n",
    " {'begin': '2022-07-12T06:20', 'end': '2022-07-14T10:00'},\n",
    " {'begin': '2022-08-03T07:50', 'end': '2022-08-05T12:00'},\n",
    " {'begin': '2022-08-07T21:40', 'end': '2022-08-07T22:40'},\n",
    " {'begin': '2022-08-17T03:50', 'end': '2022-08-19T15:40'},\n",
    " {'begin': '2022-11-02T07:40', 'end': '2022-11-03T15:00'},\n",
    " {'begin': '2022-11-09T20:10', 'end': '2022-11-10T13:50'},\n",
    " {'begin': '2022-11-11T11:10', 'end': '2022-11-18T04:50'},\n",
    " {'begin': '2022-12-03T08:20', 'end': '2023-01-09T23:00'},\n",
    " {'begin': '2023-01-23T05:50', 'end': '2023-02-02T12:20'},\n",
    " {'begin': '2023-02-15T05:40', 'end': '2023-02-21T02:40'},\n",
    " {'begin': '2023-08-26T00:40', 'end': '2023-08-26T02:20'},\n",
    " {'begin': '2023-09-22T08:20', 'end': '2023-09-25T06:00'},\n",
    " {'begin': '2023-10-10T18:40', 'end': '2023-10-10T19:00'},\n",
    " {'begin': '2023-10-11T06:20', 'end': '2023-10-11T22:40'},\n",
    " {'begin': '2023-10-12T04:40', 'end': '2023-11-18T12:30'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731263403612,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "4l7DVj-RnRm6"
   },
   "outputs": [],
   "source": [
    "offline_periods = []\n",
    "\n",
    "for period1 in offline_periods1:\n",
    "    period = {'begin': pd.to_datetime(period1['begin']) - pd.Timedelta('3h'),\n",
    "              'end': pd.to_datetime(period1['end']) - pd.Timedelta('3h')}\n",
    "    offline_periods.append(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1731263407605,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "B9mMHRq2nRm6"
   },
   "outputs": [],
   "source": [
    "X_states = X[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1731263409360,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "VJqhp-3bnRm6",
    "outputId": "c709c114-cc51-4fed-a9e2-fc4e1e9ca504"
   },
   "outputs": [],
   "source": [
    "X_states['offline'] = 0\n",
    "for period in offline_periods:\n",
    "    X_states.loc[period['begin']:period['end'],'offline'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем офлайн-периоды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aS_Q--FmnRm6",
    "outputId": "b2e098b4-611e-475c-9b7c-bd0748a819f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/data_offline.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=1, vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}],], shared_xaxes = True)\n",
    "\n",
    "for col in X.columns:\n",
    "        fig.add_trace(go.Scattergl( x = X.index,\n",
    "                                    y = X[col].values,\n",
    "                                    mode = 'lines',\n",
    "                                    name = col,\n",
    "                                    showlegend =True),\n",
    "                                    row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = X_states.index,\n",
    "                             y = X_states['offline'].values,\n",
    "                             name = f'offline',\n",
    "                             showlegend =True,\n",
    "                             fill='tozeroy',\n",
    "                             opacity=0.25,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             line={'width':1,'color':'black'},\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "plotly.offline.plot(fig, filename = \"graphs/data_offline.html\",  show_link=False, auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdkHJY-onRm6"
   },
   "source": [
    "Определим участки отказов с одинаковым паттерном - 5 дней до отключения оборудования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1731263413868,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "Qhbk-IdWnRm6"
   },
   "outputs": [],
   "source": [
    "green_failurs = [{'begin': '2022-02-15 07:10:00', 'end': '2022-02-20 04:10:00'},\n",
    "                 {'begin': '2022-04-20 09:40:00', 'end': '2022-04-25 06:40:00'},\n",
    "                 {'begin': '2022-08-12 03:40:00', 'end': '2022-08-17 00:40:00'},\n",
    "                 {'begin': '2023-09-17 08:10:00', 'end': '2023-09-22 05:10:00'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1731263414282,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "pxTelWm_nRm6",
    "outputId": "64d18cb1-7285-4f2b-a9e9-117aa705f4e0"
   },
   "outputs": [],
   "source": [
    "X_states['green_failurs'] = 0\n",
    "for period in green_failurs:\n",
    "    X_states.loc[period['begin']:period['end'],'green_failurs'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем размеченные участки отказов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/data_green_failurs.html'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=1, vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}],], shared_xaxes = True)\n",
    "\n",
    "for col in X.columns:\n",
    "        fig.add_trace(go.Scattergl( x = X.index,\n",
    "                                    y = X[col].values,\n",
    "                                    mode = 'lines',\n",
    "                                    name = col,\n",
    "                                    showlegend =True),\n",
    "                                    row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = X_states.index,\n",
    "                             y = X_states['offline'].values,\n",
    "                             name = f'offline',\n",
    "                             showlegend =True,\n",
    "                             fill='tozeroy',\n",
    "                             opacity=0.25,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             line={'width':1,'color':'black'},\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = X_states.index,\n",
    "                             y = X_states['green_failurs'].values,\n",
    "                             name = f'green_failurs',\n",
    "                             showlegend =True,\n",
    "                             fill='tozeroy',\n",
    "                             opacity=0.25,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             line={'width':1,'color':'green'},\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "plotly.offline.plot(fig, filename = \"graphs/data_green_failurs.html\",  show_link=False, auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[2 Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYLJmiGsnRm7"
   },
   "source": [
    "Для обучения возьмем данные за 2022 год, а для тестирования - за 2023. Весь год нормальной работы брать в обучающую выборку не целесообразно. Достаточно взять характерные участки, так как на тестовой выборке нормальные участки будут только примерно соответствовать тем, которые вошли в обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения возьмем только первый отказ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1731263421301,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "fpWtidtOnRm7"
   },
   "outputs": [],
   "source": [
    "failurs = green_failurs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Участки нормальной работы для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731263421568,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "FWZTAwoFnRm7"
   },
   "outputs": [],
   "source": [
    "train_periods = [{'begin': '2022-01-20 00:00:00', 'end': '2022-01-31 00:00:00'},\n",
    "    {'begin': '2022-03-17 00:00:00', 'end': '2022-03-30 00:00:00'},\n",
    "    {'begin': '2022-04-29 00:00:00', 'end': '2022-05-16 00:00:00'},\n",
    "    {'begin': '2022-06-13 00:00:00', 'end': '2022-06-29 00:00:00'},\n",
    "    {'begin': '2022-09-29 00:00:00', 'end': '2022-10-15 00:00:00'},\n",
    "    {'begin': '2022-11-06 00:00:00', 'end': '2022-11-09 17:00:00'},\n",
    "    {'begin': '2022-11-20 00:00:00', 'end': '2022-11-30 00:00:00'},]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfkmY0DrnRm8"
   },
   "source": [
    "1 Масштабирование всей выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1731259877903,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "MB3gH8gunRm8"
   },
   "outputs": [],
   "source": [
    "# обучение скалера и масштабирование всей выборки\n",
    "def scale_data(df, periods, failurs, standardize=True):\n",
    "    df_train = pd.DataFrame(columns=df.columns, index=[])\n",
    "    # добавление нормальных участков\n",
    "    for period in periods:\n",
    "        period_data = df[period['begin']:period['end']]\n",
    "        df_train = pd.concat([df_train, period_data])\n",
    "    # добавление отказов\n",
    "    for failur in failurs:\n",
    "        failur_data = df[failur['begin']:failur['end']]\n",
    "        df_train = pd.concat([df_train, failur_data])\n",
    "    if standardize:\n",
    "        s = StandardScaler()\n",
    "        s.fit(df_train)\n",
    "        df_scaled = pd.DataFrame(s.transform(df), index=df.index, columns=df.columns)\n",
    "    return df_scaled, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled, scaler = scale_data(X, train_periods, failurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Формирование окон обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731251752364,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "k-yxRRrgnRm8"
   },
   "outputs": [],
   "source": [
    "# разметка и добавление в список нормальных участков и отказов\n",
    "def create_train_list(df, periods, failurs):\n",
    "    # список участков, которые войдут в обучающую выборку\n",
    "    sample_list = []\n",
    "    # добавление нормальных участков\n",
    "    for period in periods:\n",
    "        period_data = df[period['begin']:period['end']]\n",
    "        # добавление таргета\n",
    "        period_data['failure'] = 0.0\n",
    "        period_data['normal'] = 1.0\n",
    "        sample_list.append(period_data)\n",
    "    # добавление отказов\n",
    "    for failur in failurs:\n",
    "        period_data = df[failur['begin']:failur['end']]\n",
    "        # добавление таргета\n",
    "        period_data['failure'] = 1.0\n",
    "        period_data['normal'] = 0.0\n",
    "        sample_list.append(period_data)\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_sequences(x, y, history_size):\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    index_cur_time = []\n",
    "\n",
    "    for i in range(len(x)-history_size):\n",
    "        # слайдим окном по датасету\n",
    "        input  = x.iloc[i:i+history_size, :].values\n",
    "        x_values.append(input)\n",
    "        output = y.iloc[i+history_size, :]\n",
    "        y_values.append(output)\n",
    "        index_cur_time.append(x.index[i+history_size])\n",
    "    return index_cur_time, np.array(x_values),  np.array(y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразование всех выборок из списка\n",
    "def list_to_train_sequenses(sample_list, win_size):\n",
    "    target= ['failure', 'normal']\n",
    "    index_cur_time_train = []\n",
    "    X = np.array([[[]]*(sample_list[0].shape[1]-2)]*win_size).reshape((0, win_size, (sample_list[0].shape[1]-2)))\n",
    "    Y = np.array([[],[]]).reshape((0, 2))\n",
    "    for _df in sample_list:\n",
    "        if _df.shape[0] < win_size:\n",
    "            continue\n",
    "        _index_cur_time_train, _X, _Y = to_train_sequences(_df.drop(columns=target), _df[target], history_size = win_size)\n",
    "        index_cur_time_train += _index_cur_time_train\n",
    "        X = np.concatenate((X,_X))\n",
    "        Y = np.concatenate((Y,_Y))\n",
    "    return index_cur_time_train, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перемешиваем\n",
    "def do_shuffle(index_cur_time, X_train, y_train):\n",
    "    X_train, y_train, index_cur_time = shuffle(X_train, y_train, index_cur_time, random_state=12345)\n",
    "    return index_cur_time, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_sample(X, train_periods, failurs, win_size=48):\n",
    "    # разметка и добавление в список нормальных участков и отказов\n",
    "    train_list = create_train_list(X, train_periods, failurs)\n",
    "    # преобразование всех выборок из списка\n",
    "    index_cur_time_train, X_train, Y_train = list_to_train_sequenses(train_list, win_size)\n",
    "    # перемешиваем\n",
    "    index_cur_time_train, X_train, Y_train = do_shuffle(index_cur_time_train, X_train, Y_train)\n",
    "    return index_cur_time_train, X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала используем немасштабированную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cur_time_train, X_train, Y_train = create_train_sample(X, train_periods, failurs, win_size=win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12812, 48, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Расчет весов классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(Y_train[:, 0])\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=Y_train[:, 0])\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.526939211976639, 1.0: 9.780152671755726}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Формирование окон тестовой выборки (без офлайн-участков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1731251754845,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "Zy5bxnSonRm8"
   },
   "outputs": [],
   "source": [
    "# удаление офлайн участков, создание списка оставшихся периодов\n",
    "def create_test_list(data, periods):\n",
    "    # список участков, которые войдут в тестовую выборку\n",
    "    data_list = []\n",
    "    # участок данных до первого удаляемого участка\n",
    "    data_period = data[:periods[0]['begin']]\n",
    "    # если он не пустой, то добавляем датафрейм в список\n",
    "    # (удаляемый участок мог удалиться при отсечении начала\n",
    "    # и конца обучающей выборки на предыдущем шаге)\n",
    "    if data_period.shape[0] > 0:\n",
    "        data_list.append(data_period)\n",
    "\n",
    "    # аналогичное сохранение всех участков между удаляемыми участками\n",
    "    if len(periods) > 1:\n",
    "        for i in range(len(periods)-1):\n",
    "            data_period = data[periods[i]['end']:periods[i+1]['begin']]\n",
    "            if data_period.shape[0] > 0:\n",
    "                data_list.append(data_period)\n",
    "\n",
    "    # сохранение последнего участка\n",
    "    data_period = data[periods[-1]['end']:]\n",
    "    if data_period.shape[0] > 0:\n",
    "        data_list.append(data_period)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731251762031,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "TgLYALIgnRnB"
   },
   "outputs": [],
   "source": [
    "def to_predict_sequences(x, history_size):\n",
    "    x_values = []\n",
    "    index_cur_time = []\n",
    "\n",
    "    for i in range(len(x)-history_size):\n",
    "        # слайдим окном по датасету\n",
    "        input  = x.iloc[i:i+history_size, :].values\n",
    "        x_values.append(input)\n",
    "        index_cur_time.append(x.index[i+history_size])\n",
    "    return index_cur_time, np.array(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1731251763955,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "VG0zTWBrnRnB"
   },
   "outputs": [],
   "source": [
    "# преобразование всех выборок из списка\n",
    "def list_to_predict_sequenses(sample_list, win_size):\n",
    "    index_cur_time = []\n",
    "    X = np.array([[[]]*(sample_list[0].shape[1])]*win_size).reshape((0, win_size, (sample_list[0].shape[1])))\n",
    "    for _df in sample_list:\n",
    "        if _df.shape[0] < win_size:\n",
    "            continue\n",
    "        _index_cur_time, _X = to_predict_sequences(_df, history_size = win_size)\n",
    "        index_cur_time += _index_cur_time\n",
    "        X = np.concatenate((X,_X))\n",
    "    return index_cur_time, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sample(X, offline_periods, win_size=48):\n",
    "    # создание списка участков без офлайн-участков\n",
    "    test_list = create_test_list(X, offline_periods)\n",
    "    # преобразование всех выборок из списка\n",
    "    index_cur_time_test, X_test = list_to_predict_sequenses(test_list, win_size)\n",
    "    return index_cur_time_test, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cur_time_test, X_test = create_test_sample(X, offline_periods, win_size=win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80002, 48, 12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Расчет метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем рассчитывать метрику качества на частичной разметке тестовой выборки, исключая обучающие участки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все нормальные участки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_periods = [{'begin': '2022-01-01 00:00:00', 'end': '2022-01-10 17:30:00'},\n",
    "    {'begin': '2022-01-15 07:10:00', 'end': '2022-02-01 04:10:00'},\n",
    "    {'begin': '2022-02-26 02:10:00', 'end': '2022-04-05 06:40:00'},\n",
    "    {'begin': '2022-04-29 02:20:00', 'end': '2022-05-15 02:50:00'},\n",
    "    {'begin': '2022-05-21 02:30:00', 'end': '2022-06-03 22:20:00'},\n",
    "    {'begin': '2022-06-06 02:30:00', 'end': '2022-07-11 03:20:00'},\n",
    "    {'begin': '2022-07-15 07:00:00', 'end': '2022-08-02 04:50:00'},\n",
    "    {'begin': '2022-08-20 12:40:00', 'end': '2022-11-01 04:40:00'},\n",
    "    {'begin': '2022-11-04 12:00:00', 'end': '2022-11-08 17:10:00'},\n",
    "    {'begin': '2022-11-19 01:50:00', 'end': '2022-12-02 05:20:00'},\n",
    "    {'begin': '2023-01-10 20:00:00', 'end': '2023-01-22 02:50:00'},\n",
    "    {'begin': '2023-02-03 09:20:00', 'end': '2023-02-14 02:40:00'},\n",
    "    {'begin': '2023-02-21 23:40:00', 'end': '2023-08-24 21:40:00'},\n",
    "    {'begin': '2023-09-26 03:00:00', 'end': '2023-10-09 15:40:00'},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'begin': '2022-02-15 07:10:00', 'end': '2022-02-20 04:10:00'},\n",
       " {'begin': '2022-04-20 09:40:00', 'end': '2022-04-25 06:40:00'},\n",
       " {'begin': '2022-08-12 03:40:00', 'end': '2022-08-17 00:40:00'},\n",
       " {'begin': '2023-09-17 08:10:00', 'end': '2023-09-22 05:10:00'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_failurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_states['Normal'] = 0\n",
    "for period in normal_periods:\n",
    "    X_states.loc[period['begin']:period['end'],'Normal'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(X_states, pred):\n",
    "    marks = X_states[(X_states['green_failurs'] + X_states['Normal'] > 0) & (X_states['offline'] == 0)][['green_failurs']]\n",
    "    marks = marks.join(pd.DataFrame(pred, index=index_cur_time_test, columns=['pred']), how='inner')\n",
    "    marks = marks.drop(index=index_cur_time_train, errors='ignore')\n",
    "\n",
    "    roc_auc = roc_auc_score(marks['green_failurs'], marks['pred'] > 0.5)\n",
    "    f1_score_cb = f1_score(marks['green_failurs'], marks['pred'] > 0.5)\n",
    "    mcc_cb = matthews_corrcoef(marks['green_failurs'], marks['pred'] > 0.5)\n",
    "    return roc_auc, f1_score_cb, mcc_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(columns=['roc_auc', 'f1', 'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_states['Normal_train'] = 0\n",
    "for period in train_periods:\n",
    "    X_states.loc[period['begin']:period['end'],'Normal_train'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'begin': '2022-02-15 07:10:00', 'end': '2022-02-20 04:10:00'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_states['Failurs_train'] = 0\n",
    "for period in failurs:\n",
    "    X_states.loc[period['begin']:period['end'],'Failurs_train'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем обучающие участки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/data_train.html'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=1, vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}],], shared_xaxes = True)\n",
    "\n",
    "for col in X.columns:\n",
    "        fig.add_trace(go.Scattergl( x = X.index,\n",
    "                                    y = X[col].values,\n",
    "                                    mode = 'lines',\n",
    "                                    name = col,\n",
    "                                    showlegend =True),\n",
    "                                    row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = X_states.index,\n",
    "                             y = X_states['offline'].values,\n",
    "                             name = f'offline',\n",
    "                             showlegend =True,\n",
    "                             fill='tozeroy',\n",
    "                             opacity=0.25,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             line={'width':1,'color':'black'},\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = X_states.index,\n",
    "                             y = X_states['Failurs_train'].values,\n",
    "                             name = f'Failurs_train',\n",
    "                             showlegend =True,\n",
    "                             fill='tozeroy',\n",
    "                             opacity=0.25,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             line={'width':1,'color':'red'},\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = X_states.index,\n",
    "                             y = X_states['Normal_train'].values,\n",
    "                             name = f'Normal_train',\n",
    "                             showlegend =True,\n",
    "                             fill='tozeroy',\n",
    "                             opacity=0.25,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             line={'width':1,'color':'green'},\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "plotly.offline.plot(fig, filename = \"graphs/data_train.html\",  show_link=False, auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[3 Modelling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[tsfresf + CatBoost](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем алгоритм CatBoost на признаках, сгенерированных с помощью библиотеки tsfresh. Для этого будем использовать немасштабированную выборку, сгенерированную выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "uRfw1Xd_nRnD"
   },
   "outputs": [],
   "source": [
    "# форматирование выборки для библиотеки tsfresh\n",
    "def tsfresh_formatting(array, index_cur_time_train):\n",
    "    tsfresh_df = pd.DataFrame(array)\n",
    "    time = []\n",
    "    id = []\n",
    "    for i, index_cur_time in enumerate(index_cur_time_train):\n",
    "        time_win = sorted([index_cur_time - pd.Timedelta(f'{10 * j}min') for j in range(48)])\n",
    "        time.extend(time_win)\n",
    "        id.extend([i]*48) \n",
    "    tsfresh_df['time'] = time\n",
    "    tsfresh_df['id'] = id\n",
    "    return tsfresh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "uspXGYMjnRnD"
   },
   "outputs": [],
   "source": [
    "tsfresh_df = tsfresh_formatting(X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2])), index_cur_time_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614976, 14)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsfresh_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "F4npfrwjnRnD"
   },
   "outputs": [],
   "source": [
    "def extract_tsfeatures(data, settings):\n",
    "    s_features = extract_features(\n",
    "        data,\n",
    "        column_id='id',\n",
    "        column_sort='time',\n",
    "        impute_function=impute,\n",
    "        default_fc_parameters=settings,\n",
    "        n_jobs=6,\n",
    "        disable_progressbar=True,\n",
    "        show_warnings=False\n",
    "        )\n",
    "    return s_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "h0RsywI3nRnD"
   },
   "outputs": [],
   "source": [
    "tsf_settings = MinimalFCParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4Jdg0x0rnRnD"
   },
   "outputs": [],
   "source": [
    "tsfresh_features = extract_tsfeatures(tsfresh_df, tsf_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Dw_XOgn6nRnE",
    "outputId": "f32a18f1-59cf-49c3-e37b-752331cffd73"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0__sum_values</th>\n",
       "      <th>0__median</th>\n",
       "      <th>0__mean</th>\n",
       "      <th>0__length</th>\n",
       "      <th>0__standard_deviation</th>\n",
       "      <th>0__variance</th>\n",
       "      <th>0__root_mean_square</th>\n",
       "      <th>0__maximum</th>\n",
       "      <th>0__absolute_maximum</th>\n",
       "      <th>0__minimum</th>\n",
       "      <th>...</th>\n",
       "      <th>11__sum_values</th>\n",
       "      <th>11__median</th>\n",
       "      <th>11__mean</th>\n",
       "      <th>11__length</th>\n",
       "      <th>11__standard_deviation</th>\n",
       "      <th>11__variance</th>\n",
       "      <th>11__root_mean_square</th>\n",
       "      <th>11__maximum</th>\n",
       "      <th>11__absolute_maximum</th>\n",
       "      <th>11__minimum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1737.071997</td>\n",
       "      <td>36.133365</td>\n",
       "      <td>36.189000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.279365</td>\n",
       "      <td>0.078045</td>\n",
       "      <td>36.190078</td>\n",
       "      <td>36.713207</td>\n",
       "      <td>36.713207</td>\n",
       "      <td>35.797671</td>\n",
       "      <td>...</td>\n",
       "      <td>4034.638116</td>\n",
       "      <td>83.932247</td>\n",
       "      <td>84.054961</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.285283</td>\n",
       "      <td>0.081386</td>\n",
       "      <td>84.055445</td>\n",
       "      <td>84.672313</td>\n",
       "      <td>84.672313</td>\n",
       "      <td>83.649961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1490.470948</td>\n",
       "      <td>31.036850</td>\n",
       "      <td>31.051478</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.374167</td>\n",
       "      <td>0.140001</td>\n",
       "      <td>31.053732</td>\n",
       "      <td>31.891348</td>\n",
       "      <td>31.891348</td>\n",
       "      <td>30.502792</td>\n",
       "      <td>...</td>\n",
       "      <td>3798.779621</td>\n",
       "      <td>79.056997</td>\n",
       "      <td>79.141242</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.359029</td>\n",
       "      <td>0.128902</td>\n",
       "      <td>79.142056</td>\n",
       "      <td>79.896239</td>\n",
       "      <td>79.896239</td>\n",
       "      <td>78.706052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1532.143360</td>\n",
       "      <td>32.150771</td>\n",
       "      <td>31.919653</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.579775</td>\n",
       "      <td>0.336139</td>\n",
       "      <td>31.924918</td>\n",
       "      <td>32.715341</td>\n",
       "      <td>32.715341</td>\n",
       "      <td>30.731674</td>\n",
       "      <td>...</td>\n",
       "      <td>3761.883073</td>\n",
       "      <td>78.538180</td>\n",
       "      <td>78.372564</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>0.278697</td>\n",
       "      <td>78.374342</td>\n",
       "      <td>79.087520</td>\n",
       "      <td>79.087520</td>\n",
       "      <td>76.951255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1742.413070</td>\n",
       "      <td>36.400389</td>\n",
       "      <td>36.300272</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.606225</td>\n",
       "      <td>2.579959</td>\n",
       "      <td>36.335791</td>\n",
       "      <td>38.529029</td>\n",
       "      <td>38.529029</td>\n",
       "      <td>33.386748</td>\n",
       "      <td>...</td>\n",
       "      <td>4155.001694</td>\n",
       "      <td>86.877263</td>\n",
       "      <td>86.562535</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.801387</td>\n",
       "      <td>3.244993</td>\n",
       "      <td>86.581277</td>\n",
       "      <td>88.990616</td>\n",
       "      <td>88.990616</td>\n",
       "      <td>83.115902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512.489555</td>\n",
       "      <td>31.464100</td>\n",
       "      <td>31.510199</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.764013</td>\n",
       "      <td>0.583716</td>\n",
       "      <td>31.519460</td>\n",
       "      <td>32.959488</td>\n",
       "      <td>32.959488</td>\n",
       "      <td>30.243386</td>\n",
       "      <td>...</td>\n",
       "      <td>3684.077110</td>\n",
       "      <td>76.623178</td>\n",
       "      <td>76.751606</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.928772</td>\n",
       "      <td>0.862617</td>\n",
       "      <td>76.757226</td>\n",
       "      <td>78.675519</td>\n",
       "      <td>78.675519</td>\n",
       "      <td>75.211730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12807</th>\n",
       "      <td>1782.604920</td>\n",
       "      <td>37.186239</td>\n",
       "      <td>37.137602</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.126812</td>\n",
       "      <td>0.016081</td>\n",
       "      <td>37.137819</td>\n",
       "      <td>37.415120</td>\n",
       "      <td>37.415120</td>\n",
       "      <td>36.865803</td>\n",
       "      <td>...</td>\n",
       "      <td>4262.974131</td>\n",
       "      <td>88.853291</td>\n",
       "      <td>88.811961</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.176410</td>\n",
       "      <td>0.031120</td>\n",
       "      <td>88.812136</td>\n",
       "      <td>89.173720</td>\n",
       "      <td>89.173720</td>\n",
       "      <td>88.319234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12808</th>\n",
       "      <td>1573.357838</td>\n",
       "      <td>32.715338</td>\n",
       "      <td>32.778288</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.159631</td>\n",
       "      <td>0.025482</td>\n",
       "      <td>32.778677</td>\n",
       "      <td>33.203641</td>\n",
       "      <td>33.203641</td>\n",
       "      <td>32.578024</td>\n",
       "      <td>...</td>\n",
       "      <td>3829.434849</td>\n",
       "      <td>79.781809</td>\n",
       "      <td>79.779893</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.259409</td>\n",
       "      <td>0.067293</td>\n",
       "      <td>79.780314</td>\n",
       "      <td>80.308235</td>\n",
       "      <td>80.308235</td>\n",
       "      <td>79.194334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12809</th>\n",
       "      <td>1418.020983</td>\n",
       "      <td>29.472787</td>\n",
       "      <td>29.542104</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.371043</td>\n",
       "      <td>0.137673</td>\n",
       "      <td>29.544434</td>\n",
       "      <td>30.243374</td>\n",
       "      <td>30.243374</td>\n",
       "      <td>29.068437</td>\n",
       "      <td>...</td>\n",
       "      <td>3585.198788</td>\n",
       "      <td>74.441109</td>\n",
       "      <td>74.691641</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.616791</td>\n",
       "      <td>0.380431</td>\n",
       "      <td>74.694188</td>\n",
       "      <td>75.806798</td>\n",
       "      <td>75.806798</td>\n",
       "      <td>73.716362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>1795.361349</td>\n",
       "      <td>37.430380</td>\n",
       "      <td>37.403361</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.183775</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>37.403813</td>\n",
       "      <td>37.766083</td>\n",
       "      <td>37.766083</td>\n",
       "      <td>37.109943</td>\n",
       "      <td>...</td>\n",
       "      <td>4287.372948</td>\n",
       "      <td>89.265273</td>\n",
       "      <td>89.320270</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.257274</td>\n",
       "      <td>0.066190</td>\n",
       "      <td>89.320640</td>\n",
       "      <td>89.845139</td>\n",
       "      <td>89.845139</td>\n",
       "      <td>88.776990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12811</th>\n",
       "      <td>1631.250671</td>\n",
       "      <td>34.203105</td>\n",
       "      <td>33.984389</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.339909</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>33.986089</td>\n",
       "      <td>34.485384</td>\n",
       "      <td>34.485384</td>\n",
       "      <td>33.356229</td>\n",
       "      <td>...</td>\n",
       "      <td>3934.691590</td>\n",
       "      <td>81.979094</td>\n",
       "      <td>81.972741</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.418796</td>\n",
       "      <td>0.175390</td>\n",
       "      <td>81.973811</td>\n",
       "      <td>82.551309</td>\n",
       "      <td>82.551309</td>\n",
       "      <td>81.101722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12812 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0__sum_values  0__median    0__mean  0__length  0__standard_deviation  \\\n",
       "0        1737.071997  36.133365  36.189000       48.0               0.279365   \n",
       "1        1490.470948  31.036850  31.051478       48.0               0.374167   \n",
       "2        1532.143360  32.150771  31.919653       48.0               0.579775   \n",
       "3        1742.413070  36.400389  36.300272       48.0               1.606225   \n",
       "4        1512.489555  31.464100  31.510199       48.0               0.764013   \n",
       "...              ...        ...        ...        ...                    ...   \n",
       "12807    1782.604920  37.186239  37.137602       48.0               0.126812   \n",
       "12808    1573.357838  32.715338  32.778288       48.0               0.159631   \n",
       "12809    1418.020983  29.472787  29.542104       48.0               0.371043   \n",
       "12810    1795.361349  37.430380  37.403361       48.0               0.183775   \n",
       "12811    1631.250671  34.203105  33.984389       48.0               0.339909   \n",
       "\n",
       "       0__variance  0__root_mean_square  0__maximum  0__absolute_maximum  \\\n",
       "0         0.078045            36.190078   36.713207            36.713207   \n",
       "1         0.140001            31.053732   31.891348            31.891348   \n",
       "2         0.336139            31.924918   32.715341            32.715341   \n",
       "3         2.579959            36.335791   38.529029            38.529029   \n",
       "4         0.583716            31.519460   32.959488            32.959488   \n",
       "...            ...                  ...         ...                  ...   \n",
       "12807     0.016081            37.137819   37.415120            37.415120   \n",
       "12808     0.025482            32.778677   33.203641            33.203641   \n",
       "12809     0.137673            29.544434   30.243374            30.243374   \n",
       "12810     0.033773            37.403813   37.766083            37.766083   \n",
       "12811     0.115538            33.986089   34.485384            34.485384   \n",
       "\n",
       "       0__minimum  ...  11__sum_values  11__median   11__mean  11__length  \\\n",
       "0       35.797671  ...     4034.638116   83.932247  84.054961        48.0   \n",
       "1       30.502792  ...     3798.779621   79.056997  79.141242        48.0   \n",
       "2       30.731674  ...     3761.883073   78.538180  78.372564        48.0   \n",
       "3       33.386748  ...     4155.001694   86.877263  86.562535        48.0   \n",
       "4       30.243386  ...     3684.077110   76.623178  76.751606        48.0   \n",
       "...           ...  ...             ...         ...        ...         ...   \n",
       "12807   36.865803  ...     4262.974131   88.853291  88.811961        48.0   \n",
       "12808   32.578024  ...     3829.434849   79.781809  79.779893        48.0   \n",
       "12809   29.068437  ...     3585.198788   74.441109  74.691641        48.0   \n",
       "12810   37.109943  ...     4287.372948   89.265273  89.320270        48.0   \n",
       "12811   33.356229  ...     3934.691590   81.979094  81.972741        48.0   \n",
       "\n",
       "       11__standard_deviation  11__variance  11__root_mean_square  \\\n",
       "0                    0.285283      0.081386             84.055445   \n",
       "1                    0.359029      0.128902             79.142056   \n",
       "2                    0.527917      0.278697             78.374342   \n",
       "3                    1.801387      3.244993             86.581277   \n",
       "4                    0.928772      0.862617             76.757226   \n",
       "...                       ...           ...                   ...   \n",
       "12807                0.176410      0.031120             88.812136   \n",
       "12808                0.259409      0.067293             79.780314   \n",
       "12809                0.616791      0.380431             74.694188   \n",
       "12810                0.257274      0.066190             89.320640   \n",
       "12811                0.418796      0.175390             81.973811   \n",
       "\n",
       "       11__maximum  11__absolute_maximum  11__minimum  \n",
       "0        84.672313             84.672313    83.649961  \n",
       "1        79.896239             79.896239    78.706052  \n",
       "2        79.087520             79.087520    76.951255  \n",
       "3        88.990616             88.990616    83.115902  \n",
       "4        78.675519             78.675519    75.211730  \n",
       "...            ...                   ...          ...  \n",
       "12807    89.173720             89.173720    88.319234  \n",
       "12808    80.308235             80.308235    79.194334  \n",
       "12809    75.806798             75.806798    73.716362  \n",
       "12810    89.845139             89.845139    88.776990  \n",
       "12811    82.551309             82.551309    81.101722  \n",
       "\n",
       "[12812 rows x 120 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsfresh_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_selection.selection import select_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим только те признаки, от которых зависит целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_features_selected = select_features(tsfresh_features, \n",
    "                                            pd.Series(Y_train[:,0]), \n",
    "                                            ml_task='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7__maximum</th>\n",
       "      <th>7__absolute_maximum</th>\n",
       "      <th>6__maximum</th>\n",
       "      <th>6__absolute_maximum</th>\n",
       "      <th>6__standard_deviation</th>\n",
       "      <th>6__variance</th>\n",
       "      <th>7__standard_deviation</th>\n",
       "      <th>7__variance</th>\n",
       "      <th>7__root_mean_square</th>\n",
       "      <th>6__root_mean_square</th>\n",
       "      <th>...</th>\n",
       "      <th>5__absolute_maximum</th>\n",
       "      <th>9__median</th>\n",
       "      <th>9__maximum</th>\n",
       "      <th>9__absolute_maximum</th>\n",
       "      <th>8__maximum</th>\n",
       "      <th>8__absolute_maximum</th>\n",
       "      <th>11__maximum</th>\n",
       "      <th>11__absolute_maximum</th>\n",
       "      <th>10__maximum</th>\n",
       "      <th>10__absolute_maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.009074</td>\n",
       "      <td>83.009074</td>\n",
       "      <td>82.825960</td>\n",
       "      <td>82.825960</td>\n",
       "      <td>0.243796</td>\n",
       "      <td>0.059437</td>\n",
       "      <td>0.237433</td>\n",
       "      <td>0.056375</td>\n",
       "      <td>82.529082</td>\n",
       "      <td>82.347265</td>\n",
       "      <td>...</td>\n",
       "      <td>82.185096</td>\n",
       "      <td>81.208529</td>\n",
       "      <td>82.017250</td>\n",
       "      <td>82.017250</td>\n",
       "      <td>81.910430</td>\n",
       "      <td>81.910430</td>\n",
       "      <td>84.672313</td>\n",
       "      <td>84.672313</td>\n",
       "      <td>84.641796</td>\n",
       "      <td>84.641796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.987790</td>\n",
       "      <td>79.987790</td>\n",
       "      <td>79.682619</td>\n",
       "      <td>79.682619</td>\n",
       "      <td>0.361879</td>\n",
       "      <td>0.130956</td>\n",
       "      <td>0.394493</td>\n",
       "      <td>0.155625</td>\n",
       "      <td>79.121561</td>\n",
       "      <td>78.981211</td>\n",
       "      <td>...</td>\n",
       "      <td>78.980699</td>\n",
       "      <td>77.943084</td>\n",
       "      <td>78.538179</td>\n",
       "      <td>78.538179</td>\n",
       "      <td>78.294038</td>\n",
       "      <td>78.294038</td>\n",
       "      <td>79.896239</td>\n",
       "      <td>79.896239</td>\n",
       "      <td>79.896238</td>\n",
       "      <td>79.896238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.796491</td>\n",
       "      <td>80.796491</td>\n",
       "      <td>80.491319</td>\n",
       "      <td>80.491319</td>\n",
       "      <td>0.600305</td>\n",
       "      <td>0.360366</td>\n",
       "      <td>0.632667</td>\n",
       "      <td>0.400267</td>\n",
       "      <td>79.546848</td>\n",
       "      <td>79.377798</td>\n",
       "      <td>...</td>\n",
       "      <td>78.492374</td>\n",
       "      <td>77.370879</td>\n",
       "      <td>77.973596</td>\n",
       "      <td>77.973596</td>\n",
       "      <td>77.683679</td>\n",
       "      <td>77.683679</td>\n",
       "      <td>79.087520</td>\n",
       "      <td>79.087520</td>\n",
       "      <td>78.797591</td>\n",
       "      <td>78.797591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86.533902</td>\n",
       "      <td>86.533902</td>\n",
       "      <td>86.305010</td>\n",
       "      <td>86.305010</td>\n",
       "      <td>1.859454</td>\n",
       "      <td>3.457571</td>\n",
       "      <td>1.891695</td>\n",
       "      <td>3.578509</td>\n",
       "      <td>84.265985</td>\n",
       "      <td>84.038025</td>\n",
       "      <td>...</td>\n",
       "      <td>87.800414</td>\n",
       "      <td>87.289241</td>\n",
       "      <td>89.204235</td>\n",
       "      <td>89.204235</td>\n",
       "      <td>88.929579</td>\n",
       "      <td>88.929579</td>\n",
       "      <td>88.990616</td>\n",
       "      <td>88.990616</td>\n",
       "      <td>88.990610</td>\n",
       "      <td>88.990610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.079329</td>\n",
       "      <td>80.079329</td>\n",
       "      <td>79.942012</td>\n",
       "      <td>79.942012</td>\n",
       "      <td>0.940637</td>\n",
       "      <td>0.884798</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.884828</td>\n",
       "      <td>78.241839</td>\n",
       "      <td>78.056836</td>\n",
       "      <td>...</td>\n",
       "      <td>78.202483</td>\n",
       "      <td>75.700010</td>\n",
       "      <td>77.500568</td>\n",
       "      <td>77.500568</td>\n",
       "      <td>77.134359</td>\n",
       "      <td>77.134359</td>\n",
       "      <td>78.675519</td>\n",
       "      <td>78.675519</td>\n",
       "      <td>78.431362</td>\n",
       "      <td>78.431362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12807</th>\n",
       "      <td>83.543149</td>\n",
       "      <td>83.543149</td>\n",
       "      <td>83.588929</td>\n",
       "      <td>83.588929</td>\n",
       "      <td>0.206995</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.180545</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>83.280759</td>\n",
       "      <td>83.076094</td>\n",
       "      <td>...</td>\n",
       "      <td>83.711000</td>\n",
       "      <td>83.039601</td>\n",
       "      <td>83.298995</td>\n",
       "      <td>83.298995</td>\n",
       "      <td>83.039597</td>\n",
       "      <td>83.039597</td>\n",
       "      <td>89.173720</td>\n",
       "      <td>89.173720</td>\n",
       "      <td>89.188971</td>\n",
       "      <td>89.188971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12808</th>\n",
       "      <td>78.477130</td>\n",
       "      <td>78.477130</td>\n",
       "      <td>78.080403</td>\n",
       "      <td>78.080403</td>\n",
       "      <td>0.297771</td>\n",
       "      <td>0.088667</td>\n",
       "      <td>0.337485</td>\n",
       "      <td>0.113896</td>\n",
       "      <td>77.750535</td>\n",
       "      <td>77.490021</td>\n",
       "      <td>...</td>\n",
       "      <td>79.377413</td>\n",
       "      <td>80.689688</td>\n",
       "      <td>81.193255</td>\n",
       "      <td>81.193255</td>\n",
       "      <td>80.827035</td>\n",
       "      <td>80.827035</td>\n",
       "      <td>80.308235</td>\n",
       "      <td>80.308235</td>\n",
       "      <td>80.552361</td>\n",
       "      <td>80.552361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12809</th>\n",
       "      <td>77.973599</td>\n",
       "      <td>77.973599</td>\n",
       "      <td>77.744710</td>\n",
       "      <td>77.744710</td>\n",
       "      <td>0.747354</td>\n",
       "      <td>0.558539</td>\n",
       "      <td>0.799274</td>\n",
       "      <td>0.638839</td>\n",
       "      <td>76.459826</td>\n",
       "      <td>76.281924</td>\n",
       "      <td>...</td>\n",
       "      <td>75.745789</td>\n",
       "      <td>73.968109</td>\n",
       "      <td>75.608457</td>\n",
       "      <td>75.608457</td>\n",
       "      <td>75.089619</td>\n",
       "      <td>75.089619</td>\n",
       "      <td>75.806798</td>\n",
       "      <td>75.806798</td>\n",
       "      <td>75.654234</td>\n",
       "      <td>75.654234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>84.351875</td>\n",
       "      <td>84.351875</td>\n",
       "      <td>84.168751</td>\n",
       "      <td>84.168751</td>\n",
       "      <td>0.252783</td>\n",
       "      <td>0.063899</td>\n",
       "      <td>0.252456</td>\n",
       "      <td>0.063734</td>\n",
       "      <td>83.797526</td>\n",
       "      <td>83.620143</td>\n",
       "      <td>...</td>\n",
       "      <td>84.107728</td>\n",
       "      <td>83.527892</td>\n",
       "      <td>84.107727</td>\n",
       "      <td>84.107727</td>\n",
       "      <td>83.527889</td>\n",
       "      <td>83.527889</td>\n",
       "      <td>89.845139</td>\n",
       "      <td>89.845139</td>\n",
       "      <td>89.814588</td>\n",
       "      <td>89.814588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12811</th>\n",
       "      <td>80.460817</td>\n",
       "      <td>80.460817</td>\n",
       "      <td>80.231936</td>\n",
       "      <td>80.231936</td>\n",
       "      <td>0.442368</td>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.409934</td>\n",
       "      <td>0.168046</td>\n",
       "      <td>79.599111</td>\n",
       "      <td>79.373896</td>\n",
       "      <td>...</td>\n",
       "      <td>81.086394</td>\n",
       "      <td>82.177471</td>\n",
       "      <td>82.703897</td>\n",
       "      <td>82.703897</td>\n",
       "      <td>82.261388</td>\n",
       "      <td>82.261388</td>\n",
       "      <td>82.551309</td>\n",
       "      <td>82.551309</td>\n",
       "      <td>82.566564</td>\n",
       "      <td>82.566564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12812 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       7__maximum  7__absolute_maximum  6__maximum  6__absolute_maximum  \\\n",
       "0       83.009074            83.009074   82.825960            82.825960   \n",
       "1       79.987790            79.987790   79.682619            79.682619   \n",
       "2       80.796491            80.796491   80.491319            80.491319   \n",
       "3       86.533902            86.533902   86.305010            86.305010   \n",
       "4       80.079329            80.079329   79.942012            79.942012   \n",
       "...           ...                  ...         ...                  ...   \n",
       "12807   83.543149            83.543149   83.588929            83.588929   \n",
       "12808   78.477130            78.477130   78.080403            78.080403   \n",
       "12809   77.973599            77.973599   77.744710            77.744710   \n",
       "12810   84.351875            84.351875   84.168751            84.168751   \n",
       "12811   80.460817            80.460817   80.231936            80.231936   \n",
       "\n",
       "       6__standard_deviation  6__variance  7__standard_deviation  7__variance  \\\n",
       "0                   0.243796     0.059437               0.237433     0.056375   \n",
       "1                   0.361879     0.130956               0.394493     0.155625   \n",
       "2                   0.600305     0.360366               0.632667     0.400267   \n",
       "3                   1.859454     3.457571               1.891695     3.578509   \n",
       "4                   0.940637     0.884798               0.940653     0.884828   \n",
       "...                      ...          ...                    ...          ...   \n",
       "12807               0.206995     0.042847               0.180545     0.032597   \n",
       "12808               0.297771     0.088667               0.337485     0.113896   \n",
       "12809               0.747354     0.558539               0.799274     0.638839   \n",
       "12810               0.252783     0.063899               0.252456     0.063734   \n",
       "12811               0.442368     0.195689               0.409934     0.168046   \n",
       "\n",
       "       7__root_mean_square  6__root_mean_square  ...  5__absolute_maximum  \\\n",
       "0                82.529082            82.347265  ...            82.185096   \n",
       "1                79.121561            78.981211  ...            78.980699   \n",
       "2                79.546848            79.377798  ...            78.492374   \n",
       "3                84.265985            84.038025  ...            87.800414   \n",
       "4                78.241839            78.056836  ...            78.202483   \n",
       "...                    ...                  ...  ...                  ...   \n",
       "12807            83.280759            83.076094  ...            83.711000   \n",
       "12808            77.750535            77.490021  ...            79.377413   \n",
       "12809            76.459826            76.281924  ...            75.745789   \n",
       "12810            83.797526            83.620143  ...            84.107728   \n",
       "12811            79.599111            79.373896  ...            81.086394   \n",
       "\n",
       "       9__median  9__maximum  9__absolute_maximum  8__maximum  \\\n",
       "0      81.208529   82.017250            82.017250   81.910430   \n",
       "1      77.943084   78.538179            78.538179   78.294038   \n",
       "2      77.370879   77.973596            77.973596   77.683679   \n",
       "3      87.289241   89.204235            89.204235   88.929579   \n",
       "4      75.700010   77.500568            77.500568   77.134359   \n",
       "...          ...         ...                  ...         ...   \n",
       "12807  83.039601   83.298995            83.298995   83.039597   \n",
       "12808  80.689688   81.193255            81.193255   80.827035   \n",
       "12809  73.968109   75.608457            75.608457   75.089619   \n",
       "12810  83.527892   84.107727            84.107727   83.527889   \n",
       "12811  82.177471   82.703897            82.703897   82.261388   \n",
       "\n",
       "       8__absolute_maximum  11__maximum  11__absolute_maximum  10__maximum  \\\n",
       "0                81.910430    84.672313             84.672313    84.641796   \n",
       "1                78.294038    79.896239             79.896239    79.896238   \n",
       "2                77.683679    79.087520             79.087520    78.797591   \n",
       "3                88.929579    88.990616             88.990616    88.990610   \n",
       "4                77.134359    78.675519             78.675519    78.431362   \n",
       "...                    ...          ...                   ...          ...   \n",
       "12807            83.039597    89.173720             89.173720    89.188971   \n",
       "12808            80.827035    80.308235             80.308235    80.552361   \n",
       "12809            75.089619    75.806798             75.806798    75.654234   \n",
       "12810            83.527889    89.845139             89.845139    89.814588   \n",
       "12811            82.261388    82.551309             82.551309    82.566564   \n",
       "\n",
       "       10__absolute_maximum  \n",
       "0                 84.641796  \n",
       "1                 79.896238  \n",
       "2                 78.797591  \n",
       "3                 88.990610  \n",
       "4                 78.431362  \n",
       "...                     ...  \n",
       "12807             89.188971  \n",
       "12808             80.552361  \n",
       "12809             75.654234  \n",
       "12810             89.814588  \n",
       "12811             82.566564  \n",
       "\n",
       "[12812 rows x 80 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsfresh_features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "TQ2XjfXFnRnE"
   },
   "outputs": [],
   "source": [
    "# параметры для обучения\n",
    "\n",
    "params_grid = {\n",
    "    \"n_estimators\": [200, 300, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [2, 3, 5],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_model = CatBoostClassifier(random_state=42, verbose=200, auto_class_weights='Balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4014825b6c2e433c8c5ea5c9d4faebfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6730814\ttest: 0.6731844\tbest: 0.6731844 (0)\ttotal: 163ms\tremaining: 32.3s\n",
      "199:\tlearn: 0.0110581\ttest: 0.0110146\tbest: 0.0110146 (199)\ttotal: 1.61s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.01101458393\n",
      "bestIteration = 199\n",
      "\n",
      "0:\tloss: 0.0110146\tbest: 0.0110146 (0)\ttotal: 1.71s\tremaining: 44.4s\n",
      "0:\tlearn: 0.5977760\ttest: 0.5983193\tbest: 0.5983193 (0)\ttotal: 6.32ms\tremaining: 1.26s\n",
      "199:\tlearn: 0.0003055\ttest: 0.0003148\tbest: 0.0003148 (199)\ttotal: 1.44s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003147739732\n",
      "bestIteration = 199\n",
      "\n",
      "1:\tloss: 0.0003148\tbest: 0.0003148 (1)\ttotal: 3.15s\tremaining: 39.4s\n",
      "0:\tlearn: 0.5145952\ttest: 0.5157213\tbest: 0.5157213 (0)\ttotal: 5.79ms\tremaining: 1.15s\n",
      "199:\tlearn: 0.0004245\ttest: 0.0004465\tbest: 0.0004465 (197)\ttotal: 1.2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004464810075\n",
      "bestIteration = 197\n",
      "\n",
      "2:\tloss: 0.0004465\tbest: 0.0003148 (1)\ttotal: 4.37s\tremaining: 34.9s\n",
      "0:\tlearn: 0.6730814\ttest: 0.6731844\tbest: 0.6731844 (0)\ttotal: 8.18ms\tremaining: 2.45s\n",
      "200:\tlearn: 0.0109737\ttest: 0.0109438\tbest: 0.0109438 (200)\ttotal: 1.45s\tremaining: 713ms\n",
      "299:\tlearn: 0.0039024\ttest: 0.0039151\tbest: 0.0039151 (299)\ttotal: 1.99s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.003915112237\n",
      "bestIteration = 299\n",
      "\n",
      "3:\tloss: 0.0039151\tbest: 0.0003148 (1)\ttotal: 6.37s\tremaining: 36.6s\n",
      "0:\tlearn: 0.5977760\ttest: 0.5983193\tbest: 0.5983193 (0)\ttotal: 5.49ms\tremaining: 1.64s\n",
      "200:\tlearn: 0.0003055\ttest: 0.0003148\tbest: 0.0003148 (199)\ttotal: 987ms\tremaining: 486ms\n",
      "299:\tlearn: 0.0003047\ttest: 0.0003139\tbest: 0.0003139 (280)\ttotal: 1.41s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003139381935\n",
      "bestIteration = 280\n",
      "\n",
      "4:\tloss: 0.0003139\tbest: 0.0003139 (4)\ttotal: 7.8s\tremaining: 34.3s\n",
      "0:\tlearn: 0.5145952\ttest: 0.5157213\tbest: 0.5157213 (0)\ttotal: 5.26ms\tremaining: 1.57s\n",
      "200:\tlearn: 0.0004245\ttest: 0.0004465\tbest: 0.0004465 (197)\ttotal: 976ms\tremaining: 481ms\n",
      "299:\tlearn: 0.0004242\ttest: 0.0004462\tbest: 0.0004462 (297)\ttotal: 1.38s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004462421058\n",
      "bestIteration = 297\n",
      "\n",
      "5:\tloss: 0.0004462\tbest: 0.0003139 (4)\ttotal: 9.19s\tremaining: 32.2s\n",
      "0:\tlearn: 0.6730814\ttest: 0.6731844\tbest: 0.6731844 (0)\ttotal: 4.82ms\tremaining: 2.4s\n",
      "200:\tlearn: 0.0109737\ttest: 0.0109438\tbest: 0.0109438 (200)\ttotal: 1.23s\tremaining: 1.83s\n",
      "400:\tlearn: 0.0018496\ttest: 0.0018662\tbest: 0.0018662 (400)\ttotal: 2.42s\tremaining: 597ms\n",
      "499:\tlearn: 0.0009162\ttest: 0.0009295\tbest: 0.0009295 (499)\ttotal: 3.11s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0009294761698\n",
      "bestIteration = 499\n",
      "\n",
      "6:\tloss: 0.0009295\tbest: 0.0003139 (4)\ttotal: 12.3s\tremaining: 35.2s\n",
      "0:\tlearn: 0.5977760\ttest: 0.5983193\tbest: 0.5983193 (0)\ttotal: 4.59ms\tremaining: 2.29s\n",
      "200:\tlearn: 0.0003055\ttest: 0.0003148\tbest: 0.0003148 (199)\ttotal: 1.1s\tremaining: 1.64s\n",
      "400:\tlearn: 0.0003047\ttest: 0.0003139\tbest: 0.0003139 (396)\ttotal: 1.95s\tremaining: 481ms\n",
      "499:\tlearn: 0.0003046\ttest: 0.0003139\tbest: 0.0003139 (499)\ttotal: 2.36s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003138590966\n",
      "bestIteration = 499\n",
      "\n",
      "7:\tloss: 0.0003139\tbest: 0.0003139 (7)\ttotal: 14.7s\tremaining: 34.9s\n",
      "0:\tlearn: 0.5145952\ttest: 0.5157213\tbest: 0.5157213 (0)\ttotal: 4.91ms\tremaining: 2.45s\n",
      "200:\tlearn: 0.0004245\ttest: 0.0004465\tbest: 0.0004465 (197)\ttotal: 919ms\tremaining: 1.37s\n",
      "400:\tlearn: 0.0004240\ttest: 0.0004459\tbest: 0.0004459 (396)\ttotal: 1.8s\tremaining: 445ms\n",
      "499:\tlearn: 0.0004238\ttest: 0.0004457\tbest: 0.0004457 (465)\ttotal: 2.21s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004456721922\n",
      "bestIteration = 465\n",
      "\n",
      "8:\tloss: 0.0004457\tbest: 0.0003139 (7)\ttotal: 16.9s\tremaining: 33.9s\n",
      "0:\tlearn: 0.6715769\ttest: 0.6717124\tbest: 0.6717124 (0)\ttotal: 6.83ms\tremaining: 1.36s\n",
      "199:\tlearn: 0.0055936\ttest: 0.0055961\tbest: 0.0055961 (199)\ttotal: 1.32s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.005596146272\n",
      "bestIteration = 199\n",
      "\n",
      "9:\tloss: 0.0055961\tbest: 0.0003139 (7)\ttotal: 18.3s\tremaining: 31s\n",
      "0:\tlearn: 0.5911569\ttest: 0.5918327\tbest: 0.5918327 (0)\ttotal: 6.11ms\tremaining: 1.22s\n",
      "199:\tlearn: 0.0002854\ttest: 0.0002907\tbest: 0.0002907 (199)\ttotal: 1.18s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.000290669123\n",
      "bestIteration = 199\n",
      "\n",
      "10:\tloss: 0.0002907\tbest: 0.0002907 (10)\ttotal: 19.5s\tremaining: 28.3s\n",
      "0:\tlearn: 0.5035048\ttest: 0.5048390\tbest: 0.5048390 (0)\ttotal: 5.88ms\tremaining: 1.17s\n",
      "199:\tlearn: 0.0003358\ttest: 0.0003321\tbest: 0.0003321 (199)\ttotal: 1.13s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.000332127371\n",
      "bestIteration = 199\n",
      "\n",
      "11:\tloss: 0.0003321\tbest: 0.0002907 (10)\ttotal: 20.6s\tremaining: 25.7s\n",
      "0:\tlearn: 0.6715769\ttest: 0.6717124\tbest: 0.6717124 (0)\ttotal: 5.95ms\tremaining: 1.78s\n",
      "200:\tlearn: 0.0055529\ttest: 0.0055573\tbest: 0.0055573 (200)\ttotal: 1.62s\tremaining: 800ms\n",
      "299:\tlearn: 0.0015828\ttest: 0.0015933\tbest: 0.0015933 (299)\ttotal: 2.4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.001593283898\n",
      "bestIteration = 299\n",
      "\n",
      "12:\tloss: 0.0015933\tbest: 0.0002907 (10)\ttotal: 23s\tremaining: 24.8s\n",
      "0:\tlearn: 0.5911569\ttest: 0.5918327\tbest: 0.5918327 (0)\ttotal: 15ms\tremaining: 4.48s\n",
      "200:\tlearn: 0.0002854\ttest: 0.0002907\tbest: 0.0002907 (200)\ttotal: 1.45s\tremaining: 712ms\n",
      "299:\tlearn: 0.0002847\ttest: 0.0002900\tbest: 0.0002900 (299)\ttotal: 1.97s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0002900095267\n",
      "bestIteration = 299\n",
      "\n",
      "13:\tloss: 0.0002900\tbest: 0.0002900 (13)\ttotal: 25s\tremaining: 23.2s\n",
      "0:\tlearn: 0.5035048\ttest: 0.5048390\tbest: 0.5048390 (0)\ttotal: 5.71ms\tremaining: 1.71s\n",
      "200:\tlearn: 0.0003358\ttest: 0.0003321\tbest: 0.0003321 (199)\ttotal: 1.08s\tremaining: 535ms\n",
      "299:\tlearn: 0.0003357\ttest: 0.0003320\tbest: 0.0003320 (287)\ttotal: 1.6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003319878166\n",
      "bestIteration = 287\n",
      "\n",
      "14:\tloss: 0.0003320\tbest: 0.0002900 (13)\ttotal: 26.6s\tremaining: 21.3s\n",
      "0:\tlearn: 0.6715769\ttest: 0.6717124\tbest: 0.6717124 (0)\ttotal: 6.76ms\tremaining: 3.37s\n",
      "200:\tlearn: 0.0055529\ttest: 0.0055573\tbest: 0.0055573 (200)\ttotal: 1.35s\tremaining: 2.01s\n",
      "400:\tlearn: 0.0006540\ttest: 0.0006600\tbest: 0.0006600 (400)\ttotal: 2.65s\tremaining: 654ms\n",
      "499:\tlearn: 0.0004004\ttest: 0.0004039\tbest: 0.0004039 (499)\ttotal: 3.27s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004038711926\n",
      "bestIteration = 499\n",
      "\n",
      "15:\tloss: 0.0004039\tbest: 0.0002900 (13)\ttotal: 29.9s\tremaining: 20.6s\n",
      "0:\tlearn: 0.5911569\ttest: 0.5918327\tbest: 0.5918327 (0)\ttotal: 7.79ms\tremaining: 3.89s\n",
      "200:\tlearn: 0.0002854\ttest: 0.0002907\tbest: 0.0002907 (200)\ttotal: 1.26s\tremaining: 1.87s\n",
      "400:\tlearn: 0.0002842\ttest: 0.0002896\tbest: 0.0002896 (400)\ttotal: 2.34s\tremaining: 577ms\n",
      "499:\tlearn: 0.0002840\ttest: 0.0002893\tbest: 0.0002893 (449)\ttotal: 2.86s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0002893082033\n",
      "bestIteration = 449\n",
      "\n",
      "16:\tloss: 0.0002893\tbest: 0.0002893 (16)\ttotal: 32.8s\tremaining: 19.3s\n",
      "0:\tlearn: 0.5035048\ttest: 0.5048390\tbest: 0.5048390 (0)\ttotal: 7.06ms\tremaining: 3.52s\n",
      "200:\tlearn: 0.0003358\ttest: 0.0003321\tbest: 0.0003321 (199)\ttotal: 1.1s\tremaining: 1.63s\n",
      "400:\tlearn: 0.0003357\ttest: 0.0003319\tbest: 0.0003319 (312)\ttotal: 2.19s\tremaining: 542ms\n",
      "499:\tlearn: 0.0003357\ttest: 0.0003319\tbest: 0.0003319 (312)\ttotal: 2.77s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003319489761\n",
      "bestIteration = 312\n",
      "\n",
      "17:\tloss: 0.0003319\tbest: 0.0002893 (16)\ttotal: 35.6s\tremaining: 17.8s\n",
      "0:\tlearn: 0.6607824\ttest: 0.6607643\tbest: 0.6607643 (0)\ttotal: 11.5ms\tremaining: 2.29s\n",
      "199:\tlearn: 0.0022363\ttest: 0.0022719\tbest: 0.0022719 (199)\ttotal: 2.31s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.002271935653\n",
      "bestIteration = 199\n",
      "\n",
      "18:\tloss: 0.0022719\tbest: 0.0002893 (16)\ttotal: 37.9s\tremaining: 16s\n",
      "0:\tlearn: 0.5431431\ttest: 0.5430358\tbest: 0.5430358 (0)\ttotal: 11.4ms\tremaining: 2.27s\n",
      "199:\tlearn: 0.0003250\ttest: 0.0003181\tbest: 0.0003181 (199)\ttotal: 1.91s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003180647109\n",
      "bestIteration = 199\n",
      "\n",
      "19:\tloss: 0.0003181\tbest: 0.0002893 (16)\ttotal: 39.8s\tremaining: 13.9s\n",
      "0:\tlearn: 0.4217343\ttest: 0.4214709\tbest: 0.4214709 (0)\ttotal: 11ms\tremaining: 2.19s\n",
      "199:\tlearn: 0.0003096\ttest: 0.0003113\tbest: 0.0003113 (190)\ttotal: 2.01s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003113498585\n",
      "bestIteration = 190\n",
      "\n",
      "20:\tloss: 0.0003113\tbest: 0.0002893 (16)\ttotal: 41.9s\tremaining: 12s\n",
      "0:\tlearn: 0.6607824\ttest: 0.6607643\tbest: 0.6607643 (0)\ttotal: 14.1ms\tremaining: 4.21s\n",
      "200:\tlearn: 0.0022048\ttest: 0.0022392\tbest: 0.0022392 (200)\ttotal: 2.39s\tremaining: 1.18s\n",
      "299:\tlearn: 0.0007935\ttest: 0.0008072\tbest: 0.0008072 (299)\ttotal: 3.43s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0008071562723\n",
      "bestIteration = 299\n",
      "\n",
      "21:\tloss: 0.0008072\tbest: 0.0002893 (16)\ttotal: 45.3s\tremaining: 10.3s\n",
      "0:\tlearn: 0.5431431\ttest: 0.5430358\tbest: 0.5430358 (0)\ttotal: 10.6ms\tremaining: 3.16s\n",
      "200:\tlearn: 0.0003250\ttest: 0.0003181\tbest: 0.0003181 (200)\ttotal: 2.04s\tremaining: 1s\n",
      "299:\tlearn: 0.0003062\ttest: 0.0002993\tbest: 0.0002993 (299)\ttotal: 3.07s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0002993209095\n",
      "bestIteration = 299\n",
      "\n",
      "22:\tloss: 0.0002993\tbest: 0.0002893 (16)\ttotal: 48.4s\tremaining: 8.42s\n",
      "0:\tlearn: 0.4217343\ttest: 0.4214709\tbest: 0.4214709 (0)\ttotal: 14.4ms\tremaining: 4.3s\n",
      "200:\tlearn: 0.0003096\ttest: 0.0003113\tbest: 0.0003113 (190)\ttotal: 1.91s\tremaining: 940ms\n",
      "299:\tlearn: 0.0003092\ttest: 0.0003110\tbest: 0.0003110 (297)\ttotal: 2.78s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003110183952\n",
      "bestIteration = 297\n",
      "\n",
      "23:\tloss: 0.0003110\tbest: 0.0002893 (16)\ttotal: 51.2s\tremaining: 6.4s\n",
      "0:\tlearn: 0.6607824\ttest: 0.6607643\tbest: 0.6607643 (0)\ttotal: 11ms\tremaining: 5.5s\n",
      "200:\tlearn: 0.0022048\ttest: 0.0022392\tbest: 0.0022392 (200)\ttotal: 2.09s\tremaining: 3.11s\n",
      "400:\tlearn: 0.0004800\ttest: 0.0004850\tbest: 0.0004850 (400)\ttotal: 4.07s\tremaining: 1s\n",
      "499:\tlearn: 0.0004189\ttest: 0.0004231\tbest: 0.0004231 (499)\ttotal: 4.96s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004230617195\n",
      "bestIteration = 499\n",
      "\n",
      "24:\tloss: 0.0004231\tbest: 0.0002893 (16)\ttotal: 56.2s\tremaining: 4.5s\n",
      "0:\tlearn: 0.5431431\ttest: 0.5430358\tbest: 0.5430358 (0)\ttotal: 11.2ms\tremaining: 5.6s\n",
      "200:\tlearn: 0.0003250\ttest: 0.0003181\tbest: 0.0003181 (200)\ttotal: 1.95s\tremaining: 2.9s\n",
      "400:\tlearn: 0.0003061\ttest: 0.0002993\tbest: 0.0002993 (400)\ttotal: 4.01s\tremaining: 991ms\n",
      "499:\tlearn: 0.0003059\ttest: 0.0002991\tbest: 0.0002991 (499)\ttotal: 5.07s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0002990639246\n",
      "bestIteration = 499\n",
      "\n",
      "25:\tloss: 0.0002991\tbest: 0.0002893 (16)\ttotal: 1m 1s\tremaining: 2.36s\n",
      "0:\tlearn: 0.4217343\ttest: 0.4214709\tbest: 0.4214709 (0)\ttotal: 9.56ms\tremaining: 4.77s\n",
      "200:\tlearn: 0.0003096\ttest: 0.0003113\tbest: 0.0003113 (190)\ttotal: 1.87s\tremaining: 2.78s\n",
      "400:\tlearn: 0.0003089\ttest: 0.0003107\tbest: 0.0003107 (400)\ttotal: 3.78s\tremaining: 933ms\n",
      "499:\tlearn: 0.0003088\ttest: 0.0003106\tbest: 0.0003106 (448)\ttotal: 4.63s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003105646102\n",
      "bestIteration = 448\n",
      "\n",
      "26:\tloss: 0.0003106\tbest: 0.0002893 (16)\ttotal: 1m 5s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/5]\n",
      "0:\tlearn: 0.5658561\ttest: 0.5656581\tbest: 0.5656581 (0)\ttotal: 6.05ms\tremaining: 3.02s\n",
      "200:\tlearn: 0.0002971\ttest: 0.0004798\tbest: 0.0004798 (200)\ttotal: 1.17s\tremaining: 1.73s\n",
      "400:\tlearn: 0.0002961\ttest: 0.0004777\tbest: 0.0004777 (380)\ttotal: 2.15s\tremaining: 531ms\n",
      "499:\tlearn: 0.0002961\ttest: 0.0004777\tbest: 0.0004777 (380)\ttotal: 2.66s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004777471209\n",
      "bestIteration = 380\n",
      "\n",
      "Training on fold [1/5]\n",
      "0:\tlearn: 0.5658889\ttest: 0.5660454\tbest: 0.5660454 (0)\ttotal: 6.22ms\tremaining: 3.1s\n",
      "200:\tlearn: 0.0003281\ttest: 0.0004012\tbest: 0.0004012 (199)\ttotal: 1.28s\tremaining: 1.9s\n",
      "400:\tlearn: 0.0003277\ttest: 0.0004006\tbest: 0.0004006 (379)\ttotal: 2.28s\tremaining: 562ms\n",
      "499:\tlearn: 0.0003277\ttest: 0.0004006\tbest: 0.0004006 (379)\ttotal: 2.79s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0004006485217\n",
      "bestIteration = 379\n",
      "\n",
      "Training on fold [2/5]\n",
      "0:\tlearn: 0.5685504\ttest: 0.5691099\tbest: 0.5691099 (0)\ttotal: 5.61ms\tremaining: 2.8s\n",
      "200:\tlearn: 0.0003061\ttest: 0.0003153\tbest: 0.0003153 (200)\ttotal: 1.16s\tremaining: 1.73s\n",
      "400:\tlearn: 0.0003051\ttest: 0.0003143\tbest: 0.0003143 (400)\ttotal: 2.21s\tremaining: 545ms\n",
      "499:\tlearn: 0.0003047\ttest: 0.0003140\tbest: 0.0003140 (499)\ttotal: 2.81s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003139684908\n",
      "bestIteration = 499\n",
      "\n",
      "Training on fold [3/5]\n",
      "0:\tlearn: 0.5650012\ttest: 0.5655720\tbest: 0.5655720 (0)\ttotal: 6.01ms\tremaining: 3s\n",
      "200:\tlearn: 0.0002978\ttest: 0.0002727\tbest: 0.0002727 (199)\ttotal: 1.27s\tremaining: 1.89s\n",
      "400:\tlearn: 0.0002972\ttest: 0.0002723\tbest: 0.0002723 (400)\ttotal: 2.37s\tremaining: 584ms\n",
      "499:\tlearn: 0.0002972\ttest: 0.0002722\tbest: 0.0002722 (497)\ttotal: 2.92s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0002722361348\n",
      "bestIteration = 497\n",
      "\n",
      "Training on fold [4/5]\n",
      "0:\tlearn: 0.5660958\ttest: 0.5660250\tbest: 0.5660250 (0)\ttotal: 10.1ms\tremaining: 5.05s\n",
      "200:\tlearn: 0.0003356\ttest: 0.0003380\tbest: 0.0003380 (199)\ttotal: 1.35s\tremaining: 2.01s\n",
      "400:\tlearn: 0.0003355\ttest: 0.0003376\tbest: 0.0003376 (272)\ttotal: 2.37s\tremaining: 585ms\n",
      "499:\tlearn: 0.0003355\ttest: 0.0003376\tbest: 0.0003376 (272)\ttotal: 2.88s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0003375836238\n",
      "bestIteration = 272\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'depth': 3, 'learning_rate': 0.05, 'iterations': 500},\n",
       " 'cv_results': defaultdict(list,\n",
       "             {'iterations': [0,\n",
       "               1,\n",
       "               2,\n",
       "               3,\n",
       "               4,\n",
       "               5,\n",
       "               6,\n",
       "               7,\n",
       "               8,\n",
       "               9,\n",
       "               10,\n",
       "               11,\n",
       "               12,\n",
       "               13,\n",
       "               14,\n",
       "               15,\n",
       "               16,\n",
       "               17,\n",
       "               18,\n",
       "               19,\n",
       "               20,\n",
       "               21,\n",
       "               22,\n",
       "               23,\n",
       "               24,\n",
       "               25,\n",
       "               26,\n",
       "               27,\n",
       "               28,\n",
       "               29,\n",
       "               30,\n",
       "               31,\n",
       "               32,\n",
       "               33,\n",
       "               34,\n",
       "               35,\n",
       "               36,\n",
       "               37,\n",
       "               38,\n",
       "               39,\n",
       "               40,\n",
       "               41,\n",
       "               42,\n",
       "               43,\n",
       "               44,\n",
       "               45,\n",
       "               46,\n",
       "               47,\n",
       "               48,\n",
       "               49,\n",
       "               50,\n",
       "               51,\n",
       "               52,\n",
       "               53,\n",
       "               54,\n",
       "               55,\n",
       "               56,\n",
       "               57,\n",
       "               58,\n",
       "               59,\n",
       "               60,\n",
       "               61,\n",
       "               62,\n",
       "               63,\n",
       "               64,\n",
       "               65,\n",
       "               66,\n",
       "               67,\n",
       "               68,\n",
       "               69,\n",
       "               70,\n",
       "               71,\n",
       "               72,\n",
       "               73,\n",
       "               74,\n",
       "               75,\n",
       "               76,\n",
       "               77,\n",
       "               78,\n",
       "               79,\n",
       "               80,\n",
       "               81,\n",
       "               82,\n",
       "               83,\n",
       "               84,\n",
       "               85,\n",
       "               86,\n",
       "               87,\n",
       "               88,\n",
       "               89,\n",
       "               90,\n",
       "               91,\n",
       "               92,\n",
       "               93,\n",
       "               94,\n",
       "               95,\n",
       "               96,\n",
       "               97,\n",
       "               98,\n",
       "               99,\n",
       "               100,\n",
       "               101,\n",
       "               102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               106,\n",
       "               107,\n",
       "               108,\n",
       "               109,\n",
       "               110,\n",
       "               111,\n",
       "               112,\n",
       "               113,\n",
       "               114,\n",
       "               115,\n",
       "               116,\n",
       "               117,\n",
       "               118,\n",
       "               119,\n",
       "               120,\n",
       "               121,\n",
       "               122,\n",
       "               123,\n",
       "               124,\n",
       "               125,\n",
       "               126,\n",
       "               127,\n",
       "               128,\n",
       "               129,\n",
       "               130,\n",
       "               131,\n",
       "               132,\n",
       "               133,\n",
       "               134,\n",
       "               135,\n",
       "               136,\n",
       "               137,\n",
       "               138,\n",
       "               139,\n",
       "               140,\n",
       "               141,\n",
       "               142,\n",
       "               143,\n",
       "               144,\n",
       "               145,\n",
       "               146,\n",
       "               147,\n",
       "               148,\n",
       "               149,\n",
       "               150,\n",
       "               151,\n",
       "               152,\n",
       "               153,\n",
       "               154,\n",
       "               155,\n",
       "               156,\n",
       "               157,\n",
       "               158,\n",
       "               159,\n",
       "               160,\n",
       "               161,\n",
       "               162,\n",
       "               163,\n",
       "               164,\n",
       "               165,\n",
       "               166,\n",
       "               167,\n",
       "               168,\n",
       "               169,\n",
       "               170,\n",
       "               171,\n",
       "               172,\n",
       "               173,\n",
       "               174,\n",
       "               175,\n",
       "               176,\n",
       "               177,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               181,\n",
       "               182,\n",
       "               183,\n",
       "               184,\n",
       "               185,\n",
       "               186,\n",
       "               187,\n",
       "               188,\n",
       "               189,\n",
       "               190,\n",
       "               191,\n",
       "               192,\n",
       "               193,\n",
       "               194,\n",
       "               195,\n",
       "               196,\n",
       "               197,\n",
       "               198,\n",
       "               199,\n",
       "               200,\n",
       "               201,\n",
       "               202,\n",
       "               203,\n",
       "               204,\n",
       "               205,\n",
       "               206,\n",
       "               207,\n",
       "               208,\n",
       "               209,\n",
       "               210,\n",
       "               211,\n",
       "               212,\n",
       "               213,\n",
       "               214,\n",
       "               215,\n",
       "               216,\n",
       "               217,\n",
       "               218,\n",
       "               219,\n",
       "               220,\n",
       "               221,\n",
       "               222,\n",
       "               223,\n",
       "               224,\n",
       "               225,\n",
       "               226,\n",
       "               227,\n",
       "               228,\n",
       "               229,\n",
       "               230,\n",
       "               231,\n",
       "               232,\n",
       "               233,\n",
       "               234,\n",
       "               235,\n",
       "               236,\n",
       "               237,\n",
       "               238,\n",
       "               239,\n",
       "               240,\n",
       "               241,\n",
       "               242,\n",
       "               243,\n",
       "               244,\n",
       "               245,\n",
       "               246,\n",
       "               247,\n",
       "               248,\n",
       "               249,\n",
       "               250,\n",
       "               251,\n",
       "               252,\n",
       "               253,\n",
       "               254,\n",
       "               255,\n",
       "               256,\n",
       "               257,\n",
       "               258,\n",
       "               259,\n",
       "               260,\n",
       "               261,\n",
       "               262,\n",
       "               263,\n",
       "               264,\n",
       "               265,\n",
       "               266,\n",
       "               267,\n",
       "               268,\n",
       "               269,\n",
       "               270,\n",
       "               271,\n",
       "               272,\n",
       "               273,\n",
       "               274,\n",
       "               275,\n",
       "               276,\n",
       "               277,\n",
       "               278,\n",
       "               279,\n",
       "               280,\n",
       "               281,\n",
       "               282,\n",
       "               283,\n",
       "               284,\n",
       "               285,\n",
       "               286,\n",
       "               287,\n",
       "               288,\n",
       "               289,\n",
       "               290,\n",
       "               291,\n",
       "               292,\n",
       "               293,\n",
       "               294,\n",
       "               295,\n",
       "               296,\n",
       "               297,\n",
       "               298,\n",
       "               299,\n",
       "               300,\n",
       "               301,\n",
       "               302,\n",
       "               303,\n",
       "               304,\n",
       "               305,\n",
       "               306,\n",
       "               307,\n",
       "               308,\n",
       "               309,\n",
       "               310,\n",
       "               311,\n",
       "               312,\n",
       "               313,\n",
       "               314,\n",
       "               315,\n",
       "               316,\n",
       "               317,\n",
       "               318,\n",
       "               319,\n",
       "               320,\n",
       "               321,\n",
       "               322,\n",
       "               323,\n",
       "               324,\n",
       "               325,\n",
       "               326,\n",
       "               327,\n",
       "               328,\n",
       "               329,\n",
       "               330,\n",
       "               331,\n",
       "               332,\n",
       "               333,\n",
       "               334,\n",
       "               335,\n",
       "               336,\n",
       "               337,\n",
       "               338,\n",
       "               339,\n",
       "               340,\n",
       "               341,\n",
       "               342,\n",
       "               343,\n",
       "               344,\n",
       "               345,\n",
       "               346,\n",
       "               347,\n",
       "               348,\n",
       "               349,\n",
       "               350,\n",
       "               351,\n",
       "               352,\n",
       "               353,\n",
       "               354,\n",
       "               355,\n",
       "               356,\n",
       "               357,\n",
       "               358,\n",
       "               359,\n",
       "               360,\n",
       "               361,\n",
       "               362,\n",
       "               363,\n",
       "               364,\n",
       "               365,\n",
       "               366,\n",
       "               367,\n",
       "               368,\n",
       "               369,\n",
       "               370,\n",
       "               371,\n",
       "               372,\n",
       "               373,\n",
       "               374,\n",
       "               375,\n",
       "               376,\n",
       "               377,\n",
       "               378,\n",
       "               379,\n",
       "               380,\n",
       "               381,\n",
       "               382,\n",
       "               383,\n",
       "               384,\n",
       "               385,\n",
       "               386,\n",
       "               387,\n",
       "               388,\n",
       "               389,\n",
       "               390,\n",
       "               391,\n",
       "               392,\n",
       "               393,\n",
       "               394,\n",
       "               395,\n",
       "               396,\n",
       "               397,\n",
       "               398,\n",
       "               399,\n",
       "               400,\n",
       "               401,\n",
       "               402,\n",
       "               403,\n",
       "               404,\n",
       "               405,\n",
       "               406,\n",
       "               407,\n",
       "               408,\n",
       "               409,\n",
       "               410,\n",
       "               411,\n",
       "               412,\n",
       "               413,\n",
       "               414,\n",
       "               415,\n",
       "               416,\n",
       "               417,\n",
       "               418,\n",
       "               419,\n",
       "               420,\n",
       "               421,\n",
       "               422,\n",
       "               423,\n",
       "               424,\n",
       "               425,\n",
       "               426,\n",
       "               427,\n",
       "               428,\n",
       "               429,\n",
       "               430,\n",
       "               431,\n",
       "               432,\n",
       "               433,\n",
       "               434,\n",
       "               435,\n",
       "               436,\n",
       "               437,\n",
       "               438,\n",
       "               439,\n",
       "               440,\n",
       "               441,\n",
       "               442,\n",
       "               443,\n",
       "               444,\n",
       "               445,\n",
       "               446,\n",
       "               447,\n",
       "               448,\n",
       "               449,\n",
       "               450,\n",
       "               451,\n",
       "               452,\n",
       "               453,\n",
       "               454,\n",
       "               455,\n",
       "               456,\n",
       "               457,\n",
       "               458,\n",
       "               459,\n",
       "               460,\n",
       "               461,\n",
       "               462,\n",
       "               463,\n",
       "               464,\n",
       "               465,\n",
       "               466,\n",
       "               467,\n",
       "               468,\n",
       "               469,\n",
       "               470,\n",
       "               471,\n",
       "               472,\n",
       "               473,\n",
       "               474,\n",
       "               475,\n",
       "               476,\n",
       "               477,\n",
       "               478,\n",
       "               479,\n",
       "               480,\n",
       "               481,\n",
       "               482,\n",
       "               483,\n",
       "               484,\n",
       "               485,\n",
       "               486,\n",
       "               487,\n",
       "               488,\n",
       "               489,\n",
       "               490,\n",
       "               491,\n",
       "               492,\n",
       "               493,\n",
       "               494,\n",
       "               495,\n",
       "               496,\n",
       "               497,\n",
       "               498,\n",
       "               499],\n",
       "              'test-Logloss-mean': [0.5664820773547179,\n",
       "               0.47422153765404057,\n",
       "               0.39128065869108725,\n",
       "               0.3053674583216305,\n",
       "               0.2425811520399237,\n",
       "               0.2069467251049189,\n",
       "               0.18033455907169244,\n",
       "               0.14558231728056087,\n",
       "               0.12249064923098207,\n",
       "               0.10683116389728567,\n",
       "               0.09073056031594971,\n",
       "               0.0765381667199513,\n",
       "               0.0696014643807879,\n",
       "               0.058713437825030135,\n",
       "               0.05110453418440279,\n",
       "               0.04453794121548347,\n",
       "               0.03878010759191093,\n",
       "               0.0345519593754255,\n",
       "               0.030022302438497135,\n",
       "               0.02686624726622896,\n",
       "               0.023619328141641065,\n",
       "               0.021159493856058097,\n",
       "               0.018763445383257183,\n",
       "               0.017193945292884646,\n",
       "               0.01596287575189591,\n",
       "               0.015002564163549675,\n",
       "               0.01318868327164088,\n",
       "               0.011687748293403178,\n",
       "               0.01041686796142205,\n",
       "               0.009598555104507356,\n",
       "               0.008960666046623114,\n",
       "               0.008500101964962481,\n",
       "               0.007955647188997337,\n",
       "               0.007252260120008564,\n",
       "               0.0065315058186374685,\n",
       "               0.005942083339041786,\n",
       "               0.005544127092938143,\n",
       "               0.00519429984364312,\n",
       "               0.004939315578830532,\n",
       "               0.0046750317867313715,\n",
       "               0.004382776019016182,\n",
       "               0.004126681010275521,\n",
       "               0.003830785103397296,\n",
       "               0.0036138832451429277,\n",
       "               0.0034450062181186473,\n",
       "               0.003276070037995513,\n",
       "               0.0030563525790372305,\n",
       "               0.002953392520577595,\n",
       "               0.002830034272080836,\n",
       "               0.0027224855237571806,\n",
       "               0.0026310544970196094,\n",
       "               0.0024628889560309596,\n",
       "               0.002370875017736654,\n",
       "               0.002266930228249901,\n",
       "               0.002133642283882343,\n",
       "               0.0020198266094011335,\n",
       "               0.0018976752569455431,\n",
       "               0.0017859886724171073,\n",
       "               0.0016838900488717018,\n",
       "               0.0016045288204534103,\n",
       "               0.0015224765506329705,\n",
       "               0.0014631041422477613,\n",
       "               0.0013827049092748338,\n",
       "               0.0013296364551042895,\n",
       "               0.0012977301587657568,\n",
       "               0.001243198598899623,\n",
       "               0.0011867265363663597,\n",
       "               0.001131121136248195,\n",
       "               0.0010906632520749685,\n",
       "               0.0010622945313073177,\n",
       "               0.0010340134187222538,\n",
       "               0.0010090595855201698,\n",
       "               0.0009814796735775956,\n",
       "               0.0009534699742934774,\n",
       "               0.0009247074641452557,\n",
       "               0.0009085719965429182,\n",
       "               0.0008738515095989661,\n",
       "               0.0008557403031672521,\n",
       "               0.000825639868999237,\n",
       "               0.000810007543923566,\n",
       "               0.0007802013903957554,\n",
       "               0.0007581166025430826,\n",
       "               0.000731248724927852,\n",
       "               0.0007170246842606147,\n",
       "               0.0007016238961267083,\n",
       "               0.0006827560730314777,\n",
       "               0.0006696291908084717,\n",
       "               0.0006573527392207661,\n",
       "               0.0006383994431528496,\n",
       "               0.0006210555529594809,\n",
       "               0.0006132737894461269,\n",
       "               0.0005908445570253257,\n",
       "               0.0005860504222866194,\n",
       "               0.000572692376356625,\n",
       "               0.0005619228188372157,\n",
       "               0.0005542581527321634,\n",
       "               0.0005441112756931851,\n",
       "               0.0005404063335847266,\n",
       "               0.0005277274141271257,\n",
       "               0.0005173036637352521,\n",
       "               0.000509075407293001,\n",
       "               0.0004954621439885374,\n",
       "               0.00048975002796604,\n",
       "               0.00047909254202789476,\n",
       "               0.00047696149321512844,\n",
       "               0.0004764713486414087,\n",
       "               0.0004709293796508553,\n",
       "               0.0004675257173803723,\n",
       "               0.0004560560641220084,\n",
       "               0.0004556745191945889,\n",
       "               0.0004556703274511958,\n",
       "               0.00045503877339841926,\n",
       "               0.0004520835302067289,\n",
       "               0.0004472781202300923,\n",
       "               0.0004468749779593001,\n",
       "               0.00044362011606551054,\n",
       "               0.00043796071443147415,\n",
       "               0.0004322191867550861,\n",
       "               0.00042621280278516704,\n",
       "               0.000424029125518376,\n",
       "               0.0004208840539826261,\n",
       "               0.00041596048067760736,\n",
       "               0.0004084206944133193,\n",
       "               0.0004079926214154318,\n",
       "               0.0004049643770953392,\n",
       "               0.0004031886326594385,\n",
       "               0.0004031716793184021,\n",
       "               0.0004010213246228635,\n",
       "               0.0004010125333471602,\n",
       "               0.0004001021602012012,\n",
       "               0.00040009254362138245,\n",
       "               0.00039977321929001504,\n",
       "               0.0003991790945086368,\n",
       "               0.0003987191048250227,\n",
       "               0.00039871182262779753,\n",
       "               0.00038922812635712266,\n",
       "               0.0003864147092603581,\n",
       "               0.0003864102068866377,\n",
       "               0.00038640321070846967,\n",
       "               0.0003864008417017499,\n",
       "               0.0003863978010290118,\n",
       "               0.0003839300411815534,\n",
       "               0.00038278212531168184,\n",
       "               0.0003827741183628333,\n",
       "               0.00038233593000689125,\n",
       "               0.00038010731511330277,\n",
       "               0.0003801021695326714,\n",
       "               0.00037750135139206803,\n",
       "               0.0003756438873199857,\n",
       "               0.0003751011720297168,\n",
       "               0.0003750937622993153,\n",
       "               0.00037367197533141384,\n",
       "               0.00037176977037971004,\n",
       "               0.00037136821745201523,\n",
       "               0.00036946692701788985,\n",
       "               0.0003694625133623513,\n",
       "               0.00036945813942133095,\n",
       "               0.00036944664083409515,\n",
       "               0.00036943796617305423,\n",
       "               0.00036942982602999875,\n",
       "               0.00036942819792730246,\n",
       "               0.0003694245991407341,\n",
       "               0.00036893819536894495,\n",
       "               0.00036893265441228867,\n",
       "               0.0003689309373863957,\n",
       "               0.0003684580710078365,\n",
       "               0.0003684518232738603,\n",
       "               0.0003684413531701829,\n",
       "               0.0003681457599097624,\n",
       "               0.0003681368876470858,\n",
       "               0.00036813567217916586,\n",
       "               0.0003681284794733629,\n",
       "               0.0003681246646604809,\n",
       "               0.00036667628342554857,\n",
       "               0.000366673146681214,\n",
       "               0.0003666698428007681,\n",
       "               0.00036666926125749363,\n",
       "               0.0003666651208592539,\n",
       "               0.0003666599378054805,\n",
       "               0.0003666557991253493,\n",
       "               0.00036665331185001964,\n",
       "               0.00036416422208648125,\n",
       "               0.00036388852250542915,\n",
       "               0.00036388598739125664,\n",
       "               0.0003638833976107247,\n",
       "               0.0003638786147642259,\n",
       "               0.00036387423054046615,\n",
       "               0.00036386755106944434,\n",
       "               0.00036385993609534833,\n",
       "               0.00036385602593475675,\n",
       "               0.0003614587918903536,\n",
       "               0.00036145114781091037,\n",
       "               0.0003614485177273272,\n",
       "               0.0003614436905853533,\n",
       "               0.0003614383588642398,\n",
       "               0.00036143223922467324,\n",
       "               0.0003614294593289839,\n",
       "               0.00036142179156831535,\n",
       "               0.0003614181021745678,\n",
       "               0.00036140805563277597,\n",
       "               0.0003614018193649373,\n",
       "               0.00036139917708885203,\n",
       "               0.00036139148922904125,\n",
       "               0.0003613883725456611,\n",
       "               0.0003613790923400031,\n",
       "               0.0003613729763626659,\n",
       "               0.0003613617947216636,\n",
       "               0.000361356417413981,\n",
       "               0.0003613516195049621,\n",
       "               0.0003613420013900282,\n",
       "               0.0003613370114898969,\n",
       "               0.0003613343612363992,\n",
       "               0.00036133281011795223,\n",
       "               0.00036132512278609413,\n",
       "               0.00036132155987503637,\n",
       "               0.00036131539598236833,\n",
       "               0.0003613060748781546,\n",
       "               0.000361293881297718,\n",
       "               0.0003612883478855028,\n",
       "               0.000361282165730063,\n",
       "               0.0003612766069774829,\n",
       "               0.00036126895759493345,\n",
       "               0.00036126646207503547,\n",
       "               0.000361253955751339,\n",
       "               0.00036124499185844026,\n",
       "               0.00036123602666299903,\n",
       "               0.00036122238668524,\n",
       "               0.0003612133749784114,\n",
       "               0.0003612101903845663,\n",
       "               0.0003612015149273892,\n",
       "               0.00036119361204517056,\n",
       "               0.00036118598352905733,\n",
       "               0.00036117635408967994,\n",
       "               0.00036116410857600645,\n",
       "               0.00036115924098108036,\n",
       "               0.0003611552501166486,\n",
       "               0.00036113897127606646,\n",
       "               0.00036113219096342054,\n",
       "               0.00036112697558953854,\n",
       "               0.0003611232334001964,\n",
       "               0.0003611095480583066,\n",
       "               0.0003611033123657201,\n",
       "               0.0003610979087703433,\n",
       "               0.00036108822891903756,\n",
       "               0.0003610826209308885,\n",
       "               0.0003610769841203215,\n",
       "               0.00036106592518254336,\n",
       "               0.0003610616951590569,\n",
       "               0.00036105863483965706,\n",
       "               0.0003610543096872956,\n",
       "               0.00036104803815074584,\n",
       "               0.00036104008659481994,\n",
       "               0.0003610353501062405,\n",
       "               0.00036102699578086886,\n",
       "               0.00036102048004194335,\n",
       "               0.00036101525713115774,\n",
       "               0.00036100263448280107,\n",
       "               0.000360997533553524,\n",
       "               0.00036098897599146323,\n",
       "               0.00036098378286485454,\n",
       "               0.0003609709688050619,\n",
       "               0.00036096198077579215,\n",
       "               0.0003609566958668083,\n",
       "               0.0003609533896665834,\n",
       "               0.0003609458289251445,\n",
       "               0.00036094043520798946,\n",
       "               0.0003609352049357812,\n",
       "               0.00036092127958613625,\n",
       "               0.0003609127144385303,\n",
       "               0.0003609073878294668,\n",
       "               0.0003609066934634371,\n",
       "               0.00036090024326918365,\n",
       "               0.00036089524252404167,\n",
       "               0.00036089110535605646,\n",
       "               0.00036088674259058715,\n",
       "               0.0003608826080618938,\n",
       "               0.0003608768232753387,\n",
       "               0.00036086881259186974,\n",
       "               0.0003608625609302892,\n",
       "               0.000360859365054384,\n",
       "               0.0003608561148531695,\n",
       "               0.0003608507522930007,\n",
       "               0.00036084345482456523,\n",
       "               0.00036084023847019397,\n",
       "               0.0003608369860529201,\n",
       "               0.00036083373718751174,\n",
       "               0.0003608293828514454,\n",
       "               0.0003608232535413917,\n",
       "               0.00036081932695668115,\n",
       "               0.0003608186335485532,\n",
       "               0.0003608149937923658,\n",
       "               0.0003608080476334205,\n",
       "               0.00036080413614579185,\n",
       "               0.0003607994539758756,\n",
       "               0.0003607951025036157,\n",
       "               0.00036079157741459087,\n",
       "               0.00036078765523407045,\n",
       "               0.00036078331895133707,\n",
       "               0.00036077972323651775,\n",
       "               0.0003607765139157711,\n",
       "               0.000360772879354643,\n",
       "               0.0003607693631622143,\n",
       "               0.00036076242311083634,\n",
       "               0.0003607605869280982,\n",
       "               0.0003607591792567228,\n",
       "               0.0003607577717043819,\n",
       "               0.0003607567544704397,\n",
       "               0.00036075459323512375,\n",
       "               0.00036075275785574874,\n",
       "               0.0003607493453309562,\n",
       "               0.0003607486540480679,\n",
       "               0.0003607464938007915,\n",
       "               0.0003607461100841332,\n",
       "               0.000360744583114606,\n",
       "               0.000360743508343856,\n",
       "               0.00036073242876529996,\n",
       "               0.00036072922628786817,\n",
       "               0.00036072227449857167,\n",
       "               0.00036072074814681753,\n",
       "               0.00036072005722877015,\n",
       "               0.0003607131033057119,\n",
       "               0.0003607061537003304,\n",
       "               0.000360702514944859,\n",
       "               0.00036070182421989855,\n",
       "               0.0003606999822986873,\n",
       "               0.0003606941933662272,\n",
       "               0.00036069311329054246,\n",
       "               0.00036068992735380597,\n",
       "               0.00036068923681348384,\n",
       "               0.00036068853727874164,\n",
       "               0.00036068513003385295,\n",
       "               0.00036068329810817686,\n",
       "               0.00036068145850357274,\n",
       "               0.00036067827307032294,\n",
       "               0.0003606775008198538,\n",
       "               0.00036067673477000855,\n",
       "               0.00036067056480419994,\n",
       "               0.00036066979884817347,\n",
       "               0.00036066910873595317,\n",
       "               0.00036066727774608916,\n",
       "               0.00036066619868797383,\n",
       "               0.0003606651207743101,\n",
       "               0.00036066188276467844,\n",
       "               0.00036065570931081796,\n",
       "               0.00036065494764749435,\n",
       "               0.00036065456485151524,\n",
       "               0.00036065380326166335,\n",
       "               0.0003606531134422796,\n",
       "               0.00036065240717164687,\n",
       "               0.00036064102330298663,\n",
       "               0.0003606348615013604,\n",
       "               0.000360631651407098,\n",
       "               0.00036063058305934003,\n",
       "               0.00036062950468484,\n",
       "               0.0003606251210812823,\n",
       "               0.00036062327164097633,\n",
       "               0.0003606214345994189,\n",
       "               0.0003606178016454922,\n",
       "               0.0003606109598974803,\n",
       "               0.0003606066171764163,\n",
       "               0.0003606055495863041,\n",
       "               0.000360603704353,\n",
       "               0.0003606018673519216,\n",
       "               0.0003606000315366207,\n",
       "               0.00036059896434582244,\n",
       "               0.0003605971368927112,\n",
       "               0.0003605960597993595,\n",
       "               0.0003605949737607547,\n",
       "               0.00036059313867270947,\n",
       "               0.00036058875856870125,\n",
       "               0.00036058767281503935,\n",
       "               0.00036058582923008864,\n",
       "               0.0003605839948597307,\n",
       "               0.00036058212165898586,\n",
       "               0.0003605777426560327,\n",
       "               0.00036057589987379725,\n",
       "               0.00036057152146725497,\n",
       "               0.00036056870043550633,\n",
       "               0.0003605664125533031,\n",
       "               0.00036055946692586314,\n",
       "               0.0003605554354309685,\n",
       "               0.0003605550508428369,\n",
       "               0.0003605535246550541,\n",
       "               0.00036055199865386476,\n",
       "               0.0003605512272136036,\n",
       "               0.0003605497095648476,\n",
       "               0.0003605497095648476,\n",
       "               0.0003605481840271325,\n",
       "               0.0003605466667622123,\n",
       "               0.00036054514159399286,\n",
       "               0.0003605436166121823,\n",
       "               0.00036054209991381663,\n",
       "               0.00036054022957500615,\n",
       "               0.00036053838958642457,\n",
       "               0.00036053730339793456,\n",
       "               0.0003605362298394144,\n",
       "               0.0003605351522927232,\n",
       "               0.00036053439434859477,\n",
       "               0.0003605333209694687,\n",
       "               0.0003605326215413261,\n",
       "               0.0003605315482710068,\n",
       "               0.00036053079050250814,\n",
       "               0.00036052923674447255,\n",
       "               0.00036052923674447255,\n",
       "               0.00036052892130948496,\n",
       "               0.0003605286058912554,\n",
       "               0.0003605278482576891,\n",
       "               0.0003605275328561296,\n",
       "               0.0003605275328561296,\n",
       "               0.0003605267752726737,\n",
       "               0.0003605260177393242,\n",
       "               0.0003605260177393242,\n",
       "               0.00036052494078680136,\n",
       "               0.00036052424166356696,\n",
       "               0.00036052424166356696,\n",
       "               0.0003605234842511523,\n",
       "               0.0003605234842511523,\n",
       "               0.0003605224074472283,\n",
       "               0.0003605224074472283,\n",
       "               0.0003605216460412034,\n",
       "               0.0003605209470352244,\n",
       "               0.00036051987038066805,\n",
       "               0.00036051955507937245,\n",
       "               0.00036051955507937245,\n",
       "               0.00036051955507937245,\n",
       "               0.00036051879787913244,\n",
       "               0.00036051803663470416,\n",
       "               0.00036051696424557023,\n",
       "               0.000360516194584206,\n",
       "               0.00036051512230773463,\n",
       "               0.00036051512230773463,\n",
       "               0.00036051512230773463,\n",
       "               0.0003605143653492161,\n",
       "               0.0003605132890837205,\n",
       "               0.0003605132890837205,\n",
       "               0.0003605122128813733,\n",
       "               0.00036051182943855114,\n",
       "               0.00036051182943855114,\n",
       "               0.00036051107264642413,\n",
       "               0.0003605103159043522,\n",
       "               0.0003605092398793346,\n",
       "               0.00036050892467825584,\n",
       "               0.000360508168031763,\n",
       "               0.0003605066163270618,\n",
       "               0.0003605058471944025,\n",
       "               0.0003605050806701945,\n",
       "               0.0003605043116314924,\n",
       "               0.00036050324007735944,\n",
       "               0.00036050215596411934,\n",
       "               0.000360500774773098,\n",
       "               0.00036049940219445777,\n",
       "               0.00036049832680516675,\n",
       "               0.00036049755804754203,\n",
       "               0.0003604944936321632,\n",
       "               0.0003604944936321632,\n",
       "               0.0003604934100383299,\n",
       "               0.00036049233914113034,\n",
       "               0.000360491579206981,\n",
       "               0.0003604912641705704,\n",
       "               0.00036049050842192743,\n",
       "               0.0003604897485796501,\n",
       "               0.0003604886737640443,\n",
       "               0.0003604873061536861,\n",
       "               0.00036048699118287534,\n",
       "               0.0003604862356250336,\n",
       "               0.0003604854801171264,\n",
       "               0.0003604824182576661,\n",
       "               0.00036048134804651745,\n",
       "               0.0003604805928393864,\n",
       "               0.0003604795227446908,\n",
       "               0.0003604775038223599,\n",
       "               0.0003604744435638102,\n",
       "               0.00036047138412123906,\n",
       "               0.00036046801064031954,\n",
       "               0.000360466640264151,\n",
       "               0.00036046358263509446,\n",
       "               0.0003604615692549552,\n",
       "               0.000360460574316276,\n",
       "               0.00036045996289555196,\n",
       "               0.0003604593515381296,\n",
       "               0.00036045798184352023,\n",
       "               0.00036045690887964087,\n",
       "               0.00036045614479956296,\n",
       "               0.00036045412821940776,\n",
       "               0.0003604533642358321,\n",
       "               0.0003604518363977945,\n",
       "               0.0003604503202639335,\n",
       "               0.00036044925202363893,\n",
       "               0.00036044886540580206,\n",
       "               0.00036044779297660513,\n",
       "               0.000360447029353726,\n",
       "               0.00036044627152603917,\n",
       "               0.0003604451935065371,\n",
       "               0.0003604444300309418,\n",
       "               0.00036044291467477634,\n",
       "               0.00036044252822094614,\n",
       "               0.00036044146045386837,\n",
       "               0.00036043981966509605,\n",
       "               0.0003604383046742784,\n",
       "               0.00036043677840509266],\n",
       "              'test-Logloss-std': [0.001484282911982545,\n",
       "               0.0017321276732303605,\n",
       "               0.006807589445694851,\n",
       "               0.005497412922940142,\n",
       "               0.005733522026465612,\n",
       "               0.006098446163634239,\n",
       "               0.009844665699079255,\n",
       "               0.008191642180255788,\n",
       "               0.006492453393022031,\n",
       "               0.005403394515147101,\n",
       "               0.003927138304101546,\n",
       "               0.004303505148294408,\n",
       "               0.0043251890502382,\n",
       "               0.002535819589967034,\n",
       "               0.0036483958060074214,\n",
       "               0.003707239565289522,\n",
       "               0.0033018162387916977,\n",
       "               0.00276382707931411,\n",
       "               0.0027311442400267966,\n",
       "               0.0026195708794923204,\n",
       "               0.002811895869505779,\n",
       "               0.002682868211337657,\n",
       "               0.002527759870734273,\n",
       "               0.0025391697301578135,\n",
       "               0.0025243741289392423,\n",
       "               0.0022165112556577675,\n",
       "               0.0019251818343331826,\n",
       "               0.001781383846228529,\n",
       "               0.001798260008585203,\n",
       "               0.0013449293136748347,\n",
       "               0.001314004997681052,\n",
       "               0.0014582163732529858,\n",
       "               0.0013972081381510223,\n",
       "               0.0012944778776092885,\n",
       "               0.0012426857766887262,\n",
       "               0.0012663602768965456,\n",
       "               0.001203495265878214,\n",
       "               0.0011097333850321545,\n",
       "               0.0010701197931882213,\n",
       "               0.0010290296594755019,\n",
       "               0.00102514745157169,\n",
       "               0.0010477841997841748,\n",
       "               0.0010128544703523498,\n",
       "               0.0009415897533801626,\n",
       "               0.0008575912826768382,\n",
       "               0.0007968809145019124,\n",
       "               0.0007384224102154407,\n",
       "               0.0007241968248593799,\n",
       "               0.0006990395348068442,\n",
       "               0.0006667252435536602,\n",
       "               0.0006958542800955537,\n",
       "               0.0006265665974593337,\n",
       "               0.000616282104114689,\n",
       "               0.0006113977536545666,\n",
       "               0.0005749523730629794,\n",
       "               0.0005281411938549724,\n",
       "               0.0004795799240094662,\n",
       "               0.0004127267878112937,\n",
       "               0.0003556358626540147,\n",
       "               0.00032133282876611305,\n",
       "               0.0003108516413912625,\n",
       "               0.00029542731257054824,\n",
       "               0.00026651922873025934,\n",
       "               0.00025472273964695477,\n",
       "               0.0002573598022597183,\n",
       "               0.00022664365576166007,\n",
       "               0.00021015975993687161,\n",
       "               0.00019917568824747881,\n",
       "               0.00018916668558095827,\n",
       "               0.00018989444970713646,\n",
       "               0.00019075747854514806,\n",
       "               0.00018687502173070166,\n",
       "               0.00017740583400664882,\n",
       "               0.00018231050508926027,\n",
       "               0.00017567983135351788,\n",
       "               0.00017060259252494724,\n",
       "               0.0001528510276165982,\n",
       "               0.00015850905624332064,\n",
       "               0.0001572746320586095,\n",
       "               0.0001587526230995206,\n",
       "               0.00014926050681330778,\n",
       "               0.00015145902250069437,\n",
       "               0.00013972482397949009,\n",
       "               0.00014585440279006923,\n",
       "               0.00014255190009432122,\n",
       "               0.00013397359281824565,\n",
       "               0.00013901922368615133,\n",
       "               0.0001338569500578135,\n",
       "               0.00012748336998280816,\n",
       "               0.00012435932924312595,\n",
       "               0.00012867302253108712,\n",
       "               0.00012073898384701863,\n",
       "               0.0001179807644225376,\n",
       "               0.00010880626196957989,\n",
       "               9.642642502830082e-05,\n",
       "               9.971715155610417e-05,\n",
       "               9.208820641514822e-05,\n",
       "               9.149948096654628e-05,\n",
       "               8.464760473673709e-05,\n",
       "               7.747850242840482e-05,\n",
       "               7.779712079300913e-05,\n",
       "               8.007537684411838e-05,\n",
       "               8.120152081938522e-05,\n",
       "               7.750345387001494e-05,\n",
       "               7.559571493902107e-05,\n",
       "               7.572345025727034e-05,\n",
       "               7.874842091399201e-05,\n",
       "               8.070065148460561e-05,\n",
       "               8.222070062461114e-05,\n",
       "               8.227395661353077e-05,\n",
       "               8.227520519658768e-05,\n",
       "               8.238821910198356e-05,\n",
       "               8.55236367998716e-05,\n",
       "               8.22813915158099e-05,\n",
       "               8.180247768333054e-05,\n",
       "               7.814845214382737e-05,\n",
       "               7.34858841379479e-05,\n",
       "               7.28566836207074e-05,\n",
       "               7.714553155501422e-05,\n",
       "               8.048499506427163e-05,\n",
       "               8.176791601639508e-05,\n",
       "               7.591450794536566e-05,\n",
       "               7.608591389939856e-05,\n",
       "               7.572753434152661e-05,\n",
       "               7.697280632433455e-05,\n",
       "               7.946100852369087e-05,\n",
       "               7.943682834853036e-05,\n",
       "               8.259973016000306e-05,\n",
       "               8.260381011773863e-05,\n",
       "               8.239903366614207e-05,\n",
       "               8.239674189685183e-05,\n",
       "               8.186940105127654e-05,\n",
       "               8.21352741706219e-05,\n",
       "               8.235051423317957e-05,\n",
       "               8.234529473660619e-05,\n",
       "               7.584557993124825e-05,\n",
       "               7.396999369777746e-05,\n",
       "               7.396651938992895e-05,\n",
       "               7.396818337263058e-05,\n",
       "               7.396817221438236e-05,\n",
       "               7.396574633678353e-05,\n",
       "               7.550045446675383e-05,\n",
       "               7.457405367523518e-05,\n",
       "               7.45680371361527e-05,\n",
       "               7.470854988198449e-05,\n",
       "               7.756547226469707e-05,\n",
       "               7.756229467065539e-05,\n",
       "               8.159223500383433e-05,\n",
       "               7.996271735820087e-05,\n",
       "               8.010373790329497e-05,\n",
       "               8.010694163922483e-05,\n",
       "               7.914946041458983e-05,\n",
       "               8.021616708884897e-05,\n",
       "               8.029099409359664e-05,\n",
       "               8.155409790411812e-05,\n",
       "               8.154924013305142e-05,\n",
       "               8.154986918585404e-05,\n",
       "               8.153191558297307e-05,\n",
       "               8.152606296259762e-05,\n",
       "               8.15241697277679e-05,\n",
       "               8.152520169133889e-05,\n",
       "               8.15250015267058e-05,\n",
       "               8.164015657932322e-05,\n",
       "               8.16366627187167e-05,\n",
       "               8.16356540351976e-05,\n",
       "               8.176123147977872e-05,\n",
       "               8.176450226553319e-05,\n",
       "               8.175409020327618e-05,\n",
       "               8.124403365136563e-05,\n",
       "               8.123731759528648e-05,\n",
       "               8.1237918354227e-05,\n",
       "               8.123332854168797e-05,\n",
       "               8.123494413282115e-05,\n",
       "               8.230853190846278e-05,\n",
       "               8.230860173159118e-05,\n",
       "               8.231135264226296e-05,\n",
       "               8.231218162142957e-05,\n",
       "               8.23153792553232e-05,\n",
       "               8.231371353007293e-05,\n",
       "               8.231392169295802e-05,\n",
       "               8.231500916630476e-05,\n",
       "               8.070718217582991e-05,\n",
       "               8.021231480546673e-05,\n",
       "               8.021434436005287e-05,\n",
       "               8.021365643281597e-05,\n",
       "               8.02155223187867e-05,\n",
       "               8.021325516466265e-05,\n",
       "               8.021043236767769e-05,\n",
       "               8.020445196796881e-05,\n",
       "               8.02033715755185e-05,\n",
       "               8.089553544995012e-05,\n",
       "               8.088639173719208e-05,\n",
       "               8.088399156845681e-05,\n",
       "               8.087692306254971e-05,\n",
       "               8.086716312597952e-05,\n",
       "               8.086252787481213e-05,\n",
       "               8.086039042199887e-05,\n",
       "               8.08548060492777e-05,\n",
       "               8.085345461139647e-05,\n",
       "               8.084444973951133e-05,\n",
       "               8.083500870934068e-05,\n",
       "               8.083503953931702e-05,\n",
       "               8.083251622512408e-05,\n",
       "               8.083249957435813e-05,\n",
       "               8.082528853315146e-05,\n",
       "               8.082304430882466e-05,\n",
       "               8.081988035038494e-05,\n",
       "               8.081779306466237e-05,\n",
       "               8.081472533255512e-05,\n",
       "               8.081293720320232e-05,\n",
       "               8.081145111996012e-05,\n",
       "               8.08114895296816e-05,\n",
       "               8.081236206157112e-05,\n",
       "               8.08054455134323e-05,\n",
       "               8.080175012646816e-05,\n",
       "               8.079440448594966e-05,\n",
       "               8.079580111816279e-05,\n",
       "               8.07844324207505e-05,\n",
       "               8.078554334464293e-05,\n",
       "               8.07844801541433e-05,\n",
       "               8.078279117128436e-05,\n",
       "               8.07728576273194e-05,\n",
       "               8.077212065314061e-05,\n",
       "               8.076416829904475e-05,\n",
       "               8.075797406691496e-05,\n",
       "               8.07507426165242e-05,\n",
       "               8.073232986727565e-05,\n",
       "               8.072254969335773e-05,\n",
       "               8.07187426276584e-05,\n",
       "               8.071073173976339e-05,\n",
       "               8.070517684762263e-05,\n",
       "               8.07046166326598e-05,\n",
       "               8.070026133952764e-05,\n",
       "               8.06974211554645e-05,\n",
       "               8.06967676382471e-05,\n",
       "               8.069350168989729e-05,\n",
       "               8.06736582439807e-05,\n",
       "               8.067033354751849e-05,\n",
       "               8.06664452743407e-05,\n",
       "               8.066292235724066e-05,\n",
       "               8.064692737528275e-05,\n",
       "               8.064106255738481e-05,\n",
       "               8.063751238492021e-05,\n",
       "               8.063657120509065e-05,\n",
       "               8.063180055646134e-05,\n",
       "               8.062887589823998e-05,\n",
       "               8.062646510929056e-05,\n",
       "               8.062315500395983e-05,\n",
       "               8.062510430098471e-05,\n",
       "               8.062157884856465e-05,\n",
       "               8.061810681494292e-05,\n",
       "               8.061374233685463e-05,\n",
       "               8.060932833315138e-05,\n",
       "               8.060341302026884e-05,\n",
       "               8.059838965558272e-05,\n",
       "               8.059446690729746e-05,\n",
       "               8.057766203125994e-05,\n",
       "               8.057380804438637e-05,\n",
       "               8.056869509319937e-05,\n",
       "               8.056484848667976e-05,\n",
       "               8.054870619136965e-05,\n",
       "               8.054164897102025e-05,\n",
       "               8.053834896865154e-05,\n",
       "               8.053768255320237e-05,\n",
       "               8.052849251907374e-05,\n",
       "               8.052315107856969e-05,\n",
       "               8.051926083370378e-05,\n",
       "               8.05101927688419e-05,\n",
       "               8.050376501305789e-05,\n",
       "               8.050135355918586e-05,\n",
       "               8.050205194800454e-05,\n",
       "               8.049362314639546e-05,\n",
       "               8.04908021218731e-05,\n",
       "               8.048816315315629e-05,\n",
       "               8.048507549008974e-05,\n",
       "               8.048243583728679e-05,\n",
       "               8.047907127390526e-05,\n",
       "               8.047057970628986e-05,\n",
       "               8.046663435642513e-05,\n",
       "               8.046275438930183e-05,\n",
       "               8.045877757823659e-05,\n",
       "               8.045349297254593e-05,\n",
       "               8.0446122337397e-05,\n",
       "               8.044222382806628e-05,\n",
       "               8.043825163680337e-05,\n",
       "               8.04342775550972e-05,\n",
       "               8.043119258765279e-05,\n",
       "               8.042299655209479e-05,\n",
       "               8.041982355877049e-05,\n",
       "               8.04205213597929e-05,\n",
       "               8.041682932887925e-05,\n",
       "               8.04090060621482e-05,\n",
       "               8.040586098519984e-05,\n",
       "               8.04032303344959e-05,\n",
       "               8.040014942690878e-05,\n",
       "               8.039673054805096e-05,\n",
       "               8.039355957536757e-05,\n",
       "               8.039050594193976e-05,\n",
       "               8.03868883423457e-05,\n",
       "               8.038299558430771e-05,\n",
       "               8.037930705142739e-05,\n",
       "               8.037588557140008e-05,\n",
       "               8.036807365573002e-05,\n",
       "               8.03695875020966e-05,\n",
       "               8.037101025702593e-05,\n",
       "               8.037243290447703e-05,\n",
       "               8.037357682520297e-05,\n",
       "               8.037553760706698e-05,\n",
       "               8.037705107929378e-05,\n",
       "               8.03796912007309e-05,\n",
       "               8.038038766155864e-05,\n",
       "               8.038234804503353e-05,\n",
       "               8.038262218500856e-05,\n",
       "               8.038371318442855e-05,\n",
       "               8.038468368598584e-05,\n",
       "               8.036639099777202e-05,\n",
       "               8.036249926491375e-05,\n",
       "               8.035471192141093e-05,\n",
       "               8.035580288341727e-05,\n",
       "               8.035649922652294e-05,\n",
       "               8.034871026387916e-05,\n",
       "               8.034092679963505e-05,\n",
       "               8.033725757670859e-05,\n",
       "               8.033795386012698e-05,\n",
       "               8.033947925756913e-05,\n",
       "               8.033086393070352e-05,\n",
       "               8.03318386770484e-05,\n",
       "               8.032797754448593e-05,\n",
       "               8.032867372807225e-05,\n",
       "               8.032938227999384e-05,\n",
       "               8.033202164166607e-05,\n",
       "               8.033353438567191e-05,\n",
       "               8.033505272101318e-05,\n",
       "               8.033119195810057e-05,\n",
       "               8.033174453421747e-05,\n",
       "               8.033229270645767e-05,\n",
       "               8.032395530378498e-05,\n",
       "               8.032450347067175e-05,\n",
       "               8.032519944915149e-05,\n",
       "               8.032671199725696e-05,\n",
       "               8.032768638688829e-05,\n",
       "               8.032866590253601e-05,\n",
       "               8.032470890262448e-05,\n",
       "               8.031638396042748e-05,\n",
       "               8.031692924753699e-05,\n",
       "               8.031720330961691e-05,\n",
       "               8.031774859282317e-05,\n",
       "               8.03184444228649e-05,\n",
       "               8.031915795734592e-05,\n",
       "               8.03013459493031e-05,\n",
       "               8.029301448937988e-05,\n",
       "               8.02891489180417e-05,\n",
       "               8.029011590341614e-05,\n",
       "               8.029109008242957e-05,\n",
       "               8.028796005471745e-05,\n",
       "               8.028948663367947e-05,\n",
       "               8.029100442969825e-05,\n",
       "               8.028734347659324e-05,\n",
       "               8.027970517175809e-05,\n",
       "               8.027664853337711e-05,\n",
       "               8.027761527521786e-05,\n",
       "               8.027914529595457e-05,\n",
       "               8.028066951248138e-05,\n",
       "               8.028218705343577e-05,\n",
       "               8.028315363645634e-05,\n",
       "               8.028466531815325e-05,\n",
       "               8.028563905243998e-05,\n",
       "               8.028662513232385e-05,\n",
       "               8.028814246338125e-05,\n",
       "               8.028501444608972e-05,\n",
       "               8.028600042592264e-05,\n",
       "               8.028753001541652e-05,\n",
       "               8.028904717767044e-05,\n",
       "               8.029059820950216e-05,\n",
       "               8.028747054200499e-05,\n",
       "               8.028899996019732e-05,\n",
       "               8.028587272399158e-05,\n",
       "               8.028173400689579e-05,\n",
       "               8.028235469182389e-05,\n",
       "               8.027704883140606e-05,\n",
       "               8.027357310488981e-05,\n",
       "               8.027384921707773e-05,\n",
       "               8.027494501369909e-05,\n",
       "               8.027604080644275e-05,\n",
       "               8.027659481268292e-05,\n",
       "               8.027768480211621e-05,\n",
       "               8.027768480211621e-05,\n",
       "               8.027878058713244e-05,\n",
       "               8.027987055878199e-05,\n",
       "               8.028096633758284e-05,\n",
       "               8.028206211234828e-05,\n",
       "               8.028315206387238e-05,\n",
       "               8.02847028307082e-05,\n",
       "               8.028623188141007e-05,\n",
       "               8.028721913141057e-05,\n",
       "               8.02881973142691e-05,\n",
       "               8.028917837603874e-05,\n",
       "               8.028972330281748e-05,\n",
       "               8.029070140561487e-05,\n",
       "               8.029141063753538e-05,\n",
       "               8.02923886729081e-05,\n",
       "               8.029293356587337e-05,\n",
       "               8.029405093588706e-05,\n",
       "               8.029405093588706e-05,\n",
       "               8.029448402826007e-05,\n",
       "               8.029491710148384e-05,\n",
       "               8.029546198187682e-05,\n",
       "               8.029589502941293e-05,\n",
       "               8.029589502941293e-05,\n",
       "               8.029643989915148e-05,\n",
       "               8.029698476489012e-05,\n",
       "               8.029698476489012e-05,\n",
       "               8.029796557309631e-05,\n",
       "               8.029867460510303e-05,\n",
       "               8.029867460510303e-05,\n",
       "               8.029921945099948e-05,\n",
       "               8.029921945099948e-05,\n",
       "               8.030020018894608e-05,\n",
       "               8.030020018894608e-05,\n",
       "               8.030074796560314e-05,\n",
       "               8.030145692604598e-05,\n",
       "               8.030243759332717e-05,\n",
       "               8.03028704728727e-05,\n",
       "               8.03028704728727e-05,\n",
       "               8.03028704728727e-05,\n",
       "               8.030341528429861e-05,\n",
       "               8.030396303780878e-05,\n",
       "               8.030494068501538e-05,\n",
       "               8.030549455357729e-05,\n",
       "               8.030647215710679e-05,\n",
       "               8.030647215710679e-05,\n",
       "               8.030647215710679e-05,\n",
       "               8.030701694194632e-05,\n",
       "               8.030799745356206e-05,\n",
       "               8.030799745356206e-05,\n",
       "               8.030897793161712e-05,\n",
       "               8.030925393435978e-05,\n",
       "               8.030925393435978e-05,\n",
       "               8.030979869859427e-05,\n",
       "               8.031034345875656e-05,\n",
       "               8.031132388493933e-05,\n",
       "               8.031175657326088e-05,\n",
       "               8.031230131539967e-05,\n",
       "               8.031341855385133e-05,\n",
       "               8.031397238465465e-05,\n",
       "               8.031452437002669e-05,\n",
       "               8.03150781990692e-05,\n",
       "               8.031605555875018e-05,\n",
       "               8.03170419756762e-05,\n",
       "               8.031843621384032e-05,\n",
       "               8.031982422845845e-05,\n",
       "               8.032080437250753e-05,\n",
       "               8.032135814924054e-05,\n",
       "               8.0323565937196e-05,\n",
       "               8.0323565937196e-05,\n",
       "               8.032455213406634e-05,\n",
       "               8.032552919399487e-05,\n",
       "               8.032607682830236e-05,\n",
       "               8.032650920093172e-05,\n",
       "               8.03270538443326e-05,\n",
       "               8.032760147000032e-05,\n",
       "               8.032858142802395e-05,\n",
       "               8.03299659706772e-05,\n",
       "               8.033039824183966e-05,\n",
       "               8.033094284931021e-05,\n",
       "               8.033148745262456e-05,\n",
       "               8.033369490147309e-05,\n",
       "               8.033467169005748e-05,\n",
       "               8.033521626251288e-05,\n",
       "               8.033619300382566e-05,\n",
       "               8.033846911826068e-05,\n",
       "               8.034067629035692e-05,\n",
       "               8.034288339592093e-05,\n",
       "               8.034552242526358e-05,\n",
       "               8.034690920076142e-05,\n",
       "               8.034911609115703e-05,\n",
       "               8.035138806407017e-05,\n",
       "               8.035250381578249e-05,\n",
       "               8.035334269148196e-05,\n",
       "               8.035418149483266e-05,\n",
       "               8.035556766336887e-05,\n",
       "               8.035654675459699e-05,\n",
       "               8.035709834985615e-05,\n",
       "               8.035937221683238e-05,\n",
       "               8.035992378044329e-05,\n",
       "               8.036102691202615e-05,\n",
       "               8.036212172154635e-05,\n",
       "               8.036309753723392e-05,\n",
       "               8.03633767496819e-05,\n",
       "               8.036435561504372e-05,\n",
       "               8.036490714768221e-05,\n",
       "               8.036545452682268e-05,\n",
       "               8.03664374930268e-05,\n",
       "               8.036698900980111e-05,\n",
       "               8.036808376358548e-05,\n",
       "               8.036836297373486e-05,\n",
       "               8.03693386343118e-05,\n",
       "               8.037134079013093e-05,\n",
       "               8.037243546998681e-05,\n",
       "               8.037353842850579e-05],\n",
       "              'train-Logloss-mean': [0.5662784857967287,\n",
       "               0.4739655193065002,\n",
       "               0.390826795602362,\n",
       "               0.3049212932593738,\n",
       "               0.24196693266287483,\n",
       "               0.20618248775444692,\n",
       "               0.17932164896778197,\n",
       "               0.14459355122408693,\n",
       "               0.12158833060333459,\n",
       "               0.10597663414490972,\n",
       "               0.08975824946795294,\n",
       "               0.07560731959608746,\n",
       "               0.06871788108237911,\n",
       "               0.05788545066158277,\n",
       "               0.05019885520442517,\n",
       "               0.043787175636708434,\n",
       "               0.03808111707354844,\n",
       "               0.03385734680689733,\n",
       "               0.029315771191893637,\n",
       "               0.02617283411706935,\n",
       "               0.022873447366127408,\n",
       "               0.020366417303059083,\n",
       "               0.018025539350027427,\n",
       "               0.016459756424718294,\n",
       "               0.015233971730087462,\n",
       "               0.014245403926195485,\n",
       "               0.012489385892374897,\n",
       "               0.01100812859135775,\n",
       "               0.0097642722400478,\n",
       "               0.009009492799856567,\n",
       "               0.008359004013834936,\n",
       "               0.007870354412684308,\n",
       "               0.007297272142196996,\n",
       "               0.006626660448422539,\n",
       "               0.005976391172234607,\n",
       "               0.005415815028420418,\n",
       "               0.005054797560774022,\n",
       "               0.004728294346061276,\n",
       "               0.004494661213607455,\n",
       "               0.004232115526720724,\n",
       "               0.0039524871047501185,\n",
       "               0.0037224167391350627,\n",
       "               0.003441828410547916,\n",
       "               0.0032470598509142024,\n",
       "               0.003102022399963717,\n",
       "               0.0029504027692176826,\n",
       "               0.0027535999970784144,\n",
       "               0.002661372141400346,\n",
       "               0.0025442302885935,\n",
       "               0.0024441779259365055,\n",
       "               0.0023475577517946553,\n",
       "               0.0021974268452301924,\n",
       "               0.0021193525418023605,\n",
       "               0.0020184504480568995,\n",
       "               0.0019047911503909114,\n",
       "               0.0018004369621835725,\n",
       "               0.0016970425569393485,\n",
       "               0.0016047165147058129,\n",
       "               0.0015171620395451838,\n",
       "               0.0014523575531211373,\n",
       "               0.0013830112252750965,\n",
       "               0.0013235247158012871,\n",
       "               0.0012497055623005241,\n",
       "               0.0011973096884667026,\n",
       "               0.0011692453963323846,\n",
       "               0.0011196301063356963,\n",
       "               0.0010676362209742017,\n",
       "               0.0010191180857919032,\n",
       "               0.000981608840150853,\n",
       "               0.0009512730706162269,\n",
       "               0.0009245255543202966,\n",
       "               0.0009019726679530285,\n",
       "               0.0008795986194956006,\n",
       "               0.0008548070530434223,\n",
       "               0.0008284724158330479,\n",
       "               0.0008126631923146895,\n",
       "               0.0007848704846677377,\n",
       "               0.0007679889780355378,\n",
       "               0.0007374602299941834,\n",
       "               0.0007216205208927511,\n",
       "               0.0006983767107323331,\n",
       "               0.0006753147776930896,\n",
       "               0.0006549658120083334,\n",
       "               0.0006398834272354015,\n",
       "               0.0006230365185123587,\n",
       "               0.0006069037754989598,\n",
       "               0.0005931683127979928,\n",
       "               0.0005833501575121586,\n",
       "               0.0005651737096421646,\n",
       "               0.0005494297900145715,\n",
       "               0.000541056955314761,\n",
       "               0.0005222952620150863,\n",
       "               0.0005178042079315749,\n",
       "               0.0005065437650562607,\n",
       "               0.0004996520897370145,\n",
       "               0.0004916522671954526,\n",
       "               0.0004826533272990938,\n",
       "               0.000479568691286116,\n",
       "               0.0004698944679754154,\n",
       "               0.0004622389696164594,\n",
       "               0.0004542085271040358,\n",
       "               0.00044106666100259776,\n",
       "               0.0004343883391216266,\n",
       "               0.00042488996042331473,\n",
       "               0.00042167083200696004,\n",
       "               0.000421171442052935,\n",
       "               0.0004155594444481982,\n",
       "               0.0004121729930251469,\n",
       "               0.00040081525805879146,\n",
       "               0.00040051869461882805,\n",
       "               0.0004005000797951988,\n",
       "               0.00039995394207197825,\n",
       "               0.00039695378911619857,\n",
       "               0.00039287706218180563,\n",
       "               0.0003923806864067005,\n",
       "               0.00038932118152143757,\n",
       "               0.0003858722546983071,\n",
       "               0.0003805596830670163,\n",
       "               0.0003742706281211868,\n",
       "               0.00037166516915773255,\n",
       "               0.0003685895064102606,\n",
       "               0.0003648381461469093,\n",
       "               0.00035775777555740833,\n",
       "               0.0003573335639275912,\n",
       "               0.00035455120480229956,\n",
       "               0.00035273592449971936,\n",
       "               0.0003527282339590811,\n",
       "               0.00035054406164233257,\n",
       "               0.0003505444393515458,\n",
       "               0.00034947963702038835,\n",
       "               0.0003494707728899472,\n",
       "               0.0003490732336048794,\n",
       "               0.00034845252081754086,\n",
       "               0.00034801164231037856,\n",
       "               0.0003480067837460906,\n",
       "               0.00034052932459013716,\n",
       "               0.00033831324723776707,\n",
       "               0.0003383019832647635,\n",
       "               0.00033830952144154605,\n",
       "               0.00033832636659222626,\n",
       "               0.0003383146719583909,\n",
       "               0.00033586684491857075,\n",
       "               0.00033442796129380756,\n",
       "               0.0003344255389692902,\n",
       "               0.00033393939342450874,\n",
       "               0.0003317343928726628,\n",
       "               0.00033173286632554516,\n",
       "               0.0003288197704287172,\n",
       "               0.0003266873043694579,\n",
       "               0.0003261835030035149,\n",
       "               0.0003261483239044282,\n",
       "               0.0003243963060326818,\n",
       "               0.0003229580587389725,\n",
       "               0.00032253576299033865,\n",
       "               0.0003208689868269254,\n",
       "               0.00032086469036315214,\n",
       "               0.00032084921036813763,\n",
       "               0.00032084614362917584,\n",
       "               0.0003208268289817545,\n",
       "               0.0003208449325410249,\n",
       "               0.0003208280615296201,\n",
       "               0.0003208306167027951,\n",
       "               0.00032025853159348757,\n",
       "               0.00032025171472160766,\n",
       "               0.00032024845683222827,\n",
       "               0.0003197375600512038,\n",
       "               0.0003197604488955253,\n",
       "               0.00031974444910647456,\n",
       "               0.0003192879476766685,\n",
       "               0.0003192634509848919,\n",
       "               0.00031925810609575105,\n",
       "               0.00031927463944324546,\n",
       "               0.00031927021658099546,\n",
       "               0.00031765883777376536,\n",
       "               0.0003176728407267072,\n",
       "               0.00031766881475546146,\n",
       "               0.0003176599519813945,\n",
       "               0.0003176433712065541,\n",
       "               0.0003176696047906726,\n",
       "               0.0003176539868593779,\n",
       "               0.0003176461928154482,\n",
       "               0.00031557824125756265,\n",
       "               0.0003152131837309047,\n",
       "               0.00031520935130276845,\n",
       "               0.0003152121646951711,\n",
       "               0.0003152222488871434,\n",
       "               0.0003152094301419273,\n",
       "               0.0003152044214645277,\n",
       "               0.0003151979816447178,\n",
       "               0.00031519340973643615,\n",
       "               0.00031296201064182625,\n",
       "               0.00031295403603423614,\n",
       "               0.00031295946161227875,\n",
       "               0.00031294551389394294,\n",
       "               0.00031295425599363054,\n",
       "               0.00031296314856964886,\n",
       "               0.00031295206512347114,\n",
       "               0.00031294973704743977,\n",
       "               0.00031295091205277693,\n",
       "               0.0003129354979453853,\n",
       "               0.00031292230111376645,\n",
       "               0.0003129299017767064,\n",
       "               0.0003129328122972188,\n",
       "               0.00031293243034567455,\n",
       "               0.0003129139068833102,\n",
       "               0.00031291648244181653,\n",
       "               0.000312887148122701,\n",
       "               0.0003129011405636478,\n",
       "               0.00031290196415277306,\n",
       "               0.00031287083219723954,\n",
       "               0.00031287169579692446,\n",
       "               0.0003128774566839788,\n",
       "               0.00031286311333906993,\n",
       "               0.00031285624356940117,\n",
       "               0.0003128722096222289,\n",
       "               0.00031286467628753313,\n",
       "               0.00031286011551617457,\n",
       "               0.0003128546214530225,\n",
       "               0.0003128340954706682,\n",
       "               0.00031283422141225074,\n",
       "               0.00031283797359199365,\n",
       "               0.0003128356225285072,\n",
       "               0.00031281769345154945,\n",
       "               0.00031280450318547394,\n",
       "               0.00031280472206034985,\n",
       "               0.00031279999007148275,\n",
       "               0.0003127910155727846,\n",
       "               0.00031278474039023554,\n",
       "               0.00031278387414813454,\n",
       "               0.0003127926845861892,\n",
       "               0.0003127999433203647,\n",
       "               0.00031279133639413616,\n",
       "               0.0003128236644624863,\n",
       "               0.0003128121088691939,\n",
       "               0.00031278800911759284,\n",
       "               0.0003127949718873595,\n",
       "               0.0003128035380242296,\n",
       "               0.0003127964809356317,\n",
       "               0.0003127780724312104,\n",
       "               0.00031276423343939123,\n",
       "               0.00031278467192314514,\n",
       "               0.00031276478647287217,\n",
       "               0.00031273511296989187,\n",
       "               0.0003127649020550613,\n",
       "               0.00031276331699970885,\n",
       "               0.00031275331981744784,\n",
       "               0.0003127355295911373,\n",
       "               0.0003127331089317148,\n",
       "               0.0003127340898331983,\n",
       "               0.00031275360824148835,\n",
       "               0.0003127234708889231,\n",
       "               0.0003127263001196689,\n",
       "               0.000312696677552938,\n",
       "               0.0003127133213092276,\n",
       "               0.00031271696378647163,\n",
       "               0.0003126970441476945,\n",
       "               0.00031268517673246795,\n",
       "               0.00031268135053783995,\n",
       "               0.00031268357114473515,\n",
       "               0.0003126880502880459,\n",
       "               0.00031267967309720726,\n",
       "               0.00031266155427422775,\n",
       "               0.00031264662669241777,\n",
       "               0.000312662989887897,\n",
       "               0.00031265596052518083,\n",
       "               0.00031263726455800164,\n",
       "               0.00031264987223666226,\n",
       "               0.0003126486750323519,\n",
       "               0.0003126277869667575,\n",
       "               0.0003126232669304793,\n",
       "               0.0003126371869087203,\n",
       "               0.00031261757720078983,\n",
       "               0.0003126092130485687,\n",
       "               0.0003125984257901732,\n",
       "               0.00031260446262125783,\n",
       "               0.0003126006652623086,\n",
       "               0.0003126005929691686,\n",
       "               0.00031262644803912407,\n",
       "               0.00031259150881051334,\n",
       "               0.0003125865643596159,\n",
       "               0.00031257501906119175,\n",
       "               0.0003125857833600185,\n",
       "               0.0003125806633882427,\n",
       "               0.0003125753574087481,\n",
       "               0.0003125774550451236,\n",
       "               0.0003125747512430749,\n",
       "               0.00031258052956425293,\n",
       "               0.0003125641882629004,\n",
       "               0.0003125444134104101,\n",
       "               0.0003125443027383071,\n",
       "               0.0003125600943304877,\n",
       "               0.000312555669506902,\n",
       "               0.0003125456428005126,\n",
       "               0.00031255544488061613,\n",
       "               0.00031254670964781515,\n",
       "               0.00031252012860874697,\n",
       "               0.00031251664948500553,\n",
       "               0.0003125310612324557,\n",
       "               0.00031252136866491463,\n",
       "               0.0003125251706648256,\n",
       "               0.0003125293444638503,\n",
       "               0.00031251110118994766,\n",
       "               0.00031250568710498584,\n",
       "               0.00031251885641610667,\n",
       "               0.00031251053245274455,\n",
       "               0.0003124986997776652,\n",
       "               0.0003125059538755017,\n",
       "               0.0003125164848153622,\n",
       "               0.00031249626356683394,\n",
       "               0.00031250043453337397,\n",
       "               0.00031249852063958464,\n",
       "               0.00031248992298078224,\n",
       "               0.0003124867036090571,\n",
       "               0.0003124819908984618,\n",
       "               0.0003124974029243347,\n",
       "               0.0003124955794564069,\n",
       "               0.0003124858292396334,\n",
       "               0.0003124658220567391,\n",
       "               0.00031245970115495697,\n",
       "               0.0003124646443572144,\n",
       "               0.00031247522310319406,\n",
       "               0.00031247000038874353,\n",
       "               0.0003124709635271809,\n",
       "               0.0003124690409549675,\n",
       "               0.00031247107052206576,\n",
       "               0.00031245502575686713,\n",
       "               0.00031246308696288017,\n",
       "               0.0003124619711026023,\n",
       "               0.0003124475687304173,\n",
       "               0.0003124443628090758,\n",
       "               0.0003124522279576023,\n",
       "               0.000312449993051094,\n",
       "               0.0003124581304204646,\n",
       "               0.00031244395514015385,\n",
       "               0.0003124442691867451,\n",
       "               0.0003124446970025373,\n",
       "               0.0003124291040806304,\n",
       "               0.00031243031878835334,\n",
       "               0.00031242126554281117,\n",
       "               0.000312434787254003,\n",
       "               0.00031242949933722475,\n",
       "               0.00031243074565686084,\n",
       "               0.00031244188679987236,\n",
       "               0.00031243147277713505,\n",
       "               0.00031242441355224914,\n",
       "               0.0003124264553439917,\n",
       "               0.0003124266718788035,\n",
       "               0.00031241639585998394,\n",
       "               0.000312411623219157,\n",
       "               0.00031242328111577724,\n",
       "               0.000312433752544575,\n",
       "               0.00031241166491252853,\n",
       "               0.0003124323988742167,\n",
       "               0.0003124302694421087,\n",
       "               0.00031242373141011643,\n",
       "               0.00031241179703141826,\n",
       "               0.00031240401477936194,\n",
       "               0.0003124098456290554,\n",
       "               0.00031239260153282424,\n",
       "               0.0003124007115605042,\n",
       "               0.0003124088626731709,\n",
       "               0.0003124068639985634,\n",
       "               0.0003123973005226306,\n",
       "               0.0003123997268737281,\n",
       "               0.00031238204680894907,\n",
       "               0.0003124086849082116,\n",
       "               0.0003123983061646854,\n",
       "               0.00031239726035168586,\n",
       "               0.0003123858352349495,\n",
       "               0.0003123870582872481,\n",
       "               0.0003123816776668502,\n",
       "               0.00031238363999033434,\n",
       "               0.0003123723677682106,\n",
       "               0.00031238500020548416,\n",
       "               0.0003123558885200636,\n",
       "               0.00031236340741598476,\n",
       "               0.00031236179166171403,\n",
       "               0.00031237146492502614,\n",
       "               0.00031236681599417773,\n",
       "               0.0003123733314602512,\n",
       "               0.00031235547531008686,\n",
       "               0.00031235238122108883,\n",
       "               0.00031234884695235216,\n",
       "               0.0003123481019928708,\n",
       "               0.0003123359519167917,\n",
       "               0.0003123430893971861,\n",
       "               0.0003123430893971861,\n",
       "               0.0003123461800990261,\n",
       "               0.00031233186231116236,\n",
       "               0.0003123388050299936,\n",
       "               0.00031233596892638505,\n",
       "               0.0003123355138542665,\n",
       "               0.0003123320732365837,\n",
       "               0.0003123376222980472,\n",
       "               0.00031232784341479356,\n",
       "               0.0003123203807251557,\n",
       "               0.000312317778513207,\n",
       "               0.00031232972824639686,\n",
       "               0.0003123324879003565,\n",
       "               0.00031232913884911414,\n",
       "               0.00031232525964129546,\n",
       "               0.0003123081294928285,\n",
       "               0.0003123149416900264,\n",
       "               0.0003123149416900264,\n",
       "               0.0003123224027656164,\n",
       "               0.0003123226753865241,\n",
       "               0.0003123165116054567,\n",
       "               0.00031232173718973304,\n",
       "               0.00031232173718973304,\n",
       "               0.00031232758122502416,\n",
       "               0.00031232268013887325,\n",
       "               0.00031232268013887325,\n",
       "               0.00031231018350135994,\n",
       "               0.00031230882986402935,\n",
       "               0.00031230882986402935,\n",
       "               0.0003123181184842815,\n",
       "               0.0003123181184842815,\n",
       "               0.00031231537008653734,\n",
       "               0.00031231537008653734,\n",
       "               0.00031230892821323963,\n",
       "               0.0003123148743077087,\n",
       "               0.00031231250643654225,\n",
       "               0.0003123175516453184,\n",
       "               0.0003123175516453184,\n",
       "               0.0003123175516453184,\n",
       "               0.0003123130865995763,\n",
       "               0.00031232081795069897,\n",
       "               0.00031231131414768715,\n",
       "               0.0003123133503253541,\n",
       "               0.0003123160973710461,\n",
       "               0.0003123160973710461,\n",
       "               0.0003123160973710461,\n",
       "               0.00031231571277894174,\n",
       "               0.00031230426888780624,\n",
       "               0.00031230426888780624,\n",
       "               0.0003122990899549805,\n",
       "               0.0003123008475473889,\n",
       "               0.0003123008475473889,\n",
       "               0.0003123101589093184,\n",
       "               0.00031230166234126524,\n",
       "               0.00031229750160169823,\n",
       "               0.0003122964664647941,\n",
       "               0.0003123007068774206,\n",
       "               0.0003122969343744267,\n",
       "               0.000312298251108578,\n",
       "               0.00031230190583403903,\n",
       "               0.00031228944287160917,\n",
       "               0.0003122924336101909,\n",
       "               0.0003122964943339902,\n",
       "               0.0003122847020231261,\n",
       "               0.000312283002437231,\n",
       "               0.0003122992383564623,\n",
       "               0.0003122933793877375,\n",
       "               0.00031229797058638704,\n",
       "               0.00031229797058638704,\n",
       "               0.0003123040667356814,\n",
       "               0.00031229531360447494,\n",
       "               0.0003122941035423989,\n",
       "               0.0003122909121255777,\n",
       "               0.00031229158924833526,\n",
       "               0.0003122872498736288,\n",
       "               0.0003122809434117828,\n",
       "               0.0003122875244392493,\n",
       "               0.0003122812044615207,\n",
       "               0.00031227421374704206,\n",
       "               0.00031227675873754854,\n",
       "               0.00031227386047679074,\n",
       "               0.00031227284159730806,\n",
       "               0.0003122679926378302,\n",
       "               0.0003122802518374634,\n",
       "               0.0003122745270217014,\n",
       "               0.0003122683515221571,\n",
       "               0.0003122650705190606,\n",
       "               0.0003122679382424377,\n",
       "               0.0003122573906483932,\n",
       "               0.0003122496323310042,\n",
       "               0.00031226357176201313,\n",
       "               0.00031224849581329076,\n",
       "               0.0003122506362115543,\n",
       "               0.0003122597734857428,\n",
       "               0.00031226476008358996,\n",
       "               0.00031225055213054807,\n",
       "               0.0003122539520087713,\n",
       "               0.0003122502134709209,\n",
       "               0.0003122482798015542,\n",
       "               0.00031223844053426147,\n",
       "               0.00031224112122627977,\n",
       "               0.0003122443400178847,\n",
       "               0.0003122435310953946,\n",
       "               0.00031224052093679444,\n",
       "               0.00031224575703810387,\n",
       "               0.0003122328769687266,\n",
       "               0.00031223952070071303,\n",
       "               0.0003122468917827072,\n",
       "               0.0003122431639537016,\n",
       "               0.00031223889463795134,\n",
       "               0.00031223598803486774,\n",
       "               0.0003122302455198377,\n",
       "               0.0003122368381918283,\n",
       "               0.00031223599216304113],\n",
       "              'train-Logloss-std': [0.0013376061789884962,\n",
       "               0.0029774055936431835,\n",
       "               0.006474305661923,\n",
       "               0.0048036175592106045,\n",
       "               0.005647681886447325,\n",
       "               0.005599212278566837,\n",
       "               0.009205644776639163,\n",
       "               0.007379153293358602,\n",
       "               0.005656941934878318,\n",
       "               0.004540408317890361,\n",
       "               0.0026604791884449583,\n",
       "               0.003984825339316541,\n",
       "               0.0036826512797342404,\n",
       "               0.0024333120736142974,\n",
       "               0.00233277742755895,\n",
       "               0.0022091416714820553,\n",
       "               0.001933694903359219,\n",
       "               0.0014131333648726735,\n",
       "               0.001375795172578664,\n",
       "               0.0012291681352769764,\n",
       "               0.0003878963025544715,\n",
       "               0.0004966185832013136,\n",
       "               0.0006570827093816893,\n",
       "               0.0008867952016190376,\n",
       "               0.000832604895166889,\n",
       "               0.0006612870155046328,\n",
       "               0.0005401364923541195,\n",
       "               0.0005952530995980863,\n",
       "               0.0005664458856512978,\n",
       "               0.0006411879340564376,\n",
       "               0.0006857362696701602,\n",
       "               0.0006732768080101511,\n",
       "               0.0005791518721158181,\n",
       "               0.0006274991975084736,\n",
       "               0.0005019278067998339,\n",
       "               0.0005082217480606231,\n",
       "               0.0006340380210053971,\n",
       "               0.0005956878452531511,\n",
       "               0.0006618193851456427,\n",
       "               0.0005443696012994456,\n",
       "               0.0005594349211326881,\n",
       "               0.00061034062950741,\n",
       "               0.0006070951982470546,\n",
       "               0.0005766846966643875,\n",
       "               0.0005189889874845445,\n",
       "               0.00047965099109617695,\n",
       "               0.00047790118397838426,\n",
       "               0.00048649393118492095,\n",
       "               0.00045659467872691506,\n",
       "               0.0004195812794632385,\n",
       "               0.0004229062424430075,\n",
       "               0.0003625322327027145,\n",
       "               0.0003632690488575065,\n",
       "               0.00039059448211726334,\n",
       "               0.0003641254300696818,\n",
       "               0.0003360719557832074,\n",
       "               0.00030166184372248044,\n",
       "               0.0002571313090762182,\n",
       "               0.0002274828081016426,\n",
       "               0.00021648902734358328,\n",
       "               0.0002111063118209831,\n",
       "               0.00019414903741357933,\n",
       "               0.00016561571224419725,\n",
       "               0.0001491572314787513,\n",
       "               0.00015153294508649164,\n",
       "               0.00013398344297218306,\n",
       "               0.0001244254256584762,\n",
       "               0.00011270670846652807,\n",
       "               0.00011456086541889322,\n",
       "               0.0001115635392041541,\n",
       "               0.0001098197375856007,\n",
       "               9.438897607498389e-05,\n",
       "               9.848106109309257e-05,\n",
       "               9.974999414576575e-05,\n",
       "               9.273926200751907e-05,\n",
       "               8.910792641554615e-05,\n",
       "               8.914857935461387e-05,\n",
       "               7.939286299424077e-05,\n",
       "               7.363560473631054e-05,\n",
       "               6.415310179497349e-05,\n",
       "               5.464294122203944e-05,\n",
       "               5.5523521876867937e-05,\n",
       "               6.0346483142765014e-05,\n",
       "               5.5656567291547393e-05,\n",
       "               5.1055915395348015e-05,\n",
       "               4.210800591281284e-05,\n",
       "               3.961051097043326e-05,\n",
       "               4.529571046244814e-05,\n",
       "               4.68720584423217e-05,\n",
       "               4.7974810754739174e-05,\n",
       "               4.631292346787293e-05,\n",
       "               4.234413592465674e-05,\n",
       "               4.452868897814944e-05,\n",
       "               3.948052336955364e-05,\n",
       "               3.979999278753251e-05,\n",
       "               3.845902273551777e-05,\n",
       "               4.0274794036320406e-05,\n",
       "               3.739427101705017e-05,\n",
       "               3.7726946898344894e-05,\n",
       "               3.550438080422635e-05,\n",
       "               3.948471033411229e-05,\n",
       "               3.301570720692359e-05,\n",
       "               3.773040252596527e-05,\n",
       "               4.0153388094260315e-05,\n",
       "               4.3899733449338225e-05,\n",
       "               4.336068762980492e-05,\n",
       "               4.226782926777689e-05,\n",
       "               4.254988165009886e-05,\n",
       "               4.3834383804557486e-05,\n",
       "               4.3452260165299484e-05,\n",
       "               4.345524247780102e-05,\n",
       "               4.283849112083304e-05,\n",
       "               4.211655022158486e-05,\n",
       "               4.301078917257511e-05,\n",
       "               4.2250646089672137e-05,\n",
       "               3.808143197207536e-05,\n",
       "               4.067394546353664e-05,\n",
       "               3.8081862511108355e-05,\n",
       "               3.623155127998392e-05,\n",
       "               3.760723407162578e-05,\n",
       "               3.475814923495334e-05,\n",
       "               3.3834822644437605e-05,\n",
       "               2.8134390173398793e-05,\n",
       "               2.7690289092992378e-05,\n",
       "               2.467168970126114e-05,\n",
       "               2.550394787754302e-05,\n",
       "               2.5513378466660725e-05,\n",
       "               2.734323280507044e-05,\n",
       "               2.7312893413959616e-05,\n",
       "               2.601012764033539e-05,\n",
       "               2.5996638944147323e-05,\n",
       "               2.6640672309592478e-05,\n",
       "               2.6118390240534385e-05,\n",
       "               2.575696433977729e-05,\n",
       "               2.5747238380019116e-05,\n",
       "               2.807743270518195e-05,\n",
       "               2.5365507635680072e-05,\n",
       "               2.538853163080918e-05,\n",
       "               2.5371731165312247e-05,\n",
       "               2.5381198560731374e-05,\n",
       "               2.5398649650216963e-05,\n",
       "               2.5510387635339888e-05,\n",
       "               2.3804582176997766e-05,\n",
       "               2.3781265523974452e-05,\n",
       "               2.3152733271105695e-05,\n",
       "               2.4078235129111807e-05,\n",
       "               2.4093358393646206e-05,\n",
       "               2.7597518586644977e-05,\n",
       "               2.652472422010491e-05,\n",
       "               2.5786337357179647e-05,\n",
       "               2.5776723785944965e-05,\n",
       "               2.4258189870811087e-05,\n",
       "               2.408540571325484e-05,\n",
       "               2.341818297056967e-05,\n",
       "               2.3725153645157e-05,\n",
       "               2.37154400911288e-05,\n",
       "               2.370471746890326e-05,\n",
       "               2.372242215148124e-05,\n",
       "               2.3729987036311912e-05,\n",
       "               2.3720614702732627e-05,\n",
       "               2.3696454752322256e-05,\n",
       "               2.3715823780908955e-05,\n",
       "               2.279813086390708e-05,\n",
       "               2.2792198773008617e-05,\n",
       "               2.2796436148456618e-05,\n",
       "               2.197942618053716e-05,\n",
       "               2.198871546025087e-05,\n",
       "               2.1976822159930543e-05,\n",
       "               2.2466522519376723e-05,\n",
       "               2.248727781278292e-05,\n",
       "               2.2472521824430967e-05,\n",
       "               2.2477691083068178e-05,\n",
       "               2.2474713091982742e-05,\n",
       "               2.3180907757624882e-05,\n",
       "               2.318204660238528e-05,\n",
       "               2.318680138510235e-05,\n",
       "               2.3196247631598633e-05,\n",
       "               2.3209018186805724e-05,\n",
       "               2.3197281231663867e-05,\n",
       "               2.321394752645022e-05,\n",
       "               2.3192064766163506e-05,\n",
       "               2.129790970116005e-05,\n",
       "               2.1625771495441585e-05,\n",
       "               2.162677612481093e-05,\n",
       "               2.1597490510503522e-05,\n",
       "               2.1616866433777898e-05,\n",
       "               2.1625588882770154e-05,\n",
       "               2.1621054851205174e-05,\n",
       "               2.158937434556175e-05,\n",
       "               2.1612342930590472e-05,\n",
       "               1.780531835364597e-05,\n",
       "               1.7816960205295636e-05,\n",
       "               1.779971745225976e-05,\n",
       "               1.7813129302197845e-05,\n",
       "               1.7803439147131683e-05,\n",
       "               1.7814767960309832e-05,\n",
       "               1.7824051750122834e-05,\n",
       "               1.7795460460619163e-05,\n",
       "               1.780978891577837e-05,\n",
       "               1.7809875151379447e-05,\n",
       "               1.7822507225205857e-05,\n",
       "               1.7808721979586243e-05,\n",
       "               1.783083322897463e-05,\n",
       "               1.781216777650934e-05,\n",
       "               1.7796831325003462e-05,\n",
       "               1.7792773643773978e-05,\n",
       "               1.7804895796497426e-05,\n",
       "               1.7828905554353884e-05,\n",
       "               1.7824269771181245e-05,\n",
       "               1.7797677255862806e-05,\n",
       "               1.7785725430001232e-05,\n",
       "               1.779063770535774e-05,\n",
       "               1.7797903820443552e-05,\n",
       "               1.7814062804030594e-05,\n",
       "               1.7794214779209117e-05,\n",
       "               1.7798436831215807e-05,\n",
       "               1.7808935689197252e-05,\n",
       "               1.7809626204260824e-05,\n",
       "               1.7803219776939732e-05,\n",
       "               1.7794483432840877e-05,\n",
       "               1.7790016709758557e-05,\n",
       "               1.780887167283967e-05,\n",
       "               1.7800340909986824e-05,\n",
       "               1.7795989707019154e-05,\n",
       "               1.7797043257607114e-05,\n",
       "               1.7815942566366894e-05,\n",
       "               1.779261913400691e-05,\n",
       "               1.7774917427329282e-05,\n",
       "               1.777625365094517e-05,\n",
       "               1.7810111116025144e-05,\n",
       "               1.7852483222364993e-05,\n",
       "               1.7848683831083037e-05,\n",
       "               1.7902657675652788e-05,\n",
       "               1.788555791961513e-05,\n",
       "               1.7883521558488805e-05,\n",
       "               1.7882783076985918e-05,\n",
       "               1.7884914192178637e-05,\n",
       "               1.7863818198470952e-05,\n",
       "               1.7876507622951973e-05,\n",
       "               1.788326304344091e-05,\n",
       "               1.7897183670924195e-05,\n",
       "               1.7879387090380226e-05,\n",
       "               1.7869687218051472e-05,\n",
       "               1.7874690488371116e-05,\n",
       "               1.788388100918188e-05,\n",
       "               1.7882207591409265e-05,\n",
       "               1.787179977849956e-05,\n",
       "               1.785856385011573e-05,\n",
       "               1.7861727401108176e-05,\n",
       "               1.7864784736691122e-05,\n",
       "               1.7867834877822776e-05,\n",
       "               1.78638671830342e-05,\n",
       "               1.7862675554027167e-05,\n",
       "               1.784100705629708e-05,\n",
       "               1.7853302024956324e-05,\n",
       "               1.787153103733583e-05,\n",
       "               1.7865939479589527e-05,\n",
       "               1.7866500065387477e-05,\n",
       "               1.7888126633702353e-05,\n",
       "               1.784784541215656e-05,\n",
       "               1.7842898532920946e-05,\n",
       "               1.7855035873376028e-05,\n",
       "               1.7861117003532735e-05,\n",
       "               1.786627128387305e-05,\n",
       "               1.7875906800674484e-05,\n",
       "               1.7874101844893333e-05,\n",
       "               1.7860588418701485e-05,\n",
       "               1.786829869976391e-05,\n",
       "               1.786687677148606e-05,\n",
       "               1.7864058575862653e-05,\n",
       "               1.785567140829469e-05,\n",
       "               1.7857578308249448e-05,\n",
       "               1.7871590898246798e-05,\n",
       "               1.7881090257738993e-05,\n",
       "               1.7874180180679108e-05,\n",
       "               1.787867019944569e-05,\n",
       "               1.7872010732139333e-05,\n",
       "               1.786628092338808e-05,\n",
       "               1.7888971282436287e-05,\n",
       "               1.788960612074979e-05,\n",
       "               1.789651011165753e-05,\n",
       "               1.7892886565993325e-05,\n",
       "               1.7895661243021612e-05,\n",
       "               1.7896987768121914e-05,\n",
       "               1.7897630211393018e-05,\n",
       "               1.790197417541659e-05,\n",
       "               1.7896076650973813e-05,\n",
       "               1.7912521390552722e-05,\n",
       "               1.793257200044906e-05,\n",
       "               1.79347241186302e-05,\n",
       "               1.7913580500161554e-05,\n",
       "               1.7922806521327153e-05,\n",
       "               1.7929208008700647e-05,\n",
       "               1.7918804926634803e-05,\n",
       "               1.7924046023342072e-05,\n",
       "               1.7955226286417965e-05,\n",
       "               1.7953844280118554e-05,\n",
       "               1.7941653704613962e-05,\n",
       "               1.7947679696814e-05,\n",
       "               1.7948659080983927e-05,\n",
       "               1.7940214861685807e-05,\n",
       "               1.7960049634456163e-05,\n",
       "               1.7966379012976576e-05,\n",
       "               1.7955460427052255e-05,\n",
       "               1.7960837739572805e-05,\n",
       "               1.797027918678433e-05,\n",
       "               1.79624071463267e-05,\n",
       "               1.7956917553316525e-05,\n",
       "               1.797037818060519e-05,\n",
       "               1.79667597273906e-05,\n",
       "               1.79666493194375e-05,\n",
       "               1.7977843494819833e-05,\n",
       "               1.7979386931139244e-05,\n",
       "               1.7981651264636986e-05,\n",
       "               1.796774774906563e-05,\n",
       "               1.7968733637943796e-05,\n",
       "               1.7976425999371614e-05,\n",
       "               1.7996840023707652e-05,\n",
       "               1.7999790469279867e-05,\n",
       "               1.7999757591984617e-05,\n",
       "               1.7985443810304477e-05,\n",
       "               1.7990843644113277e-05,\n",
       "               1.798732514224754e-05,\n",
       "               1.799073473767391e-05,\n",
       "               1.7987421401292193e-05,\n",
       "               1.8003283254102157e-05,\n",
       "               1.799309084211461e-05,\n",
       "               1.8002015781131828e-05,\n",
       "               1.801565488248983e-05,\n",
       "               1.8018228071551163e-05,\n",
       "               1.8006124100604227e-05,\n",
       "               1.8009940087609395e-05,\n",
       "               1.8002426950733633e-05,\n",
       "               1.801264665278885e-05,\n",
       "               1.8012494737074157e-05,\n",
       "               1.8012287829063866e-05,\n",
       "               1.8026613604621164e-05,\n",
       "               1.8026026010232305e-05,\n",
       "               1.8030606883455685e-05,\n",
       "               1.80202139491062e-05,\n",
       "               1.8025746029098416e-05,\n",
       "               1.8023977005425835e-05,\n",
       "               1.8008453880151326e-05,\n",
       "               1.8022209444344797e-05,\n",
       "               1.802564229448379e-05,\n",
       "               1.8024648032182808e-05,\n",
       "               1.8024542653911988e-05,\n",
       "               1.8031567480647086e-05,\n",
       "               1.8039960797534945e-05,\n",
       "               1.8025811634893084e-05,\n",
       "               1.8021451622451075e-05,\n",
       "               1.8037593364481925e-05,\n",
       "               1.8019911640002256e-05,\n",
       "               1.8024293005462546e-05,\n",
       "               1.803214356500412e-05,\n",
       "               1.803687779272025e-05,\n",
       "               1.8046041395236356e-05,\n",
       "               1.8040194815904985e-05,\n",
       "               1.805507079422479e-05,\n",
       "               1.804716911796366e-05,\n",
       "               1.803910817122912e-05,\n",
       "               1.8039320528040953e-05,\n",
       "               1.8044609940855752e-05,\n",
       "               1.804737724975269e-05,\n",
       "               1.8063390716984055e-05,\n",
       "               1.8038213318486413e-05,\n",
       "               1.8045855083220197e-05,\n",
       "               1.8045950720724563e-05,\n",
       "               1.805676715546667e-05,\n",
       "               1.8056947432219007e-05,\n",
       "               1.8060274397793343e-05,\n",
       "               1.8058002802958453e-05,\n",
       "               1.806462456723342e-05,\n",
       "               1.8053405098184067e-05,\n",
       "               1.8081867061179696e-05,\n",
       "               1.8073592858703575e-05,\n",
       "               1.807506112934129e-05,\n",
       "               1.8064376279230896e-05,\n",
       "               1.80552202457994e-05,\n",
       "               1.8065383145228005e-05,\n",
       "               1.8083992062981497e-05,\n",
       "               1.8085514585989146e-05,\n",
       "               1.8087256791486138e-05,\n",
       "               1.8087624435873578e-05,\n",
       "               1.809364120411225e-05,\n",
       "               1.8090101992293945e-05,\n",
       "               1.8090101992293945e-05,\n",
       "               1.8088573582115615e-05,\n",
       "               1.8095675119213045e-05,\n",
       "               1.8092224845058766e-05,\n",
       "               1.8093632753774034e-05,\n",
       "               1.809385885904421e-05,\n",
       "               1.809602674470273e-05,\n",
       "               1.809115982990593e-05,\n",
       "               1.810094085299845e-05,\n",
       "               1.8106050973338383e-05,\n",
       "               1.810469453736535e-05,\n",
       "               1.809873549628927e-05,\n",
       "               1.809614202622141e-05,\n",
       "               1.8096637198803722e-05,\n",
       "               1.8104007738765386e-05,\n",
       "               1.811254724806076e-05,\n",
       "               1.8109142094851075e-05,\n",
       "               1.8109142094851075e-05,\n",
       "               1.8101385102378234e-05,\n",
       "               1.810110189718048e-05,\n",
       "               1.810418706758274e-05,\n",
       "               1.8098764866178675e-05,\n",
       "               1.8098764866178675e-05,\n",
       "               1.809583650761181e-05,\n",
       "               1.809829176215811e-05,\n",
       "               1.809829176215811e-05,\n",
       "               1.81119669431701e-05,\n",
       "               1.810949182007911e-05,\n",
       "               1.810949182007911e-05,\n",
       "               1.810483669700585e-05,\n",
       "               1.810483669700585e-05,\n",
       "               1.810744048913129e-05,\n",
       "               1.810744048913129e-05,\n",
       "               1.8110665876679195e-05,\n",
       "               1.8104629851455908e-05,\n",
       "               1.8107443932135702e-05,\n",
       "               1.810220708112892e-05,\n",
       "               1.810220708112892e-05,\n",
       "               1.810220708112892e-05,\n",
       "               1.810445135939931e-05,\n",
       "               1.8100568640669484e-05,\n",
       "               1.8106967853896165e-05,\n",
       "               1.8105945038613642e-05,\n",
       "               1.810251907380852e-05,\n",
       "               1.810251907380852e-05,\n",
       "               1.810251907380852e-05,\n",
       "               1.810271243169526e-05,\n",
       "               1.811532225740915e-05,\n",
       "               1.811532225740915e-05,\n",
       "               1.8116316038223122e-05,\n",
       "               1.8115431248252832e-05,\n",
       "               1.8115431248252832e-05,\n",
       "               1.8110757319777172e-05,\n",
       "               1.8115021347290176e-05,\n",
       "               1.811639969303779e-05,\n",
       "               1.8117474167467415e-05,\n",
       "               1.8115337496012747e-05,\n",
       "               1.8117238164590022e-05,\n",
       "               1.811657434188707e-05,\n",
       "               1.8114734216595528e-05,\n",
       "               1.8121023596445352e-05,\n",
       "               1.8117620091028465e-05,\n",
       "               1.811710562909486e-05,\n",
       "               1.812766578220978e-05,\n",
       "               1.8127998424749684e-05,\n",
       "               1.811518564064615e-05,\n",
       "               1.81181448887425e-05,\n",
       "               1.8115825203729898e-05,\n",
       "               1.8115825203729898e-05,\n",
       "               1.8110500233784514e-05,\n",
       "               1.811690253485379e-05,\n",
       "               1.811751443664132e-05,\n",
       "               1.8120825682850894e-05,\n",
       "               1.8120483441158727e-05,\n",
       "               1.8122678794237978e-05,\n",
       "               1.813059301903903e-05,\n",
       "               1.8121225367915375e-05,\n",
       "               1.812778212807595e-05,\n",
       "               1.8131330596550464e-05,\n",
       "               1.813003728622714e-05,\n",
       "               1.8131510254898175e-05,\n",
       "               1.813597009560331e-05,\n",
       "               1.8138427721323367e-05,\n",
       "               1.812811372710308e-05,\n",
       "               1.8131168676945336e-05,\n",
       "               1.813431348110325e-05,\n",
       "               1.8135988350422783e-05,\n",
       "               1.8133294451300792e-05,\n",
       "               1.814213861613497e-05,\n",
       "               1.814611399304609e-05,\n",
       "               1.8138245037235753e-05,\n",
       "               1.8150847321558126e-05,\n",
       "               1.8148624469425446e-05,\n",
       "               1.8139146341782036e-05,\n",
       "               1.8134459645775993e-05,\n",
       "               1.8147114921815615e-05,\n",
       "               1.8145375131539486e-05,\n",
       "               1.8150010481021003e-05,\n",
       "               1.8150997909134585e-05,\n",
       "               1.8156037448595518e-05,\n",
       "               1.8154661930530956e-05,\n",
       "               1.8150557109819456e-05,\n",
       "               1.8150972666797356e-05,\n",
       "               1.815309623369615e-05,\n",
       "               1.815040734570727e-05,\n",
       "               1.815703447127597e-05,\n",
       "               1.815100645373641e-05,\n",
       "               1.8147208079464362e-05,\n",
       "               1.8149127286477052e-05,\n",
       "               1.8151329721567282e-05,\n",
       "               1.8157070381949495e-05,\n",
       "               1.8157771150918918e-05,\n",
       "               1.815436628117309e-05,\n",
       "               1.8154802589777498e-05]})}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_model.grid_search(params_grid, \n",
    "                      tsfresh_features_selected, \n",
    "                      Y_train[:,0], \n",
    "                      cv=5, \n",
    "                      plot=True, \n",
    "                      refit=True, \n",
    "                      stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 200,\n",
       " 'auto_class_weights': 'Balanced',\n",
       " 'random_state': 42,\n",
       " 'depth': 3,\n",
       " 'learning_rate': 0.05,\n",
       " 'iterations': 500}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learn': {'Logloss': 0.00029134848656418226}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvL3j7eDnRnF"
   },
   "source": [
    "Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "h3fPyiNInRnF"
   },
   "outputs": [],
   "source": [
    "tsfresh_df = tsfresh_formatting(X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2])), index_cur_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ygA0Zc4HnRnF"
   },
   "outputs": [],
   "source": [
    "tsfresh_features = extract_tsfeatures(tsfresh_df, tsf_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_features_test = tsfresh_features[tsfresh_features_selected.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "d9WmEBcXnRnG"
   },
   "outputs": [],
   "source": [
    "Y_pred = ctb_model.predict_proba(tsfresh_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "5Uqxbkv6nRnG",
    "outputId": "2ba42838-54ed-435c-d33d-90c4d216475c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027310578712237056"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "b6jBO8PVnRnG",
    "outputId": "3d8c1a89-f56c-41e5-da03-2a727b29d091"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99892961e-01, 1.07039483e-04],\n",
       "       [9.99892961e-01, 1.07039483e-04],\n",
       "       [9.99892961e-01, 1.07039483e-04],\n",
       "       ...,\n",
       "       [9.99814700e-01, 1.85299595e-04],\n",
       "       [9.99814700e-01, 1.85299595e-04],\n",
       "       [9.99814700e-01, 1.85299595e-04]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график с результатами работы модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(probability, graph_name):\n",
    "    fig = make_subplots(rows=2, cols=1, vertical_spacing=0.1,\n",
    "                        specs=[[{\"secondary_y\": True}],[{\"secondary_y\": True}]], \n",
    "                        shared_xaxes = True)\n",
    "\n",
    "    for col in X.columns:\n",
    "            fig.add_trace(go.Scattergl( x = X.index,\n",
    "                                        y = X[col].values,\n",
    "                                        mode = 'lines',\n",
    "                                        name = col,\n",
    "                                        showlegend =True),\n",
    "                                        row = 1, col = 1)\n",
    "\n",
    "    fig.add_trace(go.Scattergl(x = index_cur_time_test,\n",
    "                                        y = probability * 100,\n",
    "                                        mode = 'lines', \n",
    "                                        name = 'probability_failure', \n",
    "                                        showlegend =True),\n",
    "                                        row = 2, col = 1)\n",
    "\n",
    "    fig.add_trace(go.Bar(x = X_states.index,\n",
    "                                y = X_states['offline'].values,\n",
    "                                name = 'offline',\n",
    "                                showlegend =True,\n",
    "                                opacity=0.2,\n",
    "                                hovertemplate='%{y:.2f}',\n",
    "                                width=100,\n",
    "                                marker_color='black',\n",
    "                                marker_line_color='black'\n",
    "                                ),\n",
    "                        secondary_y=True,\n",
    "                        row = 1, col = 1)\n",
    "\n",
    "    fig.add_trace(go.Bar(x = X_states.index,\n",
    "                                y = X_states['green_failurs'].values,\n",
    "                                name = 'green_failurs',\n",
    "                                showlegend =True,\n",
    "                                opacity=0.2,\n",
    "                                hovertemplate='%{y:.2f}',\n",
    "                                width=100,\n",
    "                                marker_color='red',\n",
    "                                marker_line_color='red'\n",
    "                                ),\n",
    "                        secondary_y=True,\n",
    "                        row = 1, col = 1)\n",
    "\n",
    "    fig.add_trace(go.Bar(x = X_states.index,\n",
    "                                y = X_states['Normal'].values,\n",
    "                                name = 'Normal',\n",
    "                                showlegend =True,\n",
    "                                opacity=0.2,\n",
    "                                hovertemplate='%{y:.2f}',\n",
    "                                width=100,\n",
    "                                marker_color='green',\n",
    "                                marker_line_color='green'\n",
    "                                ),\n",
    "                        secondary_y=True,\n",
    "                        row = 1, col = 1)\n",
    "\n",
    "    fig['layout']['showlegend'] = True\n",
    "    fig['layout']['title'] = graph_name\n",
    "    fig.update_layout(hovermode = \"x unified\")\n",
    "\n",
    "    plotly.offline.plot(fig, filename = f\"graphs/{graph_name}.html\",  show_link=False, auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred[:, 1], 'tsfresh_ctb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            roc_auc        f1      MCC\n",
       "tsfresh_cb  0.56036  0.200074  0.23308"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['tsfresh_cb', :] = calc_metrics(X_states, Y_pred[:,1])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем модель с тремя сверточными слоями. Будем использовать масштабированные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "hlk-anL2nRnJ"
   },
   "outputs": [],
   "source": [
    "X_scaled, scaler = scale_data(X, train_periods, failurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cur_time_train, X_train, Y_train = create_train_sample(X_scaled, train_periods, failurs, win_size=win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cur_time_test, X_test = create_test_sample(X_scaled, offline_periods, win_size=win_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_1_'></a>[CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "_QjqAyOBnRnJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "HnD4nHCZnRnK"
   },
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=11, activation='relu', padding=\"same\")(input_layer)\n",
    "    # conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=11, activation='relu', padding=\"same\")(conv1)\n",
    "    # conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=11,activation='relu', padding=\"same\")(conv2)\n",
    "    # conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "\n",
    "    flat = tf.keras.layers.Flatten()(conv3)\n",
    "\n",
    "    dense1 = tf.keras.layers.Dense(100, activation='relu')(flat)\n",
    "    output_layer = tf.keras.layers.Dense(2, activation=\"softmax\")(dense1)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWELL(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, factor, verbose):\n",
    "        super(DWELL, self).__init__()\n",
    "        \n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.learning_rate)) # get the initiallearning rate and save it  \n",
    "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.factor = factor # во сколько раз будет уменьшаться lr\n",
    "        self._model = model\n",
    "        self.best_weights = self._model.get_weights() # set best weights to model's initial weights\n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate)) # get the current learning rate        \n",
    "        vloss = logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        if vloss > self.lowest_vloss:\n",
    "            best_model = load_model(\"CNN_callbacks/best_model.keras\")\n",
    "            self.model.set_weights(best_model.get_weights())\n",
    "            new_lr = lr * self.factor\n",
    "            tf.keras.backend.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "            if self.verbose:\n",
    "                print( '\\n model weights reset to best weights and reduced lr to ', new_lr, flush=True)\n",
    "        else:\n",
    "            self.lowest_vloss=vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "GkNomi03nRnK",
    "outputId": "cdad7e00-25e2-4714-8dfa-d89765347a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9848 - loss: 0.2103\n",
      "Epoch 1: val_loss improved from inf to 0.00297, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9850 - loss: 0.2073 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 2/20\n",
      "\u001b[1m278/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9959 - loss: 0.0141\n",
      "Epoch 2: val_loss improved from 0.00297 to 0.00006, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9959 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 5.7195e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m280/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.0113e-04\n",
      "Epoch 3: val_loss improved from 0.00006 to 0.00001, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0100e-04 - val_accuracy: 1.0000 - val_loss: 1.1258e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m272/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.1205e-05\n",
      "Epoch 4: val_loss improved from 0.00001 to 0.00001, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.1127e-05 - val_accuracy: 1.0000 - val_loss: 6.1893e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m274/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.4744e-05\n",
      "Epoch 5: val_loss improved from 0.00001 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4738e-05 - val_accuracy: 1.0000 - val_loss: 4.1353e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m273/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7175e-05\n",
      "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7178e-05 - val_accuracy: 1.0000 - val_loss: 2.9771e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2633e-05\n",
      "Epoch 7: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.2635e-05 - val_accuracy: 1.0000 - val_loss: 2.2288e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.6358e-06\n",
      "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.6379e-06 - val_accuracy: 1.0000 - val_loss: 1.7245e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m279/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.5278e-06\n",
      "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.5289e-06 - val_accuracy: 1.0000 - val_loss: 1.3612e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m274/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.9824e-06\n",
      "Epoch 10: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.9849e-06 - val_accuracy: 1.0000 - val_loss: 1.0918e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m279/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.8287e-06\n",
      "Epoch 11: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.8296e-06 - val_accuracy: 1.0000 - val_loss: 8.8146e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m280/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.9451e-06\n",
      "Epoch 12: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.9456e-06 - val_accuracy: 1.0000 - val_loss: 7.1722e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.2530e-06\n",
      "Epoch 13: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.2539e-06 - val_accuracy: 1.0000 - val_loss: 5.9381e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m280/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.7023e-06\n",
      "Epoch 14: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.7026e-06 - val_accuracy: 1.0000 - val_loss: 4.9327e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m276/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.2592e-06\n",
      "Epoch 15: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.2601e-06 - val_accuracy: 1.0000 - val_loss: 4.1364e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m276/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.8989e-06\n",
      "Epoch 16: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.8996e-06 - val_accuracy: 1.0000 - val_loss: 3.4867e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m271/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.6014e-06\n",
      "Epoch 17: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.6026e-06 - val_accuracy: 1.0000 - val_loss: 2.9317e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.3594e-06\n",
      "Epoch 18: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.3594e-06 - val_accuracy: 1.0000 - val_loss: 2.4604e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m273/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.1570e-06\n",
      "Epoch 19: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1576e-06 - val_accuracy: 1.0000 - val_loss: 2.0732e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m275/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.8786e-07\n",
      "Epoch 20: val_loss improved from 0.00000 to 0.00000, saving model to CNN_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 9.8827e-07 - val_accuracy: 1.0000 - val_loss: 1.7577e-07\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = make_model(input_shape=X_train.shape[1:])\n",
    "\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "    loss= 'BinaryCrossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "callbacks=[ModelCheckpoint(\"CNN_callbacks/best_model.keras\", monitor = 'val_loss', save_best_only = True, mode = 'min', verbose = 1),\n",
    "          EarlyStopping(monitor=\"val_loss\", patience=40, verbose=1),\n",
    "          DWELL(model = model, factor=0.985, verbose=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.3,\n",
    "    shuffle = False,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "KO0PWqBmnRnK",
    "outputId": "f519f7cc-c8fe-4a36-975c-55f72239e067"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'],\n",
    "         'b',\n",
    "         label='Training loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_EKrME8nRnK"
   },
   "source": [
    "Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "89xPuFaCnRnL",
    "outputId": "ed964ecf-6bb5-4c66-8994-b8a001999746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2501/2501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred[:, 0], 'cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVpJ_vbdnRnM"
   },
   "source": [
    "Без масштабирования окон приемлемо, ложных срабатывния нет, все отказы обнаружил, но некоторые с большим запаздыванием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['cnn', :] = calc_metrics(X_states, Y_pred[:,0])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_2_'></a>[CNN на смещенном паттерне](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По графику работы CNN видно, что модель пропускает те отказы, которые по уровню сильно выше или ниже обучающего отказа. Проверим это, протестировав модель на смещенных отказах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "0ppWV2nvnRnN"
   },
   "outputs": [],
   "source": [
    "X_biased = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIej42IZnRnN"
   },
   "source": [
    "Диапазон обучающего отказа 85 - 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkIvX8GwnRnN"
   },
   "source": [
    "Должен исчезнуть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "kYa-42zcnRnN"
   },
   "outputs": [],
   "source": [
    "X_biased.loc['2022-04-16 00:00:00':'2022-04-25 06:40:00',:] = X_biased.loc['2022-04-16 00:00:00':'2022-04-25 06:40:00',:] + 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA-FKbhmnRnN"
   },
   "source": [
    "Должен обнаружиться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "FKBFM_nCnRnN"
   },
   "outputs": [],
   "source": [
    "X_biased.loc['2022-08-11 00:00:00':'2022-08-17 00:40:00',:] = X_biased.loc['2022-08-11 00:00:00':'2022-08-17 00:40:00',:] - 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WIac0I6nRnO"
   },
   "source": [
    "Должен обнаружиться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "4itqrAw6nRnO"
   },
   "outputs": [],
   "source": [
    "X_biased.loc['2023-09-16 00:00:00':'2023-09-22 05:10:00',:] = X_biased.loc['2023-09-16 00:00:00':'2023-09-22 05:10:00',:] + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_biased_scaled, scaler = scale_data(X_biased, train_periods, failurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cur_time_test, X_test_biased = create_test_sample(X_biased_scaled, offline_periods, win_size=win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "LPdJ-akknRnP",
    "outputId": "3a981afb-3032-4457-bf3f-30300128e250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2501/2501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSBTOAZ5nRnP"
   },
   "source": [
    "Рисуем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeVEANo3nRnP",
    "outputId": "e09c6c8a-5d5a-4eab-8b33-f966af72bdc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/cnn_biased.html'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = make_subplots(rows=2, cols=1, vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}],[{\"secondary_y\": True}]], \n",
    "                    shared_xaxes = True)\n",
    "\n",
    "for col in X_biased.columns:\n",
    "        fig.add_trace(go.Scattergl( x = X_biased.index,\n",
    "                                    y = X_biased[col].values,\n",
    "                                    mode = 'lines',\n",
    "                                    name = col,\n",
    "                                    showlegend =True),\n",
    "                                    row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x = index_cur_time_test,\n",
    "                                    y = Y_pred[:, 0]*100,\n",
    "                                    mode = 'lines', name = 'probability_failure', showlegend =True),\n",
    "                                    row = 2, col = 1)\n",
    "\n",
    "fig.add_trace(go.Bar(x = X_states.index,\n",
    "                             y = X_states['offline'].values,\n",
    "                             name = 'offline',\n",
    "                             showlegend =True,\n",
    "                             opacity=0.2,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             width=100,\n",
    "                             marker_color='black',\n",
    "                             marker_line_color='black'\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Bar(x = X_states.index,\n",
    "                             y = X_states['green_failurs'].values,\n",
    "                             name = 'green_failurs',\n",
    "                             showlegend =True,\n",
    "                             opacity=0.2,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             width=100,\n",
    "                             marker_color='red',\n",
    "                             marker_line_color='red'\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "fig.add_trace(go.Bar(x = X_states.index,\n",
    "                             y = X_states['Normal'].values,\n",
    "                             name = 'Normal',\n",
    "                             showlegend =True,\n",
    "                             opacity=0.2,\n",
    "                             hovertemplate='%{y:.2f}',\n",
    "                             width=100,\n",
    "                             marker_color='green',\n",
    "                             marker_line_color='green'\n",
    "                            ),\n",
    "                      secondary_y=True,\n",
    "                      row = 1, col = 1)\n",
    "\n",
    "fig['layout']['showlegend'] = True\n",
    "fig['layout']['title'] = 'cnn_biased'\n",
    "fig.update_layout(hovermode = \"x unified\")\n",
    "\n",
    "\n",
    "plotly.offline.plot(fig, filename = \"graphs/cnn_biased.html\",  show_link=False, auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все как и предполагалось. Теперь масштабируем каждое окно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_3_'></a>[CNN с масштабированием каждого окна](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtDH7bXWnRnQ"
   },
   "source": [
    "4 Масштабирование каждого окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "OIzT51GGnRnR"
   },
   "outputs": [],
   "source": [
    "def scale_window(X):\n",
    "\twin_mean = np.mean(np.mean(X,axis=1),axis=1)\n",
    "\twin_mean = np.tile(win_mean,(X.shape[2],1)).transpose((1,0))\n",
    "\n",
    "\twin_mean = np.tile(win_mean,(X.shape[1],1,1)).transpose((1,0,2))\n",
    "\tX_scaled = X - win_mean\n",
    "\treturn X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "9gkXzYJfnRnR"
   },
   "outputs": [],
   "source": [
    "X_train_scaled = scale_window(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m273/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9797 - loss: 0.4242\n",
      "Epoch 1: val_loss improved from inf to 0.01279, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9801 - loss: 0.4137 - val_accuracy: 0.9995 - val_loss: 0.0128\n",
      "Epoch 2/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0015\n",
      "Epoch 2: val_loss improved from 0.01279 to 0.00018, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 1.7995e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m280/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.3689e-04\n",
      "Epoch 3: val_loss improved from 0.00018 to 0.00005, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.3663e-04 - val_accuracy: 1.0000 - val_loss: 4.7793e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m278/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1964e-04\n",
      "Epoch 4: val_loss improved from 0.00005 to 0.00003, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1952e-04 - val_accuracy: 1.0000 - val_loss: 3.0666e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m279/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.5866e-05\n",
      "Epoch 5: val_loss improved from 0.00003 to 0.00002, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.5829e-05 - val_accuracy: 1.0000 - val_loss: 2.1579e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m278/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.2205e-05\n",
      "Epoch 6: val_loss improved from 0.00002 to 0.00002, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 5.2184e-05 - val_accuracy: 1.0000 - val_loss: 1.5918e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m280/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.7775e-05\n",
      "Epoch 7: val_loss improved from 0.00002 to 0.00001, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.7772e-05 - val_accuracy: 1.0000 - val_loss: 1.2083e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m275/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8348e-05\n",
      "Epoch 8: val_loss improved from 0.00001 to 0.00001, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.8344e-05 - val_accuracy: 1.0000 - val_loss: 9.3887e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m278/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.1800e-05\n",
      "Epoch 9: val_loss improved from 0.00001 to 0.00001, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1800e-05 - val_accuracy: 1.0000 - val_loss: 7.4382e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7063e-05\n",
      "Epoch 10: val_loss improved from 0.00001 to 0.00001, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7065e-05 - val_accuracy: 1.0000 - val_loss: 5.9728e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.3571e-05\n",
      "Epoch 11: val_loss improved from 0.00001 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.3571e-05 - val_accuracy: 1.0000 - val_loss: 4.8508e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.0931e-05\n",
      "Epoch 12: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.0934e-05 - val_accuracy: 1.0000 - val_loss: 3.9702e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m278/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.8859e-06\n",
      "Epoch 13: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.8882e-06 - val_accuracy: 1.0000 - val_loss: 3.2737e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m280/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.2811e-06\n",
      "Epoch 14: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.2823e-06 - val_accuracy: 1.0000 - val_loss: 2.7038e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m276/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.0117e-06\n",
      "Epoch 15: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0146e-06 - val_accuracy: 1.0000 - val_loss: 2.2600e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m273/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.9830e-06\n",
      "Epoch 16: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.9873e-06 - val_accuracy: 1.0000 - val_loss: 1.8865e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m276/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.1538e-06\n",
      "Epoch 17: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.1563e-06 - val_accuracy: 1.0000 - val_loss: 1.5734e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m279/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.4821e-06\n",
      "Epoch 18: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.4833e-06 - val_accuracy: 1.0000 - val_loss: 1.3247e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m277/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.9268e-06\n",
      "Epoch 19: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9284e-06 - val_accuracy: 1.0000 - val_loss: 1.1181e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m275/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.4669e-06\n",
      "Epoch 20: val_loss improved from 0.00000 to 0.00000, saving model to CNN_win_callbacks/best_model.keras\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.4690e-06 - val_accuracy: 1.0000 - val_loss: 9.4396e-07\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = make_model(input_shape=X_train.shape[1:])\n",
    "\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "    loss= 'BinaryCrossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "callbacks=[ModelCheckpoint(\"CNN_win_callbacks/best_model.keras\", monitor = 'val_loss', save_best_only = True, mode = 'min', verbose = 1),\n",
    "          EarlyStopping(monitor=\"val_loss\", patience=40, verbose=1),\n",
    "          DWELL(model = model, factor=0.985, verbose=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.3,\n",
    "    shuffle = False,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "I7lxN1RinRnR",
    "outputId": "6f492c1d-ae59-45e9-b4a0-f70ba2e6d7a4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'],\n",
    "         'b',\n",
    "         label='Training loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "1XJql4_anRnV"
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scale_window(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "z_VTs5AanRnV",
    "outputId": "56e04823-e4e3-4c52-f329-07e7a6923582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2501/2501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukp73Dy0nRnV"
   },
   "source": [
    "Рисуем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred[:, 0], 'cnn_win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742\n",
       "cnn_win     0.880621  0.864438  0.868353"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['cnn_win', :] = calc_metrics(X_states, Y_pred[:,0])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Vanilla Transformer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_1_'></a>[С масштабированием всей выборки](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1731260319829,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "ABcuK3cDJ_kg",
    "outputId": "010455c1-c773-4d1e-be19-10e5311192bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52693921, 9.78015267])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1731260682423,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "D8bZBD83KB1E"
   },
   "outputs": [],
   "source": [
    "weights_tensor = torch.tensor(weights.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1731260688903,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "bG5Atjc3sdHI"
   },
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_dim = 12  # Размерность каждого временного шага\n",
    "num_classes = 2  # Количество классов для классификации\n",
    "seq_length = 48  # Длина временного ряда\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "hidden_dim = 128\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1731260690516,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "eMT-pZNNtAzL"
   },
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1731260696466,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "L3ZZiM_TtjdF"
   },
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          #shuffle=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1731260700138,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "YdXpoU1guXCy"
   },
   "outputs": [],
   "source": [
    "# Определим модель Transformer для классификации\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Входной линейный слой для приведения входных данных к скрытой размерности\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Трансформер Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                # dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Линейный слой для классификации\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем линейное преобразование к данным\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Применяем трансформер\n",
    "        # Для PyTorch nn.Transformer ожидает вход размером (seq_len, batch_size, input_dim)\n",
    "        x = x.permute(1, 0, 2)  # меняем размерность на (seq_len, batch_size, hidden_dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Применяем усреднение по времени (среднее по всем временным шагам)\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        # Прогоняем через линейный слой для классификации\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1731260720943,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "5Kk0J71-uk1L",
    "outputId": "fa9b9673-5e9e-4e72-e3e4-f11f402cd712"
   },
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = TransformerClassifier(input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412563,
     "status": "ok",
     "timestamp": 1731261145631,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "m3l9ITmInRnc",
    "outputId": "9bb8ea7d-bc7b-43b8-e739-58b71fdbf62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0059, Accuracy: 99.79%\n",
      "Epoch 2/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 3/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 4/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 5/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1731262935940,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "PieoA-g83z4t"
   },
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1731262946043,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "lCc73n76-hjK"
   },
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731262946043,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "dRd435ed4Csk"
   },
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         #shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "executionInfo": {
     "elapsed": 36414,
     "status": "ok",
     "timestamp": 1731263227202,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "FSrYcQl54SO4"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1731263249860,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "yXHgJMg__GD3"
   },
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1731263250145,
     "user": {
      "displayName": "Лада Демина",
      "userId": "04628529621896984284"
     },
     "user_tz": -180
    },
    "id": "gUbyf91K_aAF",
    "outputId": "5815159d-5cd7-44e4-949b-7fdb132c3501"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025086872828179296"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742\n",
       "cnn_win     0.880621  0.864438  0.868353\n",
       "tr          0.664349  0.494766  0.565771"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['tr', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "qZyvyuBrWl79"
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_2_'></a>[С масштабированием каждого окна](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train_scaled.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          # shuffle=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим модель Transformer для классификации\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Входной линейный слой для приведения входных данных к скрытой размерности\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Трансформер Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                # dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Линейный слой для классификации\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем линейное преобразование к данным\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Применяем трансформер\n",
    "        # Для PyTorch nn.Transformer ожидает вход размером (seq_len, batch_size, input_dim)\n",
    "        x = x.permute(1, 0, 2)  # меняем размерность на (seq_len, batch_size, hidden_dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Применяем усреднение по времени (среднее по всем временным шагам)\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        # Прогоняем через линейный слой для классификации\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = TransformerClassifier(input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0101, Accuracy: 99.63%\n",
      "Epoch 2/20, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 3/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 4/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 5/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test_scaled.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test_scaled.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         # shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045311367215819603"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742\n",
       "cnn_win     0.880621  0.864438  0.868353\n",
       "tr          0.664349  0.494766  0.565771\n",
       "tr_win       0.98331  0.983027  0.982512"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['tr_win', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики улетели в космос. Не случайно у меня диплом по космическим двигателям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'tr_win')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[CNN + Transformer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_1_'></a>[С масштабированием всей выборки](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52693921, 9.78015267])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_dim = 12  # Размерность каждого временного шага\n",
    "num_classes = 2  # Количество классов для классификации\n",
    "seq_length = 48  # Длина временного ряда\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          #shuffle=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим модель Transformer для классификации\n",
    "class TimesNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, seq_length):\n",
    "        super(TimesNet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, \n",
    "            nhead=4, \n",
    "            dim_feedforward=512, \n",
    "            dropout=0.1)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, \n",
    "            num_layers=4)\n",
    "\n",
    "        # 3. Полносвязные слои для классификации\n",
    "        self.fc1 = nn.Linear(128 * seq_length, 256)\n",
    "        self.fc2 = nn.Linear(256, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем свертки\n",
    "        x = x.permute(0, 2, 1)  # Изменяем форму с (batch, channels, time) на (batch, time, channels)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x)) # -> (32, 48, 128)\n",
    "        \n",
    "        # Применяем трансформер\n",
    "        x = x.permute(2, 0, 1) # (channels, batch, time) = (128, 32, 48) () ## (time, batch, channels)\n",
    "        x = self.transformer(x) # d_model=128 - кодирует временные метки\n",
    "        \n",
    "        # Преобразуем в формат для полносвязных слоёв\n",
    "        x = x.permute(1, 0, 2)  # (batch, time, channels)\n",
    "        x = x.flatten(1, -1)  # Flatten: (batch, time * channels)\n",
    "        \n",
    "        # Полносвязные слои\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = TimesNet(input_dim, num_classes, seq_length)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0430, Accuracy: 99.28%\n",
      "Epoch 2/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 3/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 4/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 5/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/10, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         #shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013337166570835729"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742\n",
       "cnn_win     0.880621  0.864438  0.868353\n",
       "tr          0.664349  0.494766  0.565771\n",
       "tr_win       0.98331  0.983027  0.982512\n",
       "cnn_tr      0.546824  0.171259  0.300617"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['cnn_tr', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'cnn_tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_2_'></a>[CNN + Transformer с масштабированием каждого окна](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train_scaled.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          #shuffle=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = TimesNet(input_dim, num_classes, seq_length)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0625, Accuracy: 98.53%\n",
      "Epoch 2/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 3/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 4/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 5/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/10, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test_scaled.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         #shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05772355691107722"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742\n",
       "cnn_win     0.880621  0.864438  0.868353\n",
       "tr          0.664349  0.494766  0.565771\n",
       "tr_win       0.98331  0.983027  0.982512\n",
       "cnn_tr      0.546824  0.171259  0.300617\n",
       "cnn_tr_win  0.991766  0.988816  0.988385"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['cnn_tr_win', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'cnn_tr_win')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[Informer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_1_'></a>[С масштабированием всей выборки](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52693921, 9.78015267])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          #shuffle=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение механизма внимания с линейной сложностью (Probabilistic Linear Time Attention)\n",
    "class ProbSparseAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1, k=20):\n",
    "        super(ProbSparseAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k  # Число ближайших соседей для внимания\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Определяем слои для проекции запросов (Q), ключей (K) и значений (V)\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        batch_size, seq_len, _ = queries.size()\n",
    "        num_heads = self.num_heads\n",
    "        head_dim = self.dim // num_heads\n",
    "\n",
    "        # Проектируем Q, K, V\n",
    "        Q = self.query_proj(queries).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        K = self.key_proj(keys).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        V = self.value_proj(values).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Считаем вероятностное внимание\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Применяем attention и получаем результат\n",
    "        attn = torch.softmax(attn_weights, dim=-1)\n",
    "        attn = self.dropout_layer(attn)\n",
    "\n",
    "        output = torch.matmul(attn, V).transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение энкодера\n",
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1, k=20):\n",
    "        super(InformerEncoderLayer, self).__init__()\n",
    "        self.attn = ProbSparseAttention(dim, num_heads, dropout, k)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Механизм внимания\n",
    "        attn_out = self.attn(x, x, x, mask)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # Feed Forward слой\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.layer_norm2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение модели Informer для классификации\n",
    "class InformerForClassification(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dim, num_heads, num_layers, dropout=0.1, k=20):\n",
    "        super(InformerForClassification, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "# Входной слой для проекции\n",
    "        self.input_proj = nn.Linear(input_size, dim)\n",
    "\n",
    "        # Энкодер Informer\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            InformerEncoderLayer(dim, num_heads, dropout, k) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Выходной слой для классификации\n",
    "        self.classifier = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Преобразуем входные данные через проекцию\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # Пропускаем через энкодер\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # Используем только последний временной шаг для классификации\n",
    "        x = x[:, -1, :]  # Берем представление последнего временного шага\n",
    "\n",
    "        # Прогоняем через классификатор\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры модели\n",
    "input_size = 12  # Например, 10 признаков на каждом временном шаге\n",
    "num_classes = 2  # Количество классов для классификации\n",
    "dim = 64  # Размерность модели\n",
    "num_heads = 4  # Количество голов в многоголовом внимании\n",
    "num_layers = 4  # Количество слоев в энкодере\n",
    "dropout = 0.1  # Дроп-аут для регуляризации\n",
    "k = 20  # Число ближайших соседей в вероятностном механизме внимания\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = InformerForClassification(input_size, num_classes, dim, num_heads, num_layers, dropout, k)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0153, Accuracy: 99.56%\n",
      "Epoch 2/20, Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch 3/20, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 4/20, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 5/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         #shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02423689407764806"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc_auc        f1       MCC\n",
       "tsfresh_cb   0.56036  0.200074   0.23308\n",
       "cnn         0.616004     0.375  0.467742\n",
       "cnn_win     0.880621  0.864438  0.868353\n",
       "tr          0.664349  0.494766  0.565771\n",
       "tr_win       0.98331  0.983027  0.982512\n",
       "cnn_tr      0.546824  0.171259  0.300617\n",
       "cnn_tr_win  0.991766  0.988816  0.988385\n",
       "informer    0.656421  0.475855  0.549699"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['informer', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'informer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_2_'></a>[Informer с масштабированием каждого окна](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train_scaled.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          #shuffle=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = InformerForClassification(input_size, num_classes, dim, num_heads, num_layers, dropout, k)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0174, Accuracy: 99.54%\n",
      "Epoch 2/10, Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch 3/10, Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch 4/10, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 5/10, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 6/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/10, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/10, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test_scaled.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         #shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048023799405014875"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer_win</th>\n",
       "      <td>0.986787</td>\n",
       "      <td>0.98661</td>\n",
       "      <td>0.986177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roc_auc        f1       MCC\n",
       "tsfresh_cb     0.56036  0.200074   0.23308\n",
       "cnn           0.616004     0.375  0.467742\n",
       "cnn_win       0.880621  0.864438  0.868353\n",
       "tr            0.664349  0.494766  0.565771\n",
       "tr_win         0.98331  0.983027  0.982512\n",
       "cnn_tr        0.546824  0.171259  0.300617\n",
       "cnn_tr_win    0.991766  0.988816  0.988385\n",
       "informer      0.656421  0.475855  0.549699\n",
       "informer_win  0.986787   0.98661  0.986177"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['informer_win', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'informer_win')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_6_'></a>[iTransformer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_1_'></a>[С масштабированием всей выборки](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_dim = 12  # Размерность каждого временного шага\n",
    "num_classes = 2  # Количество классов для классификации\n",
    "seq_length = 48  # Длина временного ряда\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "hidden_dim = 128\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          #shuffle=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим модель Transformer для классификации\n",
    "class iTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout):\n",
    "        super(iTransformer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Входной линейный слой для приведения входных данных к скрытой размерности\n",
    "        self.embedding = nn.Linear(48, 128)\n",
    "\n",
    "        # Трансформер Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=128,\n",
    "                nhead=4,\n",
    "                dim_feedforward=128,\n",
    "                # dropout=dropout\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        # Линейный слой для классификации\n",
    "        self.fc1 = nn.Linear(12 * 128, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) # -> (32, 12, 48)\n",
    "        # Применяем линейное преобразование к данным\n",
    "        x = self.embedding(x) # -> (32, 12, 128)\n",
    "\n",
    "        # Применяем трансформер\n",
    "        # Для PyTorch nn.Transformer ожидает вход размером (seq_len, batch_size, input_dim)\n",
    "        x = x.permute(1, 0, 2)  # меняем размерность на (seq_len, batch_size, hidden_dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Применяем усреднение по времени (среднее по всем временным шагам)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.flatten(1, -1)\n",
    "\n",
    "        # Прогоняем через линейный слой для классификации\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = iTransformer(input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0409, Accuracy: 98.65%\n",
      "Epoch 2/20, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 3/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 4/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 5/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         #shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037224069398265044"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer_win</th>\n",
       "      <td>0.986787</td>\n",
       "      <td>0.98661</td>\n",
       "      <td>0.986177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.696337</td>\n",
       "      <td>0.55159</td>\n",
       "      <td>0.592896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roc_auc        f1       MCC\n",
       "tsfresh_cb     0.56036  0.200074   0.23308\n",
       "cnn           0.616004     0.375  0.467742\n",
       "cnn_win       0.880621  0.864438  0.868353\n",
       "tr            0.664349  0.494766  0.565771\n",
       "tr_win         0.98331  0.983027  0.982512\n",
       "cnn_tr        0.546824  0.171259  0.300617\n",
       "cnn_tr_win    0.991766  0.988816  0.988385\n",
       "informer      0.656421  0.475855  0.549699\n",
       "informer_win  0.986787   0.98661  0.986177\n",
       "itr           0.696337   0.55159  0.592896"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['itr', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'itr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_2_'></a>[С масштабированием каждого окна](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в PyTorch тензоры\n",
    "X_tensor = torch.tensor(X_train_scaled.astype('float32'))\n",
    "y_tensor = torch.tensor(Y_train[:,0].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на обучающую и тестовую выборки\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          # shuffle=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель, функцию потерь и оптимизатор\n",
    "model = iTransformer(input_dim, hidden_dim, num_classes, num_heads, num_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0227, Accuracy: 99.15%\n",
      "Epoch 2/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 3/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 4/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 5/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        output = model(data)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Оценка точности\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "X_test_tensor = torch.tensor(X_test_scaled.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = np.array([i for i in range(X_test_scaled.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_indexes))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         # shuffle=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = torch.tensor(np.array([]))\n",
    "pred_proba_0 = torch.tensor(np.array([]))\n",
    "pred_proba_1 = torch.tensor(np.array([]))\n",
    "with torch.no_grad():\n",
    "    for data, indexes in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predictions = torch.cat((predictions, predicted))\n",
    "        pred_proba_0 = torch.cat((pred_proba_0, output[:,0]))\n",
    "        pred_proba_1 = torch.cat((pred_proba_1, output[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02819929501762456"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer_win</th>\n",
       "      <td>0.986787</td>\n",
       "      <td>0.98661</td>\n",
       "      <td>0.986177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.696337</td>\n",
       "      <td>0.55159</td>\n",
       "      <td>0.592896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr_win</th>\n",
       "      <td>0.693546</td>\n",
       "      <td>0.52671</td>\n",
       "      <td>0.550744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roc_auc        f1       MCC\n",
       "tsfresh_cb     0.56036  0.200074   0.23308\n",
       "cnn           0.616004     0.375  0.467742\n",
       "cnn_win       0.880621  0.864438  0.868353\n",
       "tr            0.664349  0.494766  0.565771\n",
       "tr_win         0.98331  0.983027  0.982512\n",
       "cnn_tr        0.546824  0.171259  0.300617\n",
       "cnn_tr_win    0.991766  0.988816  0.988385\n",
       "informer      0.656421  0.475855  0.549699\n",
       "informer_win  0.986787   0.98661  0.986177\n",
       "itr           0.696337   0.55159  0.592896\n",
       "itr_win       0.693546   0.52671  0.550744"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.loc['itr_win', :] = calc_metrics(X_states, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = softmax(torch.tensor(np.column_stack((np.array(pred_proba_0), np.array(pred_proba_1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(probabilities)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(Y_pred, 'itr_win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer_win</th>\n",
       "      <td>0.986787</td>\n",
       "      <td>0.98661</td>\n",
       "      <td>0.986177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.696337</td>\n",
       "      <td>0.55159</td>\n",
       "      <td>0.592896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr_win</th>\n",
       "      <td>0.693546</td>\n",
       "      <td>0.52671</td>\n",
       "      <td>0.550744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roc_auc        f1       MCC\n",
       "tsfresh_cb     0.56036  0.200074   0.23308\n",
       "cnn           0.616004     0.375  0.467742\n",
       "cnn_win       0.880621  0.864438  0.868353\n",
       "tr            0.664349  0.494766  0.565771\n",
       "tr_win         0.98331  0.983027  0.982512\n",
       "cnn_tr        0.546824  0.171259  0.300617\n",
       "cnn_tr_win    0.991766  0.988816  0.988385\n",
       "informer      0.656421  0.475855  0.549699\n",
       "informer_win  0.986787   0.98661  0.986177\n",
       "itr           0.696337   0.55159  0.592896\n",
       "itr_win       0.693546   0.52671  0.550744"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv('graphs/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer_win</th>\n",
       "      <td>0.986787</td>\n",
       "      <td>0.98661</td>\n",
       "      <td>0.986177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.98331</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.696337</td>\n",
       "      <td>0.55159</td>\n",
       "      <td>0.592896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr_win</th>\n",
       "      <td>0.693546</td>\n",
       "      <td>0.52671</td>\n",
       "      <td>0.550744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.56036</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.23308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roc_auc        f1       MCC\n",
       "cnn_tr_win    0.991766  0.988816  0.988385\n",
       "informer_win  0.986787   0.98661  0.986177\n",
       "tr_win         0.98331  0.983027  0.982512\n",
       "cnn_win       0.880621  0.864438  0.868353\n",
       "itr           0.696337   0.55159  0.592896\n",
       "itr_win       0.693546   0.52671  0.550744\n",
       "tr            0.664349  0.494766  0.565771\n",
       "informer      0.656421  0.475855  0.549699\n",
       "cnn           0.616004     0.375  0.467742\n",
       "tsfresh_cb     0.56036  0.200074   0.23308\n",
       "cnn_tr        0.546824  0.171259  0.300617"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.sort_values(by='roc_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[4 Выводы](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решалась задача классификации многомерных временных рядов для поиска паттернов отказа технологического оборудования. \n",
    "\n",
    "В качестве данных используются показания с датчиков температуры подшипников компрессора за 2 года (2022-2023 гг) с дискретностью 10 мин. \n",
    "\n",
    "Для оценки качества использовалась метрика AUC-ROC. Также рассчитывались метрики f1 и MCC.\n",
    "\n",
    "Было протестировано несколько алгоритмов на основе нейронных сетей, в качестве бейзлайна использовался CatBoost на признаках, сгенерированных с помощью библиотеки tsfresh:\n",
    "\n",
    "1. Tsfresf + CatBoost \n",
    "2. CNN \n",
    "3. Vanilla Transformer \n",
    "4. CNN + Vanilla Transformer\n",
    "5. Informer\n",
    "6. iTransformer\n",
    "\n",
    "В ходе тестирования было обнаружено, что на предсказания модели CNN сильно влияет величина значений тегов в рамках каждого окна (объекта), а не взаимосвязь между тегами. Модель пропускает отказы, где абсолютные значения тегов выше или ниже, чем на обучающем отказе. Тогда было протестировано масштабирование каждого окна отдельно, чтобы избавиться от уровня значений тегов.\n",
    "\n",
    "Качество модели CNN с таким подходом сильно улучшилось, поэтому все алгоритмы были протестированы с применением такого масштабирования (помечены `win`) и без него. Результаты представлены в таблице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_tr_win</th>\n",
       "      <td>0.991766</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.988385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer_win</th>\n",
       "      <td>0.986787</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.986177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_win</th>\n",
       "      <td>0.983310</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_win</th>\n",
       "      <td>0.880621</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>0.868353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr</th>\n",
       "      <td>0.696337</td>\n",
       "      <td>0.551590</td>\n",
       "      <td>0.592896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itr_win</th>\n",
       "      <td>0.693546</td>\n",
       "      <td>0.526710</td>\n",
       "      <td>0.550744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.565771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informer</th>\n",
       "      <td>0.656421</td>\n",
       "      <td>0.475855</td>\n",
       "      <td>0.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.616004</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsfresh_cb</th>\n",
       "      <td>0.560360</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.233080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_tr</th>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>0.300617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roc_auc        f1       MCC\n",
       "cnn_tr_win    0.991766  0.988816  0.988385\n",
       "informer_win  0.986787  0.986610  0.986177\n",
       "tr_win        0.983310  0.983027  0.982512\n",
       "cnn_win       0.880621  0.864438  0.868353\n",
       "itr           0.696337  0.551590  0.592896\n",
       "itr_win       0.693546  0.526710  0.550744\n",
       "tr            0.664349  0.494766  0.565771\n",
       "informer      0.656421  0.475855  0.549699\n",
       "cnn           0.616004  0.375000  0.467742\n",
       "tsfresh_cb    0.560360  0.200074  0.233080\n",
       "cnn_tr        0.546824  0.171259  0.300617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.sort_values(by='roc_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабирование каждого окна значительно улучшает результаты всех протестированных моделей, кроме iTransformer. Для нее результат почти не меняется. Худший результат показала модель на основе CNN + Vanilla Transformer. Она же показала наилучший результат с масштабированием каждого окна, ROC-AUC улетела в космос и составила 0,99 (не зря у меня диплом по космическим двигателям)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
